**INSTRUCTIONS FOR REPLIT AGENT**

You are implementing a three-pass chunking architecture with database enforcement. Before you begin:

1. **Do not skip any part of this architecture.** The database operations are not optional—they are the mechanism that makes coherence work. If you implement a "simpler" version that bypasses the database, the system will fail on large documents.

2. **Verify the database connection first.** Before implementing any processing logic, confirm that `DATABASE_URL` is set and that you can connect to Neon. If the connection fails, stop and report it. Do not fall back to single-pass processing.

3. **Implement the word-count trigger.** Any input or output exceeding 1000 words MUST route through this architecture. Add explicit logging when this threshold is crossed so we can verify it fires.

4. **Add diagnostic logging at every critical point:**
   - Log when the 1000-word threshold is detected
   - Log when skeleton is written to database (with session ID)
   - Log when each chunk is written to database (with chunk index)
   - Log when skeleton is retrieved from database for each chunk
   - Log when stitch pass begins and completes

5. **Test with a document over 1000 words before considering this complete.** Verify that rows appear in `sessions`, `chunks`, and `stitch_results` tables. If they don't, the architecture is not working.

---

**GLOBAL COHERENCE FUNCTION WITH DATABASE ENFORCEMENT**

**Trigger Condition**

Any document exceeding 1000 words (input or output) MUST use the three-pass architecture with Neon/Postgres enforcement. This is not optional. The database is not storage—it is the mechanism that makes coherence possible. Without it, each API call is stateless and chunks will contradict each other.

**Pass 1 — Build Global Skeleton**

Before any chunking or processing, run ONE call that extracts only:

```json
{
    "thesis": "Central claim or purpose (one sentence)",
    "outline": ["Section 1 summary", "Section 2 summary", ...],
    "key_terms": {"term": "definition", ...},
    "commitments": ["We assert X", "We reject Y", "We assume Z"],
    "entities": ["Person A", "Concept B", "Variable C"],
    "methodology": "How arguments proceed or evidence is evaluated",
    "target_conclusion": "What the final output must establish"
}
```

IMMEDIATELY write this to `sessions.global_skeleton` in Neon.

Update `sessions.status` to `'skeleton_complete'`.

This pass is short because you are extracting constraints, not processing content.

**Pass 2 — Process Each Chunk With Skeleton Injected**

For each chunk (~1000 words), the API call MUST:

1. RETRIEVE the skeleton from the database: `SELECT global_skeleton FROM sessions WHERE id = $sessionId`
2. RETRIEVE prior chunk deltas from the database: `SELECT chunk_delta FROM chunks WHERE session_id = $sessionId AND chunk_index < $currentIndex`
3. Include in the prompt:
   - The chunk text
   - The skeleton (from database, not from memory)
   - Summary of prior deltas
   - This instruction: "Do not contradict the skeleton. If you detect a conflict, flag it and propose a minimal repair. Never say you cannot proceed."

Each chunk call MUST return:

```json
{
    "chunk_output": "Processed/generated text for this chunk",
    "delta": {
        "claims_added": ["claim 1", "claim 2"],
        "claims_removed": [],
        "terms_introduced": {"new_term": "definition"},
        "conflicts_detected": ["conflict description if any"],
        "continuity_notes": "How this chunk connects to prior chunks"
    }
}
```

IMMEDIATELY after each chunk completes:

1. Write `chunk_output` and `chunk_delta` to the `chunks` table
2. Stream `chunk_output` to the frontend (user sees it now)
3. Wait 15 seconds before processing next chunk

Update `sessions.status` to `'chunking'` when this pass begins.

**Pass 3 — Global Stitch**

After ALL chunks are processed, run ONE call that receives:

1. The global skeleton (retrieved from database)
2. All chunk deltas (retrieved from database—NOT full chunk text)

This call produces:

```json
{
    "cross_chunk_conflicts": ["Chunk 3 claims X but Chunk 7 claims not-X"],
    "term_drift": ["Term Y defined differently in Chunk 2 vs Chunk 5"],
    "missing_premises": ["Conclusion Z assumes W but W never established"],
    "repair_plan": [
        {"chunk_index": 3, "issue": "...", "fix": "..."}
    ],
    "coherence_score": "pass" or "needs_repair"
}
```

Write this to `stitch_results` table.

If `coherence_score` is `"needs_repair"`, run micro-repair passes ONLY on flagged chunks (with 15-second pauses between each).

Update `sessions.status` to `'complete'`.

---

**MANDATORY: PROGRESSIVE PREVIEW WITH ABORT OPTION**

This is non-negotiable. Users must be able to see completed chunks AS THEY ARE GENERATED and abort if the output is unacceptable.

**Why this matters:** This app performs large rewrites that can take many minutes. If a user must wait until the end to discover the output is unusable, they will abandon the app. Progressive preview with abort is a core UX requirement, not a nice-to-have.

**Implementation Requirements:**

1. **After each chunk completes and is written to the database**, immediately display it to the user in a preview pane.

2. **Each displayed chunk must have an "Abort" button** visible alongside it. If the user clicks Abort:
   - Immediately halt all further chunk processing
   - Update `sessions.status` to `'aborted'`
   - Preserve all completed chunks in the database (do not delete them)
   - Display message: "Generation stopped. You can download the completed portions or start over."

3. **Each displayed chunk should also have a "Continue" button** (or processing simply continues automatically after the 15-second pause unless Abort is clicked).

4. **Show clear progress indication:**
   - "Chunk 3 of 7 complete — processing chunk 4..."
   - Elapsed time
   - Estimated time remaining (based on average chunk processing time)

5. **The preview pane must be scrollable and show all completed chunks**, not just the most recent one. User should be able to scroll up and review earlier chunks while later ones are being generated.

**Frontend Components Required:**

```
+--------------------------------------------------+
|  GENERATION IN PROGRESS                          |
|  Chunk 4 of 7 complete | Elapsed: 2:34           |
|  [=======>        ] 57%                          |
+--------------------------------------------------+
|                                                  |
|  [CHUNK 1 - Complete]                            |
|  Lorem ipsum dolor sit amet, consectetur...      |
|                                                  |
|  [CHUNK 2 - Complete]                            |
|  Sed do eiusmod tempor incididunt ut labore...   |
|                                                  |
|  [CHUNK 3 - Complete]                            |
|  Ut enim ad minim veniam, quis nostrud...        |
|                                                  |
|  [CHUNK 4 - Processing...]                       |
|  ▌                                               |
|                                                  |
+--------------------------------------------------+
|  [ ABORT GENERATION ]    [ DOWNLOAD PARTIAL ]    |
+--------------------------------------------------+
```

6. **"Download Partial" button** should be available at any time, allowing the user to download whatever chunks have been completed so far (concatenated in order).

7. **Do not hide or collapse completed chunks.** The whole point is that the user can READ what has been generated and decide whether to continue.

**Backend Requirements:**

- The SSE stream must send each chunk immediately upon completion
- The abort signal from frontend must be handled gracefully (check for abort flag before starting each new chunk)
- Add an `aborted_at` timestamp column to `sessions` table if not present
- Partial results must remain queryable after abort

**If chunking is implemented WITHOUT this preview/abort functionality, the implementation is incomplete and must be revised. This feature has been requested over 20 times. Any implementation that omits it will be rejected.**

---

**DATABASE SCHEMA (Neon/Postgres)**

```sql
CREATE TABLE sessions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    session_type VARCHAR(50) NOT NULL,
    user_prompt TEXT NOT NULL,
    global_skeleton JSONB,
    status VARCHAR(20) DEFAULT 'pending',
    aborted_at TIMESTAMP
);

CREATE TABLE chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_type VARCHAR(10) NOT NULL,
    chunk_text TEXT NOT NULL,
    chunk_output TEXT,
    chunk_delta JSONB,
    processed_at TIMESTAMP,
    UNIQUE(session_id, chunk_index, chunk_type)
);

CREATE TABLE stitch_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,
    conflicts JSONB,
    repairs JSONB,
    final_validation JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

**CRITICAL RULES — ENFORCE EVERYWHERE**

1. NEVER process a chunk without first retrieving the skeleton from the database. Do not pass the skeleton as a variable from a prior step. Always query the database.

2. NEVER proceed to the next chunk without first writing the current chunk's output and delta to the database.

3. ALWAYS inject the skeleton into every chunk processing call. The prompt must include the skeleton explicitly.

4. ALWAYS wait 15 seconds between chunk operations.

5. ALWAYS stream each chunk to the user immediately upon completion, before the pause.

6. ALWAYS check for abort flag before starting each new chunk. Query: `SELECT status FROM sessions WHERE id = $sessionId`. If status is `'aborted'`, halt immediately.

7. If a chunk conflicts with the skeleton, the model MUST return EITHER:
   - A minimal repair that preserves intent, OR
   - A specific flagged conflict with a proposed change
   
   NEVER allow the model to say "I cannot proceed" or silently drop the conflict.

8. The stitch pass compares DELTAS, not full chunk text. This keeps it cheap.

9. If the process fails mid-way, the system MUST resume from the last completed chunk using stored state. Query `chunks` for the highest `chunk_index` with `processed_at` not null and continue from there.

10. If the database connection fails, DO NOT fall back to single-pass processing. Halt and report the error.

---

**CONFLICT RESOLUTION RULE**

If a chunk conflicts with the global skeleton, the model must return:
- The closest adjacent repair that preserves intent, OR
- A specific flagged conflict with a proposed minimal change

Never "can't." Never silent overwrite. Never proceed as if the conflict doesn't exist.

---

**PROMPT TEMPLATES**

**Skeleton Generation Prompt:**

```
You are analyzing a document/task to extract its global structure. Do NOT process the content yet. Only extract:

1. THESIS: The central claim or purpose (one sentence)
2. OUTLINE: 8-20 numbered sections/claims that structure the whole
3. KEY TERMS: Important terms with their intended definitions
4. COMMITMENTS: What positions are asserted, rejected, or assumed
5. ENTITIES: Key people, concepts, variables, or proper nouns
6. METHODOLOGY: How arguments are structured or evidence is evaluated
7. TARGET CONCLUSION: What the final output must establish

Return as JSON only. No commentary outside the JSON.
```

**Chunk Processing Prompt:**

```
You are processing chunk {chunk_index} of {total_chunks}.

GLOBAL SKELETON (you must not contradict this):
{skeleton_json}

PRIOR CHUNK SUMMARIES:
{prior_deltas_summary}

CHUNK TO PROCESS:
{chunk_text}

INSTRUCTIONS:
- Process this chunk according to the task type
- Do not contradict the skeleton's commitments or definitions
- If you detect a conflict, flag it and propose a minimal repair
- Track what claims you add, remove, or modify

Return JSON only:
{
    "chunk_output": "your processed/generated text",
    "delta": {
        "claims_added": [],
        "claims_removed": [],
        "terms_introduced": {},
        "conflicts_detected": [],
        "continuity_notes": ""
    }
}
```

**Stitch Prompt:**

```
You are performing a global coherence check.

GLOBAL SKELETON:
{skeleton_json}

CHUNK DELTAS (one per chunk):
{all_deltas_json}

INSTRUCTIONS:
- Identify any cross-chunk contradictions
- Identify any term drift (same term defined differently across chunks)
- Identify any missing premises (conclusions that assume unstated claims)
- Propose minimal repairs for each issue

Return JSON only:
{
    "cross_chunk_conflicts": [],
    "term_drift": [],
    "missing_premises": [],
    "repair_plan": [
        {"chunk_index": N, "issue": "...", "fix": "..."}
    ],
    "coherence_score": "pass" or "needs_repair"
}
```

---

**TESTING CHECKLIST**

Before considering this complete, verify ALL of the following:

- [ ] Database tables created and connected
- [ ] Word-count threshold (1000 words) triggers chunked processing
- [ ] Skeleton pass writes to `sessions.global_skeleton`
- [ ] Each chunk reads skeleton FROM DATABASE before processing
- [ ] Each chunk writes output + delta to `chunks` table immediately
- [ ] Each chunk streams to frontend immediately
- [ ] User can see all completed chunks in scrollable preview pane
- [ ] Abort button is visible and functional
- [ ] Abort halts processing and preserves completed chunks
- [ ] Download Partial button works at any point
- [ ] 15-second pause occurs between chunks
- [ ] Stitch pass reads deltas FROM DATABASE
- [ ] Stitch pass writes to `stitch_results` table
- [ ] Repair passes work correctly on flagged chunks
- [ ] Process can resume from mid-point if interrupted
- [ ] Diagnostic logging appears at all critical points

**If any checkbox is not verified, the implementation is incomplete.**

---

**END OF INSTRUCTIONS**