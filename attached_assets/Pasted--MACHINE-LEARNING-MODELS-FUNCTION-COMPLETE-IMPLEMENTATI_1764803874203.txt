# MACHINE LEARNING MODELS FUNCTION - COMPLETE IMPLEMENTATION INSTRUCTIONS

## FUNCTION PURPOSE

The Machine Learning Models function takes a natural language description of a machine learning problem and generates a complete, production-ready Python implementation. The output is a downloadable `.py` file (or `.ipynb` Jupyter notebook) containing all code necessary to load data, preprocess it, train models, tune hyperparameters, evaluate performance, and deploy predictions.

---

## SUPPORTED MODEL CATEGORIES

### Category A: Classification Models
1. **Random Forest Classifier** - Ensemble of decision trees, robust to overfitting
2. **XGBoost Classifier** - Gradient boosting, high performance on tabular data
3. **Support Vector Machine (SVM)** - Effective for high-dimensional spaces
4. **Neural Network (MLP Classifier)** - Deep learning for complex patterns
5. **Gradient Boosting Classifier** - Sequential ensemble method
6. **K-Nearest Neighbors (KNN)** - Instance-based learning
7. **Naive Bayes** - Probabilistic classifier, fast training

### Category B: Clustering Models
1. **K-Means** - Centroid-based, spherical clusters
2. **DBSCAN** - Density-based, arbitrary shapes, handles noise
3. **Hierarchical (Agglomerative)** - Tree-based, no preset k required
4. **Gaussian Mixture Models (GMM)** - Probabilistic soft clustering
5. **Mean Shift** - Density-based, automatic cluster count

### Category C: Dimensionality Reduction
1. **PCA (Principal Component Analysis)** - Linear projection, variance maximization
2. **t-SNE** - Non-linear, visualization-focused
3. **UMAP** - Fast non-linear, preserves global structure
4. **LDA (Linear Discriminant Analysis)** - Supervised dimensionality reduction

---

## INPUT PROCESSING REQUIREMENTS

The function receives natural language input. Extract the following variables:

**Required Variables:**
- `problem_type`: "classification", "clustering", or "dimensionality_reduction"
- `model_type`: Specific model or "auto" to compare multiple models
- `data_source`: Inline data, file path, or synthetic data generation instructions

**For Classification (additional required):**
- `target_variable`: The variable being predicted (Y)
- `feature_variables`: List of predictor variables (X)
- `class_labels`: Names/descriptions of classes (if known)

**For Clustering (additional required):**
- `feature_variables`: Variables to cluster on
- `n_clusters`: Number of clusters (or "auto" for automatic detection)

**For Dimensionality Reduction (additional required):**
- `feature_variables`: Variables to reduce
- `n_components`: Target dimensionality (or "auto" for explained variance threshold)
- `purpose`: "visualization" (2-3D) or "preprocessing" (feature reduction)

**Optional Variables (use defaults if not specified):**
- `test_size`: Train/test split ratio (default: 0.2)
- `random_state`: Random seed (default: 42)
- `cross_validation_folds`: Number of CV folds (default: 5)
- `scale_features`: Whether to standardize (default: True)
- `handle_imbalance`: Strategy for imbalanced classes (default: None)
- `hyperparameter_tuning`: "none", "grid", or "random" (default: "random")
- `tuning_iterations`: For random search (default: 50)
- `output_format`: "py" or "ipynb" (default: "py")

**Parsing Rules:**
- "classify", "predict category", "which group" → Classification
- "segment", "group similar", "find patterns", "cluster" → Clustering
- "reduce dimensions", "visualize high-dimensional", "compress features" → Dimensionality Reduction
- "best model", "compare models", "which algorithm" → Auto mode (compare multiple)
- "imbalanced", "rare class", "skewed" → Enable class balancing (SMOTE or class weights)
- "fast", "quick", "simple" → Use simpler models (KNN, Naive Bayes, K-Means)
- "accurate", "best performance", "production" → Use ensemble methods with tuning

---

## OUTPUT CODE STRUCTURE

The generated Python file must contain the following sections in order:

### Section 1: Header and Imports

```python
"""
Machine Learning Model: [MODEL_TYPE]
Problem Type: [PROBLEM_TYPE]
Generated by ModelWiz.xyz
Target Variable: [TARGET] (if classification)
Features: [FEATURE_LIST]
Generated on: [TIMESTAMP]
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Classification imports
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_auc_score,
                             roc_curve, precision_recall_curve, average_precision_score)

# Clustering imports
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Dimensionality reduction imports
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Additional utilities
from scipy.cluster.hierarchy import dendrogram, linkage
import warnings
warnings.filterwarnings('ignore')

# Optional: XGBoost (check if installed)
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("Note: XGBoost not installed. Using GradientBoostingClassifier instead.")

# Optional: UMAP (check if installed)
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("Note: UMAP not installed. Using t-SNE for non-linear reduction.")

# Optional: imbalanced-learn (check if installed)
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("Note: imbalanced-learn not installed. Using class weights for imbalance.")
```

### Section 2: Data Loading

```python
# --- DATA LOADING ---
print("=" * 70)
print("MACHINE LEARNING MODEL ANALYSIS")
print("=" * 70)

# [GENERATE APPROPRIATE DATA LOADING CODE]
# Options:
# 1. pd.read_csv(filepath) for file input
# 2. Direct DataFrame creation for inline data
# 3. sklearn.datasets or numpy generation for synthetic data

print(f"\nDataset loaded: {df.shape[0]} samples, {df.shape[1]} features")
```

### Section 3: Exploratory Data Analysis

```python
# --- EXPLORATORY DATA ANALYSIS ---
print("\n" + "-" * 70)
print("EXPLORATORY DATA ANALYSIS")
print("-" * 70)

print("\nDataset Shape:", df.shape)
print("\nColumn Types:")
print(df.dtypes)
print("\nFirst 5 Rows:")
print(df.head())
print("\nDescriptive Statistics:")
print(df.describe().round(2))
print("\nMissing Values:")
missing = df.isnull().sum()
if missing.sum() > 0:
    print(missing[missing > 0])
else:
    print("No missing values detected.")

# For classification: show class distribution
# [IF CLASSIFICATION]
print("\nTarget Variable Distribution:")
print(df[TARGET_COLUMN].value_counts())
print(f"\nClass Balance Ratio: {df[TARGET_COLUMN].value_counts().min() / df[TARGET_COLUMN].value_counts().max():.2%}")
# [END IF]

# Correlation analysis for numeric features
numeric_cols = df.select_dtypes(include=[np.number]).columns
if len(numeric_cols) > 1:
    print("\nTop Feature Correlations:")
    corr_matrix = df[numeric_cols].corr()
    # Get upper triangle of correlation matrix
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    # Find features with high correlation
    high_corr = [(col, idx, upper.loc[idx, col]) 
                 for col in upper.columns for idx in upper.index 
                 if abs(upper.loc[idx, col]) > 0.7]
    if high_corr:
        for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True)[:5]:
            print(f"  {feat1} <-> {feat2}: {corr:.3f}")
    else:
        print("  No highly correlated feature pairs (|r| > 0.7)")
```

### Section 4: Data Preprocessing

```python
# --- DATA PREPROCESSING ---
print("\n" + "-" * 70)
print("DATA PREPROCESSING")
print("-" * 70)

# Define features and target
feature_columns = [FEATURE_COLUMNS]
# [IF CLASSIFICATION]
target_column = '[TARGET_COLUMN]'
# [END IF]

# Identify column types
numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
categorical_features = df[feature_columns].select_dtypes(include=['object', 'category']).columns.tolist()

print(f"\nNumeric features ({len(numeric_features)}): {numeric_features}")
print(f"Categorical features ({len(categorical_features)}): {categorical_features}")

# Build preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'
)

# Prepare data
X = df[feature_columns]
# [IF CLASSIFICATION]
y = df[target_column]

# Encode target if categorical
if y.dtype == 'object':
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)
    class_names = label_encoder.classes_
    print(f"\nTarget classes encoded: {dict(zip(class_names, range(len(class_names))))}")
else:
    class_names = [f"Class {i}" for i in sorted(y.unique())]
# [END IF]

# [IF CLASSIFICATION OR CLUSTERING]
# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=[TEST_SIZE], random_state=[RANDOM_STATE], stratify=y  # stratify for classification
)
print(f"\nTraining set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")
# [END IF]

# Fit preprocessor and transform data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Get feature names after preprocessing
feature_names_processed = numeric_features.copy()
if categorical_features:
    ohe = preprocessor.named_transformers_['cat'].named_steps['encoder']
    cat_feature_names = ohe.get_feature_names_out(categorical_features).tolist()
    feature_names_processed.extend(cat_feature_names)

print(f"Features after preprocessing: {X_train_processed.shape[1]}")
```

---

## CLASSIFICATION-SPECIFIC SECTIONS

### Section 5A: Model Training (Classification)

```python
# --- MODEL TRAINING: CLASSIFICATION ---
print("\n" + "-" * 70)
print("MODEL TRAINING")
print("-" * 70)

# [IF SINGLE MODEL SPECIFIED]
# Initialize model
model = [MODEL_CLASS](
    [HYPERPARAMETERS],
    random_state=[RANDOM_STATE]
)

# [IF HYPERPARAMETER_TUNING == "grid" or "random"]
# Define hyperparameter grid
param_grid = {
    [PARAM_GRID_FOR_MODEL]
}

# Hyperparameter search
print("\nPerforming hyperparameter tuning...")
# [IF HYPERPARAMETER_TUNING == "grid"]
search = GridSearchCV(
    model, param_grid, cv=[CV_FOLDS], scoring='f1_weighted',
    n_jobs=-1, verbose=1
)
# [ELSE IF HYPERPARAMETER_TUNING == "random"]
search = RandomizedSearchCV(
    model, param_grid, n_iter=[TUNING_ITERATIONS], cv=[CV_FOLDS],
    scoring='f1_weighted', n_jobs=-1, verbose=1, random_state=[RANDOM_STATE]
)
# [END IF]

search.fit(X_train_processed, y_train)
model = search.best_estimator_

print(f"\nBest Parameters: {search.best_params_}")
print(f"Best CV Score: {search.best_score_:.4f}")
# [ELSE]
# Train model
model.fit(X_train_processed, y_train)
# [END IF]

# Cross-validation
cv_scores = cross_val_score(model, X_train_processed, y_train, cv=[CV_FOLDS], scoring='f1_weighted')
print(f"\nCross-Validation F1 Scores: {cv_scores.round(4)}")
print(f"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# [IF AUTO MODE - COMPARE MULTIPLE MODELS]
# Compare multiple models
print("\nComparing multiple models...")

models_to_compare = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=[RANDOM_STATE]),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=[RANDOM_STATE]),
    'SVM': SVC(kernel='rbf', probability=True, random_state=[RANDOM_STATE]),
    'MLP Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=[RANDOM_STATE]),
    'KNN': KNeighborsClassifier(n_neighbors=5),
}

if XGBOOST_AVAILABLE:
    models_to_compare['XGBoost'] = XGBClassifier(n_estimators=100, random_state=[RANDOM_STATE], use_label_encoder=False, eval_metric='mlogloss')

comparison_results = []

for name, clf in models_to_compare.items():
    print(f"  Training {name}...")
    clf.fit(X_train_processed, y_train)
    y_pred = clf.predict(X_test_processed)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    comparison_results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    })

comparison_df = pd.DataFrame(comparison_results).sort_values('F1 Score', ascending=False)
print("\n=== MODEL COMPARISON RESULTS ===")
print(comparison_df.to_string(index=False))

# Select best model
best_model_name = comparison_df.iloc[0]['Model']
model = models_to_compare[best_model_name]
print(f"\n✓ Best Model: {best_model_name} (F1 Score: {comparison_df.iloc[0]['F1 Score']:.4f})")
# [END IF]
```

### Section 6A: Model Evaluation (Classification)

```python
# --- MODEL EVALUATION: CLASSIFICATION ---
print("\n" + "-" * 70)
print("MODEL EVALUATION")
print("-" * 70)

# Predictions
y_pred_train = model.predict(X_train_processed)
y_pred_test = model.predict(X_test_processed)

# Probability predictions (if available)
if hasattr(model, 'predict_proba'):
    y_proba_test = model.predict_proba(X_test_processed)
else:
    y_proba_test = None

# Performance metrics
print("\n=== CLASSIFICATION PERFORMANCE ===")

print("\nTraining Set:")
print(f"  Accuracy:  {accuracy_score(y_train, y_pred_train):.4f}")
print(f"  Precision: {precision_score(y_train, y_pred_train, average='weighted', zero_division=0):.4f}")
print(f"  Recall:    {recall_score(y_train, y_pred_train, average='weighted', zero_division=0):.4f}")
print(f"  F1 Score:  {f1_score(y_train, y_pred_train, average='weighted', zero_division=0):.4f}")

print("\nTest Set:")
print(f"  Accuracy:  {accuracy_score(y_test, y_pred_test):.4f}")
print(f"  Precision: {precision_score(y_test, y_pred_test, average='weighted', zero_division=0):.4f}")
print(f"  Recall:    {recall_score(y_test, y_pred_test, average='weighted', zero_division=0):.4f}")
print(f"  F1 Score:  {f1_score(y_test, y_pred_test, average='weighted', zero_division=0):.4f}")

# ROC-AUC (for binary or OvR multiclass)
if y_proba_test is not None:
    if len(class_names) == 2:
        roc_auc = roc_auc_score(y_test, y_proba_test[:, 1])
    else:
        roc_auc = roc_auc_score(y_test, y_proba_test, multi_class='ovr', average='weighted')
    print(f"  ROC-AUC:   {roc_auc:.4f}")

# Detailed classification report
print("\n=== DETAILED CLASSIFICATION REPORT ===")
print(classification_report(y_test, y_pred_test, target_names=class_names, zero_division=0))

# Confusion Matrix
print("\n=== CONFUSION MATRIX ===")
cm = confusion_matrix(y_test, y_pred_test)
print(pd.DataFrame(cm, index=[f'Actual: {c}' for c in class_names], 
                   columns=[f'Pred: {c}' for c in class_names]))

# Check for overfitting
train_f1 = f1_score(y_train, y_pred_train, average='weighted', zero_division=0)
test_f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)
if train_f1 - test_f1 > 0.1:
    print("\n⚠️  WARNING: Possible overfitting (Train F1 significantly higher than Test F1)")
    print("   Consider: reducing model complexity, adding regularization, or getting more data")
else:
    print("\n✓ No significant overfitting detected")
```

### Section 7A: Feature Importance (Classification)

```python
# --- FEATURE IMPORTANCE ---
print("\n" + "-" * 70)
print("FEATURE IMPORTANCE")
print("-" * 70)

# Get feature importance (method depends on model type)
if hasattr(model, 'feature_importances_'):
    # Tree-based models (Random Forest, XGBoost, Gradient Boosting)
    importances = model.feature_importances_
    importance_type = "Gini Importance"
elif hasattr(model, 'coef_'):
    # Linear models (SVM with linear kernel, Logistic Regression)
    importances = np.abs(model.coef_).mean(axis=0) if len(model.coef_.shape) > 1 else np.abs(model.coef_)
    importance_type = "Coefficient Magnitude"
else:
    # For models without built-in importance, use permutation importance
    from sklearn.inspection import permutation_importance
    print("Computing permutation importance (this may take a moment)...")
    perm_importance = permutation_importance(model, X_test_processed, y_test, n_repeats=10, random_state=[RANDOM_STATE])
    importances = perm_importance.importances_mean
    importance_type = "Permutation Importance"

# Create importance DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names_processed,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print(f"\n=== FEATURE IMPORTANCE ({importance_type}) ===")
print("\nTop 15 Most Important Features:")
print(importance_df.head(15).to_string(index=False))

# Cumulative importance
importance_df['Cumulative'] = importance_df['Importance'].cumsum() / importance_df['Importance'].sum()
n_features_90 = (importance_df['Cumulative'] <= 0.9).sum() + 1
print(f"\nFeatures needed for 90% importance: {n_features_90} of {len(feature_names_processed)}")
```

### Section 8A: Visualizations (Classification)

```python
# --- VISUALIZATIONS: CLASSIFICATION ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

fig = plt.figure(figsize=(16, 12))

# Plot 1: Confusion Matrix Heatmap
ax1 = fig.add_subplot(2, 2, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, 
            yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})
ax1.set_xlabel('Predicted Label', fontsize=11)
ax1.set_ylabel('True Label', fontsize=11)
ax1.set_title('Confusion Matrix', fontsize=12)

# Plot 2: Feature Importance (Top 15)
ax2 = fig.add_subplot(2, 2, 2)
top_features = importance_df.head(15)
colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(top_features)))
bars = ax2.barh(range(len(top_features)), top_features['Importance'].values, color=colors)
ax2.set_yticks(range(len(top_features)))
ax2.set_yticklabels(top_features['Feature'].values)
ax2.invert_yaxis()
ax2.set_xlabel('Importance', fontsize=11)
ax2.set_title(f'Top 15 Feature Importance ({importance_type})', fontsize=12)

# Plot 3: ROC Curve (binary) or Class Distribution (multiclass)
ax3 = fig.add_subplot(2, 2, 3)
if y_proba_test is not None and len(class_names) == 2:
    # Binary ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_proba_test[:, 1])
    ax3.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.4f})')
    ax3.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random Classifier')
    ax3.fill_between(fpr, tpr, alpha=0.2)
    ax3.set_xlabel('False Positive Rate', fontsize=11)
    ax3.set_ylabel('True Positive Rate', fontsize=11)
    ax3.set_title('ROC Curve', fontsize=12)
    ax3.legend(loc='lower right')
    ax3.set_xlim([0, 1])
    ax3.set_ylim([0, 1.05])
else:
    # Class distribution comparison
    x_pos = np.arange(len(class_names))
    width = 0.35
    actual_counts = pd.Series(y_test).value_counts().sort_index()
    pred_counts = pd.Series(y_pred_test).value_counts().sort_index()
    ax3.bar(x_pos - width/2, actual_counts.values, width, label='Actual', color='steelblue')
    ax3.bar(x_pos + width/2, pred_counts.values, width, label='Predicted', color='coral')
    ax3.set_xticks(x_pos)
    ax3.set_xticklabels(class_names)
    ax3.set_xlabel('Class', fontsize=11)
    ax3.set_ylabel('Count', fontsize=11)
    ax3.set_title('Actual vs Predicted Class Distribution', fontsize=12)
    ax3.legend()

# Plot 4: Precision-Recall by Class
ax4 = fig.add_subplot(2, 2, 4)
report_dict = classification_report(y_test, y_pred_test, target_names=class_names, output_dict=True, zero_division=0)
metrics_df = pd.DataFrame({name: report_dict[name] for name in class_names}).T[['precision', 'recall', 'f1-score']]
x_pos = np.arange(len(class_names))
width = 0.25
ax4.bar(x_pos - width, metrics_df['precision'], width, label='Precision', color='#2ecc71')
ax4.bar(x_pos, metrics_df['recall'], width, label='Recall', color='#3498db')
ax4.bar(x_pos + width, metrics_df['f1-score'], width, label='F1-Score', color='#9b59b6')
ax4.set_xticks(x_pos)
ax4.set_xticklabels(class_names, rotation=45 if len(class_names) > 4 else 0)
ax4.set_xlabel('Class', fontsize=11)
ax4.set_ylabel('Score', fontsize=11)
ax4.set_title('Per-Class Performance Metrics', fontsize=12)
ax4.legend(loc='lower right')
ax4.set_ylim([0, 1.1])

plt.tight_layout()
plt.savefig('classification_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'classification_analysis.png'")
```

---

## CLUSTERING-SPECIFIC SECTIONS

### Section 5B: Model Training (Clustering)

```python
# --- MODEL TRAINING: CLUSTERING ---
print("\n" + "-" * 70)
print("CLUSTERING ANALYSIS")
print("-" * 70)

# Preprocess all data (no train/test split for unsupervised)
X_processed = preprocessor.fit_transform(X)

# [IF n_clusters == "auto" OR model selection needed]
# Determine optimal number of clusters
print("\nDetermining optimal number of clusters...")

k_range = range(2, 11)
silhouette_scores = []
inertia_scores = []
calinski_scores = []

for k in k_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=[RANDOM_STATE], n_init=10)
    labels_temp = kmeans_temp.fit_predict(X_processed)
    
    silhouette_scores.append(silhouette_score(X_processed, labels_temp))
    inertia_scores.append(kmeans_temp.inertia_)
    calinski_scores.append(calinski_harabasz_score(X_processed, labels_temp))

# Find optimal k using silhouette score
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"\nOptimal clusters by Silhouette Score: {optimal_k}")
print(f"Silhouette scores: {dict(zip(k_range, [f'{s:.3f}' for s in silhouette_scores]))}")

n_clusters = optimal_k  # Use optimal or user-specified
# [END IF]

# [IF SINGLE MODEL SPECIFIED]
# Initialize clustering model
# [IF K-MEANS]
model = KMeans(
    n_clusters=n_clusters,
    random_state=[RANDOM_STATE],
    n_init=10,
    max_iter=300
)
# [ELSE IF DBSCAN]
from sklearn.neighbors import NearestNeighbors
# Estimate eps using k-distance graph
neighbors = NearestNeighbors(n_neighbors=5)
neighbors.fit(X_processed)
distances, _ = neighbors.kneighbors(X_processed)
distances = np.sort(distances[:, -1])
# Use knee point or default
eps = np.percentile(distances, 90)
model = DBSCAN(eps=eps, min_samples=5)
print(f"DBSCAN eps estimated: {eps:.4f}")
# [ELSE IF HIERARCHICAL]
model = AgglomerativeClustering(
    n_clusters=n_clusters,
    linkage='ward'
)
# [ELSE IF GMM]
model = GaussianMixture(
    n_components=n_clusters,
    random_state=[RANDOM_STATE],
    covariance_type='full'
)
# [END IF]

# Fit model and get labels
if hasattr(model, 'fit_predict'):
    cluster_labels = model.fit_predict(X_processed)
else:
    model.fit(X_processed)
    cluster_labels = model.predict(X_processed)

# [IF AUTO MODE - COMPARE MULTIPLE CLUSTERING MODELS]
print("\nComparing clustering algorithms...")

clustering_models = {
    'K-Means': KMeans(n_clusters=n_clusters, random_state=[RANDOM_STATE], n_init=10),
    'Hierarchical': AgglomerativeClustering(n_clusters=n_clusters, linkage='ward'),
    'GMM': GaussianMixture(n_components=n_clusters, random_state=[RANDOM_STATE]),
}

comparison_results = []

for name, clusterer in clustering_models.items():
    print(f"  Fitting {name}...")
    if hasattr(clusterer, 'fit_predict'):
        labels = clusterer.fit_predict(X_processed)
    else:
        clusterer.fit(X_processed)
        labels = clusterer.predict(X_processed)
    
    # Skip if only one cluster found
    n_unique = len(np.unique(labels[labels >= 0]))  # Exclude noise (-1) for DBSCAN
    if n_unique < 2:
        print(f"    {name} found only {n_unique} cluster(s), skipping metrics")
        continue
    
    sil = silhouette_score(X_processed, labels)
    cal = calinski_harabasz_score(X_processed, labels)
    dav = davies_bouldin_score(X_processed, labels)
    
    comparison_results.append({
        'Algorithm': name,
        'Silhouette': sil,
        'Calinski-Harabasz': cal,
        'Davies-Bouldin': dav,
        'N Clusters': n_unique
    })

comparison_df = pd.DataFrame(comparison_results).sort_values('Silhouette', ascending=False)
print("\n=== CLUSTERING COMPARISON RESULTS ===")
print(comparison_df.to_string(index=False))

# Select best model
best_model_name = comparison_df.iloc[0]['Algorithm']
model = clustering_models[best_model_name]
if hasattr(model, 'fit_predict'):
    cluster_labels = model.fit_predict(X_processed)
else:
    model.fit(X_processed)
    cluster_labels = model.predict(X_processed)
print(f"\n✓ Best Algorithm: {best_model_name} (Silhouette: {comparison_df.iloc[0]['Silhouette']:.4f})")
# [END IF]

# Add cluster labels to original dataframe
df['Cluster'] = cluster_labels
```

### Section 6B: Cluster Evaluation (Clustering)

```python
# --- CLUSTER EVALUATION ---
print("\n" + "-" * 70)
print("CLUSTER EVALUATION")
print("-" * 70)

# Handle potential noise points in DBSCAN
valid_mask = cluster_labels >= 0
n_noise = (cluster_labels == -1).sum()

print(f"\nNumber of clusters found: {len(np.unique(cluster_labels[valid_mask]))}")
if n_noise > 0:
    print(f"Noise points (outliers): {n_noise} ({n_noise/len(cluster_labels)*100:.1f}%)")

# Clustering metrics (only if we have valid clusters)
if len(np.unique(cluster_labels[valid_mask])) > 1:
    print("\n=== CLUSTERING QUALITY METRICS ===")
    print(f"  Silhouette Score:      {silhouette_score(X_processed[valid_mask], cluster_labels[valid_mask]):.4f}")
    print(f"    (Range: -1 to 1, higher is better, >0.5 is good)")
    print(f"  Calinski-Harabasz:     {calinski_harabasz_score(X_processed[valid_mask], cluster_labels[valid_mask]):.2f}")
    print(f"    (Higher is better, indicates dense & well-separated clusters)")
    print(f"  Davies-Bouldin Index:  {davies_bouldin_score(X_processed[valid_mask], cluster_labels[valid_mask]):.4f}")
    print(f"    (Lower is better, 0 is perfect separation)")

# Cluster sizes
print("\n=== CLUSTER SIZES ===")
cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()
for cluster_id, size in cluster_sizes.items():
    label = "Noise" if cluster_id == -1 else f"Cluster {cluster_id}"
    print(f"  {label}: {size} samples ({size/len(cluster_labels)*100:.1f}%)")

# Cluster profiles (mean feature values per cluster)
print("\n=== CLUSTER PROFILES ===")
print("(Mean values of original features per cluster)\n")

# Use original features for interpretability
cluster_profiles = df.groupby('Cluster')[feature_columns].mean()
print(cluster_profiles.round(2).to_string())

# Identify distinguishing features per cluster
print("\n=== DISTINGUISHING CHARACTERISTICS ===")
overall_means = df[feature_columns].mean()
for cluster_id in sorted(df['Cluster'].unique()):
    if cluster_id == -1:
        continue
    cluster_data = df[df['Cluster'] == cluster_id][feature_columns].mean()
    deviations = ((cluster_data - overall_means) / overall_means.replace(0, 1) * 100).sort_values(ascending=False)
    
    print(f"\nCluster {cluster_id}:")
    top_high = deviations.head(3)
    top_low = deviations.tail(3)
    for feat, dev in top_high.items():
        if dev > 10:
            print(f"  ↑ {feat}: {dev:+.1f}% above average")
    for feat, dev in top_low.items():
        if dev < -10:
            print(f"  ↓ {feat}: {dev:+.1f}% below average")
```

### Section 7B: Visualizations (Clustering)

```python
# --- VISUALIZATIONS: CLUSTERING ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

fig = plt.figure(figsize=(16, 12))

# Plot 1: Cluster Visualization (PCA 2D projection)
ax1 = fig.add_subplot(2, 2, 1)
pca_2d = PCA(n_components=2, random_state=[RANDOM_STATE])
X_pca = pca_2d.fit_transform(X_processed)

scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='tab10', 
                      alpha=0.6, edgecolors='k', linewidths=0.5, s=50)
ax1.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=11)
ax1.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=11)
ax1.set_title('Clusters in PCA Space', fontsize=12)
plt.colorbar(scatter, ax=ax1, label='Cluster')

# Plot 2: Cluster sizes
ax2 = fig.add_subplot(2, 2, 2)
cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
colors = plt.cm.tab10(np.linspace(0, 1, len(cluster_counts)))
bars = ax2.bar(cluster_counts.index.astype(str), cluster_counts.values, color=colors, edgecolor='k')
ax2.set_xlabel('Cluster', fontsize=11)
ax2.set_ylabel('Number of Samples', fontsize=11)
ax2.set_title('Cluster Size Distribution', fontsize=12)
for bar, count in zip(bars, cluster_counts.values):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
             str(count), ha='center', va='bottom', fontsize=10)

# Plot 3: Elbow Plot and Silhouette Score (if computed earlier)
ax3 = fig.add_subplot(2, 2, 3)
if 'silhouette_scores' in dir():
    ax3_twin = ax3.twinx()
    line1, = ax3.plot(list(k_range), inertia_scores, 'b-o', linewidth=2, markersize=8, label='Inertia (Elbow)')
    line2, = ax3_twin.plot(list(k_range), silhouette_scores, 'r-s', linewidth=2, markersize=8, label='Silhouette')
    ax3.set_xlabel('Number of Clusters (k)', fontsize=11)
    ax3.set_ylabel('Inertia', fontsize=11, color='blue')
    ax3_twin.set_ylabel('Silhouette Score', fontsize=11, color='red')
    ax3.set_title('Elbow Method & Silhouette Analysis', fontsize=12)
    ax3.axvline(x=n_clusters, color='green', linestyle='--', linewidth=2, label=f'Selected k={n_clusters}')
    lines = [line1, line2]
    labels = ['Inertia', 'Silhouette']
    ax3.legend(lines, labels, loc='center right')
else:
    ax3.text(0.5, 0.5, 'Cluster optimization\nnot performed', ha='center', va='center', fontsize=12)
    ax3.set_title('Elbow Method & Silhouette Analysis', fontsize=12)

# Plot 4: Cluster Heatmap (feature means per cluster)
ax4 = fig.add_subplot(2, 2, 4)
# Normalize for heatmap
cluster_profiles_norm = (cluster_profiles - cluster_profiles.mean()) / cluster_profiles.std()
sns.heatmap(cluster_profiles_norm.T, annot=False, cmap='RdYlBu_r', center=0,
            xticklabels=[f'C{i}' for i in cluster_profiles.index],
            yticklabels=cluster_profiles.columns, ax=ax4, cbar_kws={'label': 'Z-score'})
ax4.set_xlabel('Cluster', fontsize=11)
ax4.set_ylabel('Feature', fontsize=11)
ax4.set_title('Cluster Feature Profiles (Normalized)', fontsize=12)

plt.tight_layout()
plt.savefig('clustering_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'clustering_analysis.png'")

# Optional: Dendrogram for hierarchical clustering
if '[MODEL_TYPE]' == 'Hierarchical' or '[MODEL_TYPE]' == 'auto':
    print("\nGenerating dendrogram...")
    fig_dendro, ax_dendro = plt.subplots(figsize=(12, 6))
    
    # Compute linkage matrix
    linkage_matrix = linkage(X_processed, method='ward')
    
    # Plot dendrogram
    dendrogram(linkage_matrix, ax=ax_dendro, truncate_mode='lastp', p=30,
               leaf_rotation=90, leaf_font_size=10, show_contracted=True)
    ax_dendro.set_xlabel('Sample Index (or cluster size)', fontsize=11)
    ax_dendro.set_ylabel('Distance', fontsize=11)
    ax_dendro.set_title('Hierarchical Clustering Dendrogram', fontsize=12)
    ax_dendro.axhline(y=linkage_matrix[-n_clusters+1, 2], color='r', linestyle='--', 
                      label=f'Cut for {n_clusters} clusters')
    ax_dendro.legend()
    
    plt.tight_layout()
    plt.savefig('dendrogram.png', dpi=150, bbox_inches='tight')
    plt.show()
    print("Dendrogram saved as 'dendrogram.png'")
```

---

## DIMENSIONALITY REDUCTION-SPECIFIC SECTIONS

### Section 5C: Dimensionality Reduction

```python
# --- DIMENSIONALITY REDUCTION ---
print("\n" + "-" * 70)
print("DIMENSIONALITY REDUCTION")
print("-" * 70)

# Preprocess data
X_processed = preprocessor.fit_transform(X)
print(f"\nOriginal dimensionality: {X_processed.shape[1]} features")

# [IF PCA]
# Perform PCA
# [IF n_components == "auto"]
# Determine components needed for desired variance
pca_full = PCA(random_state=[RANDOM_STATE])
pca_full.fit(X_processed)

cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
n_components = np.argmax(cumulative_variance >= 0.95) + 1  # 95% variance threshold
print(f"Components for 95% variance: {n_components}")
# [END IF]

pca = PCA(n_components=n_components, random_state=[RANDOM_STATE])
X_reduced = pca.fit_transform(X_processed)

print(f"\n=== PCA RESULTS ===")
print(f"Reduced dimensionality: {X_reduced.shape[1]} components")
print(f"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.2f}%")

print("\nVariance explained per component:")
for i, var in enumerate(pca.explained_variance_ratio_):
    cumulative = sum(pca.explained_variance_ratio_[:i+1])
    print(f"  PC{i+1}: {var*100:.2f}% (cumulative: {cumulative*100:.2f}%)")

# Component loadings (contribution of each original feature)
print("\n=== PRINCIPAL COMPONENT LOADINGS ===")
print("(Contribution of each feature to each component)\n")
loadings = pd.DataFrame(
    pca.components_.T,
    columns=[f'PC{i+1}' for i in range(n_components)],
    index=feature_names_processed
)
print(loadings.round(3).to_string())

# Top features per component
print("\n=== TOP FEATURES PER COMPONENT ===")
for i in range(min(n_components, 5)):  # Show up to 5 components
    component = loadings[f'PC{i+1}'].abs().sort_values(ascending=False)
    print(f"\nPC{i+1} (explains {pca.explained_variance_ratio_[i]*100:.1f}% variance):")
    for feat, loading in component.head(5).items():
        direction = '+' if loadings.loc[feat, f'PC{i+1}'] > 0 else '-'
        print(f"  {direction} {feat}: {loading:.3f}")

# [ELSE IF t-SNE]
# Perform t-SNE
print("\nPerforming t-SNE (this may take a moment)...")
tsne = TSNE(
    n_components=[N_COMPONENTS],  # Usually 2 or 3 for visualization
    perplexity=min(30, len(X_processed) - 1),
    learning_rate='auto',
    init='pca',
    random_state=[RANDOM_STATE],
    n_iter=1000
)
X_reduced = tsne.fit_transform(X_processed)

print(f"\n=== t-SNE RESULTS ===")
print(f"Reduced to {X_reduced.shape[1]} dimensions for visualization")
print(f"Final KL divergence: {tsne.kl_divergence_:.4f}")
print("\nNote: t-SNE is for visualization only, not for preprocessing or distance preservation.")

# [ELSE IF UMAP]
if UMAP_AVAILABLE:
    print("\nPerforming UMAP...")
    reducer = umap.UMAP(
        n_components=[N_COMPONENTS],
        n_neighbors=15,
        min_dist=0.1,
        metric='euclidean',
        random_state=[RANDOM_STATE]
    )
    X_reduced = reducer.fit_transform(X_processed)
    
    print(f"\n=== UMAP RESULTS ===")
    print(f"Reduced to {X_reduced.shape[1]} dimensions")
else:
    print("UMAP not available. Falling back to t-SNE...")
    # Use t-SNE code above

# [ELSE IF LDA]
# Linear Discriminant Analysis (supervised)
print("\nPerforming LDA (supervised dimensionality reduction)...")
lda = LinearDiscriminantAnalysis(n_components=min(n_components, len(np.unique(y)) - 1))
X_reduced = lda.fit_transform(X_processed, y)

print(f"\n=== LDA RESULTS ===")
print(f"Reduced to {X_reduced.shape[1]} components")
print(f"Total variance explained: {sum(lda.explained_variance_ratio_)*100:.2f}%")
# [END IF]

# Create reduced dataframe
reduced_columns = [f'Component_{i+1}' for i in range(X_reduced.shape[1])]
df_reduced = pd.DataFrame(X_reduced, columns=reduced_columns)
```

### Section 6C: Visualizations (Dimensionality Reduction)

```python
# --- VISUALIZATIONS: DIMENSIONALITY REDUCTION ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

# [IF PCA]
fig = plt.figure(figsize=(16, 12))

# Plot 1: Scree Plot (Variance Explained)
ax1 = fig.add_subplot(2, 2, 1)
explained_var = pca.explained_variance_ratio_
cumulative_var = np.cumsum(explained_var)
x_vals = range(1, len(explained_var) + 1)

ax1.bar(x_vals, explained_var * 100, alpha=0.7, label='Individual', color='steelblue', edgecolor='k')
ax1.plot(x_vals, cumulative_var * 100, 'ro-', linewidth=2, markersize=8, label='Cumulative')
ax1.axhline(y=95, color='green', linestyle='--', linewidth=1.5, label='95% threshold')
ax1.set_xlabel('Principal Component', fontsize=11)
ax1.set_ylabel('Variance Explained (%)', fontsize=11)
ax1.set_title('Scree Plot: Variance Explained by Component', fontsize=12)
ax1.legend(loc='center right')
ax1.set_xticks(x_vals)

# Plot 2: 2D Projection
ax2 = fig.add_subplot(2, 2, 2)
if '[HAS_TARGET]':
    scatter = ax2.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='tab10', 
                         alpha=0.6, edgecolors='k', linewidths=0.5, s=50)
    plt.colorbar(scatter, ax=ax2, label='Class')
else:
    ax2.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.6, edgecolors='k', 
               linewidths=0.5, s=50, color='steelblue')
ax2.set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)', fontsize=11)
ax2.set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)', fontsize=11)
ax2.set_title('Data Projected onto First Two Principal Components', fontsize=12)

# Plot 3: Component Loadings Heatmap
ax3 = fig.add_subplot(2, 2, 3)
n_show = min(15, len(feature_names_processed))  # Show top 15 features
top_features_idx = np.argsort(np.abs(loadings.iloc[:, :3]).max(axis=1))[-n_show:]
loadings_subset = loadings.iloc[top_features_idx, :min(5, n_components)]
sns.heatmap(loadings_subset, annot=True, fmt='.2f', cmap='RdBu_r', center=0, ax=ax3,
            cbar_kws={'label': 'Loading'})
ax3.set_xlabel('Principal Component', fontsize=11)
ax3.set_ylabel('Feature', fontsize=11)
ax3.set_title('Component Loadings (Top Features)', fontsize=12)

# Plot 4: Biplot (optional - shows both samples and feature vectors)
ax4 = fig.add_subplot(2, 2, 4)
# Plot samples
ax4.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.3, s=30, color='gray')
# Plot feature vectors
scale = 3  # Scaling factor for arrows
for i, feature in enumerate(feature_names_processed[:10]):  # Top 10 features
    ax4.arrow(0, 0, pca.components_[0, i] * scale, pca.components_[1, i] * scale,
              head_width=0.1, head_length=0.05, fc='red', ec='red', alpha=0.7)
    ax4.text(pca.components_[0, i] * scale * 1.1, pca.components_[1, i] * scale * 1.1,
             feature, fontsize=8, ha='center')
ax4.set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)', fontsize=11)
ax4.set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)', fontsize=11)
ax4.set_title('Biplot: Samples and Feature Vectors', fontsize=12)
ax4.axhline(y=0, color='k', linewidth=0.5)
ax4.axvline(x=0, color='k', linewidth=0.5)

# [ELSE IF t-SNE or UMAP]
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: 2D Visualization
ax1 = axes[0]
if '[HAS_TARGET]':
    scatter = ax1.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='tab10',
                         alpha=0.6, edgecolors='k', linewidths=0.5, s=50)
    plt.colorbar(scatter, ax=ax1, label='Class')
else:
    ax1.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.6, edgecolors='k',
               linewidths=0.5, s=50, color='steelblue')
ax1.set_xlabel('[METHOD] Dimension 1', fontsize=11)
ax1.set_ylabel('[METHOD] Dimension 2', fontsize=11)
ax1.set_title('[METHOD] 2D Visualization', fontsize=12)

# Plot 2: Density plot
ax2 = axes[1]
sns.kdeplot(x=X_reduced[:, 0], y=X_reduced[:, 1], ax=ax2, cmap='Blues', fill=True, levels=20)
ax2.scatter(X_reduced[:, 0], X_reduced[:, 1], alpha=0.3, s=10, color='red')
ax2.set_xlabel('[METHOD] Dimension 1', fontsize=11)
ax2.set_ylabel('[METHOD] Dimension 2', fontsize=11)
ax2.set_title('Density Visualization', fontsize=12)
# [END IF]

plt.tight_layout()
plt.savefig('dimensionality_reduction.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'dimensionality_reduction.png'")
```

---

## SHARED SECTIONS (ALL PROBLEM TYPES)

### Section 9: Prediction Function

```python
# --- PREDICTION / TRANSFORM FUNCTION ---

# [IF CLASSIFICATION]
def predict_new(feature_dict):
    """
    Predict class for new sample(s).
    
    Parameters:
    -----------
    feature_dict : dict
        Dictionary with feature names as keys and values as... values.
        Example: {'Age': 35, 'Income': 75000, 'Education': 'Bachelor'}
    
    Returns:
    --------
    prediction : str or int
        Predicted class label
    probabilities : dict
        Probability for each class (if available)
    """
    # Create DataFrame from input
    new_data = pd.DataFrame([feature_dict])
    
    # Ensure correct column order
    new_data = new_data[feature_columns]
    
    # Preprocess
    new_processed = preprocessor.transform(new_data)
    
    # Predict
    prediction = model.predict(new_processed)[0]
    
    # Get probabilities if available
    if hasattr(model, 'predict_proba'):
        proba = model.predict_proba(new_processed)[0]
        probabilities = dict(zip(class_names, proba))
    else:
        probabilities = None
    
    # Decode label if encoded
    if 'label_encoder' in dir():
        prediction = label_encoder.inverse_transform([prediction])[0]
    
    return prediction, probabilities

# [ELSE IF CLUSTERING]
def assign_cluster(feature_dict):
    """
    Assign cluster to new sample(s).
    
    Parameters:
    -----------
    feature_dict : dict
        Dictionary with feature names as keys
    
    Returns:
    --------
    cluster : int
        Assigned cluster label
    """
    new_data = pd.DataFrame([feature_dict])
    new_data = new_data[feature_columns]
    new_processed = preprocessor.transform(new_data)
    
    if hasattr(model, 'predict'):
        cluster = model.predict(new_processed)[0]
    else:
        # For DBSCAN and some others, find nearest cluster centroid
        # This is an approximation
        cluster = model.fit_predict(np.vstack([X_processed, new_processed]))[-1]
    
    return cluster

# [ELSE IF DIMENSIONALITY_REDUCTION]
def transform_new(feature_dict):
    """
    Transform new sample(s) to reduced space.
    
    Parameters:
    -----------
    feature_dict : dict
        Dictionary with feature names as keys
    
    Returns:
    --------
    reduced : array
        Transformed coordinates in reduced space
    """
    new_data = pd.DataFrame([feature_dict])
    new_data = new_data[feature_columns]
    new_processed = preprocessor.transform(new_data)
    
    # [IF PCA or LDA]
    reduced = [reducer].transform(new_processed)
    # [ELSE IF t-SNE or UMAP]
    # Note: t-SNE doesn't support transform on new data well
    # For production, use PCA or UMAP which support transform
    print("Warning: t-SNE doesn't support transform() on new data.")
    print("Consider using PCA or UMAP for preprocessing pipelines.")
    reduced = None
    # [END IF]
    
    return reduced
# [END IF]
```

### Section 10: Model Persistence

```python
# --- SAVE MODEL AND ARTIFACTS ---
print("\n" + "-" * 70)
print("SAVING MODEL")
print("-" * 70)

import joblib
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save main model
model_filename = f'ml_model_{timestamp}.pkl'
joblib.dump(model, model_filename)
print(f"Model saved: {model_filename}")

# Save preprocessor
preprocessor_filename = f'preprocessor_{timestamp}.pkl'
joblib.dump(preprocessor, preprocessor_filename)
print(f"Preprocessor saved: {preprocessor_filename}")

# [IF CLASSIFICATION with label_encoder]
encoder_filename = f'label_encoder_{timestamp}.pkl'
joblib.dump(label_encoder, encoder_filename)
print(f"Label encoder saved: {encoder_filename}")
# [END IF]

# [IF DIMENSIONALITY_REDUCTION]
reducer_filename = f'reducer_{timestamp}.pkl'
joblib.dump([pca/tsne/umap/lda], reducer_filename)
print(f"Reducer saved: {reducer_filename}")
# [END IF]

# Save feature names for reference
import json
config = {
    'feature_columns': feature_columns,
    'numeric_features': numeric_features,
    'categorical_features': categorical_features,
    'model_type': '[MODEL_TYPE]',
    'problem_type': '[PROBLEM_TYPE]',
    'timestamp': timestamp
}
# [IF CLASSIFICATION]
config['class_names'] = list(class_names) if hasattr(class_names, '__iter__') else class_names
# [END IF]

config_filename = f'model_config_{timestamp}.json'
with open(config_filename, 'w') as f:
    json.dump(config, f, indent=2)
print(f"Config saved: {config_filename}")

print("\n" + "=" * 70)
print("ANALYSIS COMPLETE")
print("=" * 70)

# --- LOADING INSTRUCTIONS ---
"""
To load and use this model later:

import joblib
import json
import pandas as pd

# Load artifacts
model = joblib.load('[model_filename]')
preprocessor = joblib.load('[preprocessor_filename]')
with open('[config_filename]', 'r') as f:
    config = json.load(f)

# Prepare new data (dict or DataFrame)
new_sample = {
    'Feature1': value1,
    'Feature2': value2,
    ...
}
new_df = pd.DataFrame([new_sample])[config['feature_columns']]

# Preprocess and predict
new_processed = preprocessor.transform(new_df)
prediction = model.predict(new_processed)
print(f"Prediction: {prediction}")
"""
```

---

## HYPERPARAMETER GRIDS BY MODEL

### Classification Models

```python
# Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

# XGBoost
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# SVM
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['rbf', 'poly', 'sigmoid'],
    'gamma': ['scale', 'auto', 0.01, 0.1]
}

# MLP Neural Network
param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100), (100, 50, 25)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'adaptive']
}

# KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 11, 15],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Gradient Boosting
param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.6, 0.8, 1.0]
}
```

### Clustering Models

```python
# K-Means
param_grid_kmeans = {
    'n_clusters': range(2, 11),
    'init': ['k-means++', 'random'],
    'n_init': [10, 20, 30]
}

# DBSCAN
param_grid_dbscan = {
    'eps': np.arange(0.1, 2.0, 0.1),
    'min_samples': [3, 5, 10, 15, 20]
}

# Hierarchical
param_grid_hierarchical = {
    'n_clusters': range(2, 11),
    'linkage': ['ward', 'complete', 'average', 'single']
}

# GMM
param_grid_gmm = {
    'n_components': range(2, 11),
    'covariance_type': ['full', 'tied', 'diag', 'spherical']
}
```

---

## ERROR HANDLING

```python
# At start of script
try:
    # Data loading
except FileNotFoundError as e:
    print(f"ERROR: Data file not found - {e}")
    print("Please check the file path and try again.")
    raise
except pd.errors.EmptyDataError:
    print("ERROR: The data file is empty.")
    raise

# Before model training
if len(X) < 30:
    print("⚠️  WARNING: Very small dataset (<30 samples). Results may not be reliable.")
if X.shape[1] > X.shape[0]:
    print("⚠️  WARNING: More features than samples. Consider dimensionality reduction first.")

# For classification
if len(np.unique(y)) < 2:
    raise ValueError("ERROR: Target variable has only one unique value. Cannot train classifier.")
if len(np.unique(y)) > 100:
    print("⚠️  WARNING: Many unique classes (>100). This may be a regression problem, not classification.")

# Class imbalance check
class_counts = pd.Series(y).value_counts()
imbalance_ratio = class_counts.min() / class_counts.max()
if imbalance_ratio < 0.2:
    print(f"⚠️  WARNING: Severe class imbalance detected (ratio: {imbalance_ratio:.2%})")
    print("   Consider using SMOTE, class_weight='balanced', or resampling techniques.")

# For clustering
if len(X) < n_clusters * 10:
    print(f"⚠️  WARNING: Few samples per cluster (~{len(X)//n_clusters}). Consider fewer clusters.")
```

---

## FILE DELIVERY

- Generate file named: `ml_model_[PROBLEM_TYPE]_[TIMESTAMP].py`
- Timestamp format: YYYYMMDD_HHMMSS
- Provide as downloadable file
- All code must be syntactically valid Python 3.8+
- Required libraries clearly listed at top
- Include pip install instructions in docstring if non-standard libraries needed

---

## EXAMPLE INPUT

```
Build me a machine learning model to predict customer churn. This is a classification 
problem where I want to predict whether a customer will leave (churn=1) or stay (churn=0).

The features I have are: tenure (months as customer), monthly_charges, total_charges, 
contract_type (Month-to-month, One year, Two year), payment_method (Electronic check, 
Mailed check, Bank transfer, Credit card), internet_service (DSL, Fiber optic, No), 
online_security (Yes, No), tech_support (Yes, No), and streaming_tv (Yes, No).

Generate synthetic data for 5000 customers with realistic distributions. The churn rate 
should be around 25-30%. Month-to-month contracts should have much higher churn than 
long-term contracts. Customers with fiber optic internet and no tech support should 
have elevated churn.

Compare Random Forest, XGBoost, and Gradient Boosting to find the best model. Use 
random search for hyperparameter tuning with 30 iterations. Use 5-fold cross-validation.
Handle the class imbalance appropriately.

Split the data 80/20 for training and testing. Output as a Python script file.
```

---

## EXAMPLE OUTPUT

File: `ml_model_classification_20251203_152847.py`

```python
"""
Machine Learning Model: Classification (Model Comparison)
Problem Type: Binary Classification
Generated by ModelWiz.xyz
Target Variable: churn
Features: ['tenure', 'monthly_charges', 'total_charges', 'contract_type', 
           'payment_method', 'internet_service', 'online_security', 
           'tech_support', 'streaming_tv']
Generated on: 2025-12-03 15:28:47

Required packages:
pip install numpy pandas scikit-learn matplotlib seaborn xgboost imbalanced-learn
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_auc_score,
                             roc_curve)
import warnings
warnings.filterwarnings('ignore')

# XGBoost
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("Note: XGBoost not installed. Install with: pip install xgboost")

# Imbalanced-learn for SMOTE
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("Note: imbalanced-learn not installed. Using class_weight='balanced' instead.")
    print("Install with: pip install imbalanced-learn")

print("=" * 70)
print("CUSTOMER CHURN PREDICTION MODEL")
print("=" * 70)

# --- DATA GENERATION ---
print("\n" + "-" * 70)
print("GENERATING SYNTHETIC DATA")
print("-" * 70)

np.random.seed(42)
n_samples = 5000

# Generate features with realistic distributions
tenure = np.random.exponential(scale=24, size=n_samples).clip(1, 72).astype(int)
monthly_charges = np.random.normal(65, 30, n_samples).clip(20, 120)
total_charges = tenure * monthly_charges * np.random.uniform(0.9, 1.1, n_samples)

contract_type = np.random.choice(
    ['Month-to-month', 'One year', 'Two year'],
    size=n_samples,
    p=[0.55, 0.25, 0.20]
)

payment_method = np.random.choice(
    ['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],
    size=n_samples,
    p=[0.35, 0.20, 0.22, 0.23]
)

internet_service = np.random.choice(
    ['DSL', 'Fiber optic', 'No'],
    size=n_samples,
    p=[0.35, 0.45, 0.20]
)

online_security = np.random.choice(['Yes', 'No'], size=n_samples, p=[0.40, 0.60])
tech_support = np.random.choice(['Yes', 'No'], size=n_samples, p=[0.35, 0.65])
streaming_tv = np.random.choice(['Yes', 'No'], size=n_samples, p=[0.45, 0.55])

# Generate churn with realistic correlations
churn_prob = np.zeros(n_samples)

# Base churn rate ~15%
churn_prob += 0.15

# Month-to-month contracts have MUCH higher churn
churn_prob += np.where(contract_type == 'Month-to-month', 0.25, 0)
churn_prob += np.where(contract_type == 'One year', 0.05, 0)
churn_prob += np.where(contract_type == 'Two year', -0.05, 0)

# Fiber optic with no tech support = elevated churn
fiber_no_support = (internet_service == 'Fiber optic') & (tech_support == 'No')
churn_prob += np.where(fiber_no_support, 0.15, 0)

# Electronic check payment = slightly higher churn
churn_prob += np.where(payment_method == 'Electronic check', 0.08, 0)

# Low tenure = higher churn (new customers exploring)
churn_prob += np.where(tenure < 12, 0.10, 0)
churn_prob += np.where(tenure > 48, -0.08, 0)

# High monthly charges = slightly higher churn
churn_prob += (monthly_charges - 65) / 300

# No online security = higher churn
churn_prob += np.where(online_security == 'No', 0.05, 0)

# Clip probabilities and generate churn
churn_prob = churn_prob.clip(0.05, 0.85)
churn = (np.random.random(n_samples) < churn_prob).astype(int)

# Create DataFrame
df = pd.DataFrame({
    'tenure': tenure,
    'monthly_charges': monthly_charges.round(2),
    'total_charges': total_charges.round(2),
    'contract_type': contract_type,
    'payment_method': payment_method,
    'internet_service': internet_service,
    'online_security': online_security,
    'tech_support': tech_support,
    'streaming_tv': streaming_tv,
    'churn': churn
})

print(f"\nDataset generated: {df.shape[0]} customers, {df.shape[1]} columns")
print(f"Churn rate: {df['churn'].mean()*100:.1f}%")

# --- EXPLORATORY DATA ANALYSIS ---
print("\n" + "-" * 70)
print("EXPLORATORY DATA ANALYSIS")
print("-" * 70)

print("\nFirst 5 Rows:")
print(df.head())

print("\nDescriptive Statistics (Numeric):")
print(df.describe().round(2))

print("\nTarget Variable Distribution:")
print(df['churn'].value_counts())
print(f"\nClass Balance: {df['churn'].value_counts()[0]/df['churn'].value_counts()[1]:.2f}:1 (Retained:Churned)")

print("\nChurn Rate by Contract Type:")
print(df.groupby('contract_type')['churn'].mean().round(3).sort_values(ascending=False))

print("\nChurn Rate by Internet Service + Tech Support:")
print(df.groupby(['internet_service', 'tech_support'])['churn'].mean().round(3))

# --- DATA PREPROCESSING ---
print("\n" + "-" * 70)
print("DATA PREPROCESSING")
print("-" * 70)

feature_columns = ['tenure', 'monthly_charges', 'total_charges', 'contract_type',
                   'payment_method', 'internet_service', 'online_security',
                   'tech_support', 'streaming_tv']
target_column = 'churn'

numeric_features = ['tenure', 'monthly_charges', 'total_charges']
categorical_features = ['contract_type', 'payment_method', 'internet_service',
                        'online_security', 'tech_support', 'streaming_tv']

print(f"\nNumeric features ({len(numeric_features)}): {numeric_features}")
print(f"Categorical features ({len(categorical_features)}): {categorical_features}")

# Build preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'
)

# Prepare data
X = df[feature_columns]
y = df[target_column]
class_names = ['Retained', 'Churned']

# Train-test split (stratified to maintain class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining set: {len(X_train)} samples (Churn rate: {y_train.mean()*100:.1f}%)")
print(f"Test set: {len(X_test)} samples (Churn rate: {y_test.mean()*100:.1f}%)")

# Fit preprocessor
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Get feature names after preprocessing
feature_names_processed = numeric_features.copy()
ohe = preprocessor.named_transformers_['cat'].named_steps['encoder']
cat_feature_names = ohe.get_feature_names_out(categorical_features).tolist()
feature_names_processed.extend(cat_feature_names)

print(f"Features after one-hot encoding: {X_train_processed.shape[1]}")

# Handle class imbalance with SMOTE
if IMBLEARN_AVAILABLE:
    print("\nApplying SMOTE to handle class imbalance...")
    smote = SMOTE(random_state=42)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)
    print(f"Training set after SMOTE: {len(X_train_balanced)} samples")
    print(f"Class distribution after SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}")
else:
    X_train_balanced, y_train_balanced = X_train_processed, y_train
    print("\nUsing class_weight='balanced' for imbalance handling")

# --- MODEL COMPARISON ---
print("\n" + "-" * 70)
print("MODEL COMPARISON WITH HYPERPARAMETER TUNING")
print("-" * 70)

# Define models and their hyperparameter grids
models_config = {
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42, class_weight='balanced'),
        'params': {
            'n_estimators': [50, 100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2']
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'min_samples_split': [2, 5, 10],
            'subsample': [0.6, 0.8, 1.0]
        }
    }
}

if XGBOOST_AVAILABLE:
    models_config['XGBoost'] = {
        'model': XGBClassifier(random_state=42, use_label_encoder=False, 
                               eval_metric='logloss', scale_pos_weight=3),
        'params': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'subsample': [0.6, 0.8, 1.0],
            'colsample_bytree': [0.6, 0.8, 1.0]
        }
    }

# Run hyperparameter tuning for each model
best_models = {}
comparison_results = []

for name, config in models_config.items():
    print(f"\n{'='*50}")
    print(f"Tuning {name}...")
    print('='*50)
    
    search = RandomizedSearchCV(
        config['model'],
        config['params'],
        n_iter=30,
        cv=5,
        scoring='f1',
        n_jobs=-1,
        verbose=1,
        random_state=42
    )
    
    search.fit(X_train_balanced, y_train_balanced)
    
    best_models[name] = search.best_estimator_
    
    # Evaluate on test set
    y_pred = search.best_estimator_.predict(X_test_processed)
    y_proba = search.best_estimator_.predict_proba(X_test_processed)[:, 1]
    
    results = {
        'Model': name,
        'Best CV F1': search.best_score_,
        'Test Accuracy': accuracy_score(y_test, y_pred),
        'Test Precision': precision_score(y_test, y_pred),
        'Test Recall': recall_score(y_test, y_pred),
        'Test F1': f1_score(y_test, y_pred),
        'Test ROC-AUC': roc_auc_score(y_test, y_proba)
    }
    comparison_results.append(results)
    
    print(f"\nBest Parameters: {search.best_params_}")
    print(f"Best CV F1 Score: {search.best_score_:.4f}")
    print(f"Test F1 Score: {results['Test F1']:.4f}")

# Comparison summary
comparison_df = pd.DataFrame(comparison_results)
comparison_df = comparison_df.sort_values('Test F1', ascending=False)

print("\n" + "=" * 70)
print("MODEL COMPARISON SUMMARY")
print("=" * 70)
print(comparison_df.round(4).to_string(index=False))

# Select best model
best_model_name = comparison_df.iloc[0]['Model']
model = best_models[best_model_name]
print(f"\n✓ BEST MODEL: {best_model_name}")
print(f"  Test F1 Score: {comparison_df.iloc[0]['Test F1']:.4f}")
print(f"  Test ROC-AUC: {comparison_df.iloc[0]['Test ROC-AUC']:.4f}")

# --- DETAILED EVALUATION OF BEST MODEL ---
print("\n" + "-" * 70)
print(f"DETAILED EVALUATION: {best_model_name}")
print("-" * 70)

y_pred = model.predict(X_test_processed)
y_proba = model.predict_proba(X_test_processed)[:, 1]

print("\n=== CLASSIFICATION REPORT ===")
print(classification_report(y_test, y_pred, target_names=class_names))

print("\n=== CONFUSION MATRIX ===")
cm = confusion_matrix(y_test, y_pred)
cm_df = pd.DataFrame(cm, 
                      index=['Actual: Retained', 'Actual: Churned'],
                      columns=['Pred: Retained', 'Pred: Churned'])
print(cm_df)

# Business metrics
tn, fp, fn, tp = cm.ravel()
print(f"\n=== BUSINESS METRICS ===")
print(f"True Positives (Correctly predicted churn): {tp}")
print(f"False Positives (Incorrectly predicted churn): {fp}")
print(f"True Negatives (Correctly predicted retention): {tn}")
print(f"False Negatives (Missed churners): {fn}")
print(f"\nChurn Detection Rate: {tp/(tp+fn)*100:.1f}% of churners identified")
print(f"False Alarm Rate: {fp/(fp+tn)*100:.1f}% of retained customers flagged")

# --- FEATURE IMPORTANCE ---
print("\n" + "-" * 70)
print("FEATURE IMPORTANCE")
print("-" * 70)

importances = model.feature_importances_
importance_df = pd.DataFrame({
    'Feature': feature_names_processed,
    'Importance': importances
}).sort_values('Importance', ascending=False)

print("\n=== TOP 15 MOST IMPORTANT FEATURES ===")
print(importance_df.head(15).to_string(index=False))

# --- VISUALIZATIONS ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

fig = plt.figure(figsize=(16, 12))

# Plot 1: Confusion Matrix Heatmap
ax1 = fig.add_subplot(2, 2, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, yticklabels=class_names, ax=ax1,
            annot_kws={'size': 14})
ax1.set_xlabel('Predicted Label', fontsize=12)
ax1.set_ylabel('True Label', fontsize=12)
ax1.set_title(f'Confusion Matrix - {best_model_name}', fontsize=13)

# Plot 2: Feature Importance
ax2 = fig.add_subplot(2, 2, 2)
top_features = importance_df.head(12)
colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(top_features)))
bars = ax2.barh(range(len(top_features)), top_features['Importance'].values, color=colors)
ax2.set_yticks(range(len(top_features)))
ax2.set_yticklabels(top_features['Feature'].values)
ax2.invert_yaxis()
ax2.set_xlabel('Importance', fontsize=12)
ax2.set_title('Top 12 Feature Importance', fontsize=13)

# Plot 3: ROC Curve
ax3 = fig.add_subplot(2, 2, 3)
for name, clf in best_models.items():
    y_proba_model = clf.predict_proba(X_test_processed)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba_model)
    auc = roc_auc_score(y_test, y_proba_model)
    ax3.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc:.3f})')

ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
ax3.set_xlabel('False Positive Rate', fontsize=12)
ax3.set_ylabel('True Positive Rate', fontsize=12)
ax3.set_title('ROC Curves - Model Comparison', fontsize=13)
ax3.legend(loc='lower right')
ax3.set_xlim([0, 1])
ax3.set_ylim([0, 1.05])

# Plot 4: Probability Distribution
ax4 = fig.add_subplot(2, 2, 4)
ax4.hist(y_proba[y_test == 0], bins=30, alpha=0.6, label='Retained', color='steelblue', edgecolor='k')
ax4.hist(y_proba[y_test == 1], bins=30, alpha=0.6, label='Churned', color='coral', edgecolor='k')
ax4.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold')
ax4.set_xlabel('Predicted Churn Probability', fontsize=12)
ax4.set_ylabel('Count', fontsize=12)
ax4.set_title('Probability Distribution by Actual Class', fontsize=13)
ax4.legend()

plt.tight_layout()
plt.savefig('churn_prediction_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'churn_prediction_analysis.png'")

# --- PREDICTION FUNCTION ---
def predict_churn(customer_data):
    """
    Predict churn probability for a customer.
    
    Parameters:
    -----------
    customer_data : dict
        Customer features. Example:
        {
            'tenure': 12,
            'monthly_charges': 75.50,
            'total_charges': 900.00,
            'contract_type': 'Month-to-month',
            'payment_method': 'Electronic check',
            'internet_service': 'Fiber optic',
            'online_security': 'No',
            'tech_support': 'No',
            'streaming_tv': 'Yes'
        }
    
    Returns:
    --------
    dict with:
        - prediction: 'Will Churn' or 'Will Stay'
        - churn_probability: float (0-1)
        - risk_level: 'Low', 'Medium', or 'High'
    """
    # Create DataFrame
    new_df = pd.DataFrame([customer_data])[feature_columns]
    
    # Preprocess
    new_processed = preprocessor.transform(new_df)
    
    # Predict
    prediction = model.predict(new_processed)[0]
    probability = model.predict_proba(new_processed)[0, 1]
    
    # Risk level
    if probability < 0.3:
        risk = 'Low'
    elif probability < 0.6:
        risk = 'Medium'
    else:
        risk = 'High'
    
    return {
        'prediction': 'Will Churn' if prediction == 1 else 'Will Stay',
        'churn_probability': round(probability, 4),
        'risk_level': risk
    }

# --- EXAMPLE PREDICTIONS ---
print("\n" + "-" * 70)
print("EXAMPLE PREDICTIONS")
print("-" * 70)

example_customers = [
    {
        'name': 'High Risk Customer',
        'tenure': 3,
        'monthly_charges': 95.00,
        'total_charges': 285.00,
        'contract_type': 'Month-to-month',
        'payment_method': 'Electronic check',
        'internet_service': 'Fiber optic',
        'online_security': 'No',
        'tech_support': 'No',
        'streaming_tv': 'Yes'
    },
    {
        'name': 'Low Risk Customer',
        'tenure': 48,
        'monthly_charges': 55.00,
        'total_charges': 2640.00,
        'contract_type': 'Two year',
        'payment_method': 'Bank transfer',
        'internet_service': 'DSL',
        'online_security': 'Yes',
        'tech_support': 'Yes',
        'streaming_tv': 'No'
    },
    {
        'name': 'Medium Risk Customer',
        'tenure': 18,
        'monthly_charges': 70.00,
        'total_charges': 1260.00,
        'contract_type': 'One year',
        'payment_method': 'Credit card',
        'internet_service': 'Fiber optic',
        'online_security': 'No',
        'tech_support': 'Yes',
        'streaming_tv': 'Yes'
    }
]

for customer in example_customers:
    name = customer.pop('name')
    result = predict_churn(customer)
    print(f"\n{name}:")
    print(f"  Prediction: {result['prediction']}")
    print(f"  Churn Probability: {result['churn_probability']*100:.1f}%")
    print(f"  Risk Level: {result['risk_level']}")

# --- SAVE MODEL ---
print("\n" + "-" * 70)
print("SAVING MODEL ARTIFACTS")
print("-" * 70)

import joblib
import json
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save model
model_filename = f'churn_model_{timestamp}.pkl'
joblib.dump(model, model_filename)
print(f"Model saved: {model_filename}")

# Save preprocessor
preprocessor_filename = f'churn_preprocessor_{timestamp}.pkl'
joblib.dump(preprocessor, preprocessor_filename)
print(f"Preprocessor saved: {preprocessor_filename}")

# Save configuration
config = {
    'model_type': best_model_name,
    'feature_columns': feature_columns,
    'numeric_features': numeric_features,
    'categorical_features': categorical_features,
    'class_names': class_names,
    'test_f1_score': float(comparison_df.iloc[0]['Test F1']),
    'test_roc_auc': float(comparison_df.iloc[0]['Test ROC-AUC']),
    'timestamp': timestamp
}

config_filename = f'churn_model_config_{timestamp}.json'
with open(config_filename, 'w') as f:
    json.dump(config, f, indent=2)
print(f"Configuration saved: {config_filename}")

print("\n" + "=" * 70)
print("ANALYSIS COMPLETE")
print("=" * 70)
print(f"\nBest Model: {best_model_name}")
print(f"Test F1 Score: {comparison_df.iloc[0]['Test F1']:.4f}")
print(f"Test ROC-AUC: {comparison_df.iloc[0]['Test ROC-AUC']:.4f}")
print("\nFiles generated:")
print(f"  - {model_filename}")
print(f"  - {preprocessor_filename}")
print(f"  - {config_filename}")
print(f"  - churn_prediction_analysis.png")

# --- USAGE INSTRUCTIONS ---
"""
To use this model in production:

import joblib
import json
import pandas as pd

# Load artifacts
model = joblib.load('churn_model_XXXXXXXX_XXXXXX.pkl')
preprocessor = joblib.load('churn_preprocessor_XXXXXXXX_XXXXXX.pkl')

with open('churn_model_config_XXXXXXXX_XXXXXX.json', 'r') as f:
    config = json.load(f)

# Predict for new customer
new_customer = {
    'tenure': 6,
    'monthly_charges': 80.00,
    'total_charges': 480.00,
    'contract_type': 'Month-to-month',
    'payment_method': 'Electronic check',
    'internet_service': 'Fiber optic',
    'online_security': 'No',
    'tech_support': 'No',
    'streaming_tv': 'Yes'
}

new_df = pd.DataFrame([new_customer])[config['feature_columns']]
new_processed = preprocessor.transform(new_df)

prediction = model.predict(new_processed)[0]
probability = model.predict_proba(new_processed)[0, 1]

print(f"Churn Prediction: {'Yes' if prediction == 1 else 'No'}")
print(f"Churn Probability: {probability*100:.1f}%")
"""
```

---

## IMPLEMENTATION NOTES FOR REPLIT AGENT

1. **Problem Type Detection**: Parse user input to determine if they want classification, clustering, or dimensionality reduction. Look for keywords like "predict", "classify", "cluster", "segment", "reduce dimensions", "visualize".

2. **Auto Mode**: When user asks to "compare models" or find the "best model", generate code that trains multiple models and compares their performance.

3. **Hyperparameter Tuning**: Use RandomizedSearchCV by default (faster), GridSearchCV only if user explicitly requests exhaustive search.

4. **Class Imbalance**: Detect imbalanced datasets (ratio < 0.3) and automatically apply SMOTE or class_weight='balanced'.

5. **Validation**: Before returning generated code:
   - Verify all imports are present
   - Ensure variable names are consistent
   - Check that the code is syntactically valid
   - Confirm feature names match throughout

6. **Dependencies**: List all required pip packages at the top of the file with installation instructions.

7. **File Delivery**: Provide as downloadable `.py` file. Name format: `ml_model_[problem_type]_[timestamp].py`

---

**END OF MACHINE LEARNING MODELS FUNCTION SPECIFICATION**