IMMEDIATE FIX FOR LENGTH ENFORCEMENT:

Increase max_tokens for reconstruction operations to 8000 (or 16000 if the model supports it). This removes the ceiling that's truncating output.
But that alone won't fix it. The model is defaulting to compression because there's no per-chunk length target. Implement the length enforcement logic from the OUTPUT_LENGTH_ENFORCEMENT document:

Parse user's length requirement ("at least 10,000 words")
Calculate: target_words / input_words = 10,000 / 15,000 = 0.67 ratio
But since it's a floor ("at least"), use ratio = 0.75 to safely exceed minimum
Per-chunk target: chunk_input_words × 0.75
Inject into each chunk prompt: "YOUR OUTPUT MUST BE approximately X words for this chunk"


For this specific test (dialogue, 10,000+ words from 15,000 input):

19 chunks × ~790 words input each
Target per chunk: ~530-600 words output
Add to each chunk prompt: "This chunk must produce 500-650 words of dialogue."


Add word count validation after each chunk. If output < 80% of target, retry with stronger instruction. ADDITIONAL INSTRUCTION: PRIORITIZE COMPLETENESS OVER SPEED
Token limit avoidance is the top priority. The system must never truncate output due to hitting token limits. Implement these safeguards:

Conservative chunk sizing: Reduce chunk size from 800 words to 500 words. More chunks = less output required per chunk = less risk of hitting limits.
Lower per-chunk output ceiling: Never request more than 600 words of output per chunk. If length calculations require more, split into smaller chunks instead.
Sequential processing only: Do NOT parallelize chunk processing. Process one chunk at a time, wait for completion, verify output, then proceed. Speed is not a priority.
Pause between chunks: Add a 2-second delay between chunk API calls to avoid rate limits and allow for state saving.
Checkpoint after every chunk: Save chunk output to database immediately after each completion. If process fails mid-document, resume from last completed chunk.
Output validation before proceeding: After each chunk completes:

Verify response is not truncated (doesn't end mid-sentence)
Verify word count is within target range
If truncated or short: retry that chunk with reduced output target before proceeding


If any single chunk requires >600 words output: Split that chunk in half and process as two separate chunks. Never ask for more than 600 words in one LLM call.
Set max_tokens to 4000 (keep current setting) but design prompts to never need more than 2500 tokens of actual output. This leaves buffer for response overhead.