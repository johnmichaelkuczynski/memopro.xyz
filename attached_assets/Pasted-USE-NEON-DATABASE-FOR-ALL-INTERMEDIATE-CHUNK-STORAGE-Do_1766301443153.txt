USE NEON DATABASE FOR ALL INTERMEDIATE CHUNK STORAGE
Do NOT store intermediate chunks in memory or Replit's local filesystem. Use the Neon Postgres database (DATABASE_URL) for all processing state. This provides:

Crash recovery: If process fails at chunk 45/72, resume from chunk 45 instead of starting over
Memory management: Don't accumulate 72 chunks in RAM
Debugging: Can inspect intermediate state to diagnose problems

SCHEMA FOR INTERMEDIATE PROCESSING:
sql-- Create if not exists
CREATE TABLE IF NOT EXISTS reconstruction_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT,
    document_title TEXT,
    original_text TEXT,
    target_words INTEGER,
    total_input_words INTEGER,
    num_chunks INTEGER,
    chunk_target_words INTEGER,
    custom_instructions TEXT,
    status TEXT DEFAULT 'pending',  -- 'pending', 'processing', 'complete', 'failed'
    current_chunk INTEGER DEFAULT 0,
    final_output TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS reconstruction_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID REFERENCES reconstruction_jobs(id) ON DELETE CASCADE,
    chunk_index INTEGER,
    chunk_input_text TEXT,
    chunk_output_text TEXT,
    target_words INTEGER,
    actual_words INTEGER,
    retry_count INTEGER DEFAULT 0,
    status TEXT DEFAULT 'pending',  -- 'pending', 'processing', 'complete', 'retry', 'failed'
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index for fast lookups
CREATE INDEX IF NOT EXISTS idx_chunks_job_id ON reconstruction_chunks(job_id);
CREATE INDEX IF NOT EXISTS idx_chunks_status ON reconstruction_chunks(status);
```

**PROCESSING WORKFLOW:**
```
1. START JOB:
   - Insert row into reconstruction_jobs with status='processing'
   - Calculate num_chunks, chunk_target_words
   - Insert [num_chunks] rows into reconstruction_chunks with status='pending'
   - Each row has chunk_index, chunk_input_text, target_words

2. PROCESS CHUNKS:
   - Query: SELECT * FROM reconstruction_chunks WHERE job_id=X AND status='pending' ORDER BY chunk_index LIMIT 1
   - Update status='processing'
   - Call LLM
   - Update: chunk_output_text, actual_words, status='complete'
   - COMMIT after each chunk (don't batch)
   - Wait 3 seconds
   - Repeat until no pending chunks

3. ON FAILURE/CRASH:
   - Job remains in database with status='processing'
   - On restart, query: SELECT * FROM reconstruction_jobs WHERE status='processing'
   - Resume from first chunk where status != 'complete'

4. FINAL ASSEMBLY:
   - Query: SELECT chunk_output_text FROM reconstruction_chunks WHERE job_id=X ORDER BY chunk_index
   - Concatenate all outputs
   - Update reconstruction_jobs: final_output=concatenated, status='complete'

5. CLEANUP (after user downloads or after 24 hours):
   - DELETE FROM reconstruction_jobs WHERE id=X
   - Cascade deletes reconstruction_chunks automatically
CLEANUP POLICY:

Keep completed jobs for 24 hours (allows user to re-download)
Run cleanup job hourly: DELETE FROM reconstruction_jobs WHERE status='complete' AND updated_at < NOW() - INTERVAL '24 hours'
Failed jobs kept for 7 days for debugging, then deleted

MEMORY RULES:

Never hold more than 1 chunk in memory at a time
After saving chunk to database, clear from memory
Final assembly: stream from database, don't load all chunks into memory at once
For very large outputs (>50,000 words), write final output directly to database, don't concatenate in memory