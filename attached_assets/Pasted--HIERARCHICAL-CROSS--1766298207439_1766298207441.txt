═══════════════════════════════════════════════════════════════════════════════
HIERARCHICAL CROSS-CHUNK COHERENCE (HCC) ARCHITECTURE
For Processing Large Documents (100,000+ Words)
═══════════════════════════════════════════════════════════════════════════════

PURPOSE
───────
This document specifies an architecture for processing book-length documents
(100,000+ words) while maintaining cross-chunk coherence. The flat CC 
architecture (skeleton → chunks → stitch) does not scale beyond ~5,000 words.
This hierarchical architecture solves that limitation.

═══════════════════════════════════════════════════════════════════════════════
CORE PROBLEM AT SCALE
═══════════════════════════════════════════════════════════════════════════════

At 100,000 words with 800-word chunks:
- 125 chunks to process
- Skeleton must represent book-length argument (too large to inject everywhere)
- Stitch pass receives 125 delta reports (exceeds context window)
- Commitment ledger has thousands of entries (can't inject all into each chunk)
- Cross-references span chapters (flat structure can't track)
- Processing time: potentially hours
- Cost: potentially hundreds of dollars per run

SOLUTION: Hierarchical processing that mirrors document structure.

═══════════════════════════════════════════════════════════════════════════════
HIERARCHICAL STRUCTURE
═══════════════════════════════════════════════════════════════════════════════

BOOK (100,000 words)
│
├── Book-Level Skeleton (compressed, ~1500-2000 tokens max)
│   Contains: Master thesis, major divisions, global terms, core commitments
│
├── PART 1 (25,000 words)
│   │
│   ├── Part-Level Skeleton (~800-1000 tokens)
│   │   Contains: Part thesis, chapter summaries, part-specific terms
│   │   Inherits: Compressed book-level commitments
│   │
│   ├── CHAPTER 1 (5,000 words)
│   │   │
│   │   ├── Chapter-Level Skeleton (~500-800 tokens)
│   │   │   Contains: Chapter thesis, section outline, chapter terms
│   │   │   Inherits: Compressed part-level commitments
│   │   │
│   │   ├── Chunk 1 (800 words) → Process → Delta
│   │   ├── Chunk 2 (800 words) → Process → Delta
│   │   ├── Chunk 3 (800 words) → Process → Delta
│   │   ├── ...
│   │   │
│   │   ├── Chapter Stitch (receives chapter skeleton + chunk deltas)
│   │   │
│   │   └── Chapter Output + Chapter Delta (for part stitch)
│   │
│   ├── CHAPTER 2 (5,000 words)
│   │   └── [same structure]
│   │
│   ├── ...
│   │
│   ├── Part Stitch (receives part skeleton + chapter deltas)
│   │
│   └── Part Output + Part Delta (for book stitch)
│
├── PART 2 (25,000 words)
│   └── [same structure]
│
├── ...
│
├── Book Stitch (receives book skeleton + part deltas)
│
└── Final Book Output

═══════════════════════════════════════════════════════════════════════════════
SKELETON INHERITANCE AND COMPRESSION
═══════════════════════════════════════════════════════════════════════════════

Each level inherits a COMPRESSED version of the level above:

BOOK SKELETON (full: ~2000 tokens)
    ↓ compress to ~500 tokens
PART SKELETON (own content: ~500 tokens + inherited: ~500 tokens = ~1000 tokens)
    ↓ compress to ~300 tokens  
CHAPTER SKELETON (own content: ~400 tokens + inherited: ~300 tokens = ~700 tokens)
    ↓ inject into chunks
CHUNK PROCESSING (receives: ~700 token skeleton + ~800 word chunk)

This keeps context window usage manageable at every level.

COMPRESSION RULES:
- Preserve thesis statements verbatim
- Preserve key term definitions verbatim
- Compress examples to references ("as shown in Ch.3 via the X example")
- Compress argument chains to conclusions
- Preserve all cross-reference pointers

═══════════════════════════════════════════════════════════════════════════════
DELTA PROPAGATION
═══════════════════════════════════════════════════════════════════════════════

Deltas flow UPWARD through the hierarchy:

CHUNK DELTA (~100 tokens each)
- New claims introduced
- Terms used (confirm or flag drift)
- Conflicts detected
- Cross-references made

    ↓ aggregate at chapter stitch

CHAPTER DELTA (~200 tokens each)
- Chapter's net contribution to argument
- New commitments established
- Conflicts resolved or flagged
- Cross-references to other chapters

    ↓ aggregate at part stitch

PART DELTA (~300 tokens each)
- Part's net contribution to book argument
- Major commitments established
- Inter-chapter coherence status
- Cross-references to other parts

    ↓ aggregate at book stitch

BOOK STITCH
- Receives: book skeleton + all part deltas
- Total input: ~2000 + (4 parts × 300) = ~3200 tokens
- Manageable within context window

═══════════════════════════════════════════════════════════════════════════════
PROCESSING PHASES
═══════════════════════════════════════════════════════════════════════════════

PHASE 1: STRUCTURE DETECTION
────────────────────────────
Before any processing, detect document structure:
- Identify parts/sections (if any)
- Identify chapters
- Identify major section breaks within chapters
- Store structure map in database

If document has no explicit structure, impose one:
- Every ~25,000 words becomes a "virtual part"
- Every ~5,000 words becomes a "virtual chapter"

PHASE 2: SKELETON EXTRACTION (TOP-DOWN)
───────────────────────────────────────
Extract skeletons from top to bottom:

2a. Book-Level Skeleton
    - Process: full document (or representative samples if too large)
    - Extract: master thesis, major divisions, global terms, core commitments
    - Store: book_skeleton in database
    - Token budget: ~2000 tokens max

2b. Part-Level Skeletons (for each part)
    - Input: part text + compressed book skeleton
    - Extract: part thesis, chapter summaries, part-specific terms
    - Store: part_skeletons[part_id] in database
    - Token budget: ~1000 tokens max each

2c. Chapter-Level Skeletons (for each chapter)
    - Input: chapter text + compressed part skeleton
    - Extract: chapter thesis, section outline, chapter terms
    - Store: chapter_skeletons[chapter_id] in database
    - Token budget: ~700 tokens max each

PHASE 3: CHUNK PROCESSING (BOTTOM-UP)
─────────────────────────────────────
Process chunks within each chapter:

For each chapter:
    - Divide into ~800 word chunks (respect paragraph boundaries)
    - For each chunk:
        - Input: chunk text + chapter skeleton
        - Process: apply reconstruction/analysis function
        - Output: processed chunk + chunk delta
        - Store: chunk_outputs[chunk_id], chunk_deltas[chunk_id]
    
    *** PAUSE POINT: After each chapter, save state ***

PHASE 4: CHAPTER STITCH (for each chapter)
──────────────────────────────────────────
After all chunks in a chapter are processed:
    - Input: chapter skeleton + all chunk deltas for this chapter
    - Detect: intra-chapter contradictions, term drift, redundancies
    - Generate: repair plan
    - Execute: micro-repairs on flagged chunks
    - Assemble: chapter output
    - Generate: chapter delta (for part stitch)
    - Store: chapter_output, chapter_delta

    *** PAUSE POINT: After each chapter stitch, save state ***

PHASE 5: PART STITCH (for each part)
────────────────────────────────────
After all chapters in a part are stitched:
    - Input: part skeleton + all chapter deltas for this part
    - Detect: inter-chapter contradictions, cross-reference failures
    - Generate: repair plan
    - Execute: repairs on flagged chapters (re-run chapter stitch if needed)
    - Assemble: part output
    - Generate: part delta (for book stitch)
    - Store: part_output, part_delta

    *** PAUSE POINT: After each part stitch, save state ***

PHASE 6: BOOK STITCH
────────────────────
After all parts are stitched:
    - Input: book skeleton + all part deltas
    - Detect: inter-part contradictions, global coherence issues
    - Generate: repair plan
    - Execute: repairs on flagged parts (re-run part stitch if needed)
    - Assemble: final book output
    - Store: final_output

═══════════════════════════════════════════════════════════════════════════════
DATABASE SCHEMA
═══════════════════════════════════════════════════════════════════════════════

-- Document structure
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT,
    title TEXT,
    original_text TEXT,
    word_count INTEGER,
    structure_map JSONB,  -- detected/imposed hierarchy
    book_skeleton JSONB,
    final_output TEXT,
    status TEXT,  -- 'structure_detected', 'skeletons_extracted', 'processing', 'complete'
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Parts (or virtual parts)
CREATE TABLE document_parts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id),
    part_index INTEGER,
    part_title TEXT,
    original_text TEXT,
    word_count INTEGER,
    part_skeleton JSONB,
    compressed_book_skeleton JSONB,  -- inherited from above
    part_output TEXT,
    part_delta JSONB,
    status TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Chapters (or virtual chapters)
CREATE TABLE document_chapters (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    part_id UUID REFERENCES document_parts(id),
    document_id UUID REFERENCES documents(id),
    chapter_index INTEGER,
    chapter_title TEXT,
    original_text TEXT,
    word_count INTEGER,
    chapter_skeleton JSONB,
    compressed_part_skeleton JSONB,  -- inherited from above
    chapter_output TEXT,
    chapter_delta JSONB,
    status TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Chunks
CREATE TABLE document_chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    chapter_id UUID REFERENCES document_chapters(id),
    document_id UUID REFERENCES documents(id),
    chunk_index INTEGER,
    chunk_input_text TEXT,
    chunk_output_text TEXT,
    chunk_delta JSONB,
    status TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Processing runs (for audit/debugging/resumption)
CREATE TABLE processing_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id),
    run_type TEXT,  -- 'structure', 'book_skeleton', 'part_skeleton', 'chapter_skeleton', 
                    -- 'chunk_process', 'chapter_stitch', 'part_stitch', 'book_stitch'
    target_id UUID,  -- which part/chapter/chunk this run was for
    run_input JSONB,
    run_output JSONB,
    tokens_used INTEGER,
    duration_ms INTEGER,
    status TEXT,  -- 'started', 'complete', 'failed'
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Checkpoints (for resumption after failures or pauses)
CREATE TABLE processing_checkpoints (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID REFERENCES documents(id),
    phase TEXT,  -- 'skeleton_extraction', 'chunk_processing', 'stitching'
    current_part_index INTEGER,
    current_chapter_index INTEGER,
    current_chunk_index INTEGER,
    state_snapshot JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

═══════════════════════════════════════════════════════════════════════════════
REPLIT AGENT INSTRUCTIONS
═══════════════════════════════════════════════════════════════════════════════

*** CRITICAL: READ THIS ENTIRE SECTION BEFORE IMPLEMENTING ***

OVERVIEW
────────
You are implementing a hierarchical cross-chunk coherence (HCC) system for
processing book-length documents (up to 200,000 words). This is complex.
Do NOT try to implement everything at once. Follow the phased approach below.

IMPLEMENTATION PHASES
─────────────────────

PHASE A: Database Setup (Do First)
1. Create all tables listed in DATABASE SCHEMA above
2. Add indexes on foreign keys and status columns
3. Test with simple insert/select operations
4. STOP and verify before proceeding

PHASE B: Structure Detection
1. Create function: detectDocumentStructure(text)
   - Input: full document text
   - Detect: explicit parts (PART I, BOOK ONE, etc.)
   - Detect: explicit chapters (Chapter 1, CHAPTER ONE, etc.)
   - Detect: major section breaks (multiple newlines, horizontal rules)
   - If no explicit structure: impose virtual structure
     - Virtual part every ~25,000 words
     - Virtual chapter every ~5,000 words
   - Output: structure_map JSON
   - Store: in documents table

2. Create function: splitByStructure(text, structure_map)
   - Split document into parts and chapters based on structure_map
   - Store each part in document_parts
   - Store each chapter in document_chapters
   - Respect natural boundaries (don't split mid-paragraph)

3. STOP and test with sample documents before proceeding

PHASE C: Skeleton Extraction
1. Create function: extractBookSkeleton(document_id)
   - If document < 50,000 words: process full text
   - If document >= 50,000 words: process intro + conclusion + chapter starts
   - Extract: thesis, major divisions, global terms (with definitions), 
     core commitments, cross-reference patterns
   - Enforce: 2000 token max (truncate/compress if needed)
   - Store: in documents.book_skeleton

2. Create function: compressSkeleton(skeleton, target_tokens)
   - Compress a skeleton to target token count
   - Preserve: thesis statements, key term definitions
   - Compress: examples to references, argument chains to conclusions
   - Output: compressed skeleton

3. Create function: extractPartSkeleton(part_id)
   - Input: part text + compressed book skeleton (500 tokens)
   - Extract: part thesis, chapter summaries, part-specific terms
   - Enforce: 1000 token max (including inherited)
   - Store: in document_parts.part_skeleton

4. Create function: extractChapterSkeleton(chapter_id)
   - Input: chapter text + compressed part skeleton (300 tokens)
   - Extract: chapter thesis, section outline, chapter-specific terms
   - Enforce: 700 token max (including inherited)
   - Store: in document_chapters.chapter_skeleton

5. Create function: extractAllSkeletons(document_id)
   - Call extractBookSkeleton
   - For each part: call extractPartSkeleton
   - For each chapter: call extractChapterSkeleton
   - Update checkpoint after each extraction
   - PAUSE after each part to avoid token limits

6. STOP and test skeleton extraction before proceeding

PHASE D: Chunk Processing
1. Create function: chunkChapter(chapter_id)
   - Split chapter into ~800 word chunks
   - Respect paragraph boundaries (never split mid-sentence)
   - Store chunks in document_chunks

2. Create function: processChunk(chunk_id, function_type, custom_instructions)
   - Input: chunk text + chapter skeleton
   - Apply: reconstruction/analysis function per function_type
   - Output: processed chunk text + chunk delta
   - Chunk delta must include:
     - new_claims: [] (claims introduced in this chunk)
     - terms_used: [] (key terms referenced)
     - conflicts: [] (any conflicts with skeleton)
     - cross_refs: [] (references to other parts of document)
   - Enforce: delta max 100 tokens
   - Store: chunk_output_text, chunk_delta

3. Create function: processChapterChunks(chapter_id, function_type, custom_instructions)
   - Get all chunks for chapter
   - For each chunk: call processChunk
   - Update checkpoint after each chunk
   - PAUSE every 5 chunks to avoid rate limits

4. STOP and test chunk processing on one chapter before proceeding

PHASE E: Stitching
1. Create function: stitchChapter(chapter_id)
   - Input: chapter skeleton + all chunk deltas for chapter
   - Detect: contradictions, term drift, redundancies
   - Generate: conflict_list, repair_plan
   - If repairs needed:
     - Re-process only flagged chunks with repair instructions
   - Assemble: chapter output from processed chunks
   - Generate: chapter delta (for part stitch)
     - net_contribution: (chapter's main argument)
     - new_commitments: [] 
     - resolved_conflicts: []
     - cross_refs: []
   - Enforce: chapter delta max 200 tokens
   - Store: chapter_output, chapter_delta

2. Create function: stitchPart(part_id)
   - Input: part skeleton + all chapter deltas for part
   - Detect: inter-chapter contradictions, cross-ref failures
   - Generate: conflict_list, repair_plan
   - If repairs needed:
     - Re-run stitchChapter on flagged chapters
   - Assemble: part output from chapter outputs
   - Generate: part delta (for book stitch)
   - Enforce: part delta max 300 tokens
   - Store: part_output, part_delta

3. Create function: stitchBook(document_id)
   - Input: book skeleton + all part deltas
   - Detect: inter-part contradictions, global coherence issues
   - Generate: conflict_list, repair_plan
   - If repairs needed:
     - Re-run stitchPart on flagged parts
   - Assemble: final output from part outputs
   - Store: final_output

PHASE F: Orchestration
1. Create function: processDocument(document_id, function_type, custom_instructions)
   - This is the main entry point
   - Steps:
     a. Check for existing checkpoint (enable resumption)
     b. If no checkpoint or phase incomplete: detectDocumentStructure
     c. extractAllSkeletons (with pauses)
     d. For each part:
          For each chapter:
            chunkChapter
            processChapterChunks (with pauses)
            stitchChapter
            SAVE CHECKPOINT
          stitchPart
          SAVE CHECKPOINT
     e. stitchBook
     f. Return final_output

2. Create function: resumeProcessing(document_id)
   - Load last checkpoint
   - Resume from that point
   - Continue with processDocument logic

═══════════════════════════════════════════════════════════════════════════════
CRITICAL IMPLEMENTATION RULES
═══════════════════════════════════════════════════════════════════════════════

*** TOKEN LIMIT MANAGEMENT ***

1. NEVER process more than 5 chunks in a single execution without pausing
2. ALWAYS save checkpoint after each chapter completion
3. ALWAYS save checkpoint after each part completion
4. Use streaming for LLM calls where possible
5. Track tokens_used in processing_runs table
6. If approaching rate limits, implement exponential backoff

*** ERROR HANDLING ***

1. Wrap ALL LLM calls in try/catch
2. On failure: save state, log error, mark run as 'failed'
3. On resumption: check for failed runs, retry or skip
4. Never lose work - every successful chunk/stitch must be persisted

*** SKELETON ENFORCEMENT ***

1. If a chunk conflicts with skeleton, the model must:
   - Produce closest adjacent repair that preserves intent, OR
   - Return explicit flagged conflict with proposed minimal change
   - NEVER silently contradict skeleton
   - NEVER return "can't" without alternative

2. If skeleton extraction produces skeleton > token budget:
   - Compress iteratively until within budget
   - Preserve thesis and term definitions at all costs
   - Log warning that compression was required

*** CHUNK BOUNDARY RULES ***

1. NEVER split mid-sentence
2. NEVER split mid-paragraph if avoidable
3. Prefer splitting at section breaks
4. Target 800 words but allow 600-1000 range for better boundaries
5. If chapter < 600 words, process as single chunk

*** TESTING REQUIREMENTS ***

Before deploying:
1. Test with 5,000 word document (1 chapter equivalent)
2. Test with 25,000 word document (1 part equivalent)
3. Test with 50,000 word document (multi-part)
4. Test resumption by killing process mid-run
5. Test error recovery by simulating LLM failures

═══════════════════════════════════════════════════════════════════════════════
PROMPT TEMPLATES
═══════════════════════════════════════════════════════════════════════════════

BOOK SKELETON EXTRACTION PROMPT:
────────────────────────────────
You are extracting a structural skeleton from a book-length document.
This skeleton will guide coherent processing of individual sections.

Extract the following (total must be under 2000 tokens):

1. THESIS (1-3 sentences): The central argument or purpose of this work

2. MAJOR DIVISIONS (numbered list): The main parts/sections and their purposes

3. KEY TERMS (definition list): Important terms with their specific meanings 
   as used in this document. Format: "TERM: definition"

4. CORE COMMITMENTS (numbered list): What the document asserts, rejects, assumes
   Format: "ASSERTS: X" / "REJECTS: Y" / "ASSUMES: Z"

5. CROSS-REFERENCE PATTERNS: How the document refers to its own parts
   (e.g., "the author frequently references Chapter 3's argument about X")

Be precise. Preserve exact terminology. This skeleton constrains all downstream
processing - errors here propagate everywhere.

DOCUMENT TEXT:
{document_text}

---

CHUNK PROCESSING PROMPT:
────────────────────────
You are processing one chunk of a larger document. You must maintain coherence
with the document's established structure and commitments.

CHAPTER SKELETON (you must honor this):
{chapter_skeleton}

PROCESSING INSTRUCTIONS:
{custom_instructions}

CONSTRAINTS:
- Do NOT contradict any commitment in the skeleton
- Use key terms EXACTLY as defined in the skeleton
- If you detect a conflict between the chunk content and the skeleton,
  FLAG IT EXPLICITLY rather than silently violating coherence
- Preserve the chunk's contribution to the chapter's argument

CHUNK TEXT:
{chunk_text}

Provide:
1. PROCESSED_TEXT: The reconstructed/processed chunk
2. DELTA_REPORT (JSON, max 100 tokens):
   {
     "new_claims": ["claim1", "claim2"],
     "terms_used": ["term1", "term2"],
     "conflicts": ["conflict description if any"],
     "cross_refs": ["reference to other section if any"]
   }

---

CHAPTER STITCH PROMPT:
──────────────────────
You are stitching together processed chunks of a chapter to ensure coherence.

CHAPTER SKELETON:
{chapter_skeleton}

CHUNK DELTAS:
{chunk_deltas_json}

Review these deltas for:
1. CONTRADICTIONS: Do any chunks contradict each other or the skeleton?
2. TERM DRIFT: Is any key term used inconsistently?
3. REDUNDANCIES: Do multiple chunks make the same point unnecessarily?
4. GAPS: Is anything from the skeleton missing from the chunks?

Provide:
1. CONFLICT_LIST: Specific issues found (or "none")
2. REPAIR_PLAN: For each conflict, which chunk needs what change
3. CHAPTER_DELTA (JSON, max 200 tokens):
   {
     "net_contribution": "chapter's main argument in one sentence",
     "new_commitments": ["commitment established by this chapter"],
     "resolved_conflicts": ["any skeleton conflicts resolved here"],
     "cross_refs": ["references to other chapters"]
   }

═══════════════════════════════════════════════════════════════════════════════
PERFORMANCE EXPECTATIONS
═══════════════════════════════════════════════════════════════════════════════

For a 100,000 word document:
- Structure detection: ~1 minute
- Skeleton extraction: ~10-15 minutes (with pauses)
- Chunk processing: ~2-4 hours (with pauses and rate limits)
- Stitching: ~30-60 minutes
- Total: 3-6 hours

For a 200,000 word document:
- Double the above estimates
- Consider breaking into multiple sessions
- Resumption capability is critical

Cost estimate (rough):
- 100k word document: $50-150 depending on model and reconstruction complexity
- 200k word document: $100-300

These are acceptable for institutional clients processing high-value documents.

═══════════════════════════════════════════════════════════════════════════════
FUTURE ENHANCEMENTS (NOT FOR INITIAL IMPLEMENTATION)
═══════════════════════════════════════════════════════════════════════════════

1. PARALLEL PROCESSING
   - Process chapters within a part in parallel
   - Requires careful checkpoint management
   - Can reduce processing time by 50-70%

2. RETRIEVAL-AUGMENTED SKELETON
   - For very large documents, embed skeleton entries
   - Retrieve only relevant entries for each chunk
   - Reduces token usage per chunk

3. INCREMENTAL PROCESSING
   - Process document as it's being written/edited
   - Update only affected chunks and re-stitch
   - Useful for document collaboration tools

4. CONFIDENCE SCORING
   - Track confidence that each chunk honors skeleton
   - Prioritize low-confidence chunks for human review
   - Reduce unnecessary re-processing

═══════════════════════════════════════════════════════════════════════════════
END OF ARCHITECTURE DOCUMENT
═══════════════════════════════════════════════════════════════════════════════