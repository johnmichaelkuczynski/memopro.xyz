═══════════════════════════════════════════════════════════════════════════════
OUTPUT LENGTH ENFORCEMENT FOR CHUNK PROCESSING
═══════════════════════════════════════════════════════════════════════════════

PROBLEM
───────
When user requests specific output length (e.g., "5000-6000 words"), the app
fails to meet the target. Output is truncated or compressed despite explicit
instructions.

ROOT CAUSE
──────────
Length instructions are passed to the final assembly or overall process, but
individual chunk processing doesn't know its proportional output target. Each
chunk defaults to compression, resulting in cumulative shortfall.

SOLUTION
────────
Calculate per-chunk output targets and inject them into each chunk's processing
prompt. Enforce length at the chunk level, not just the document level.

═══════════════════════════════════════════════════════════════════════════════
LENGTH CALCULATION LOGIC
═══════════════════════════════════════════════════════════════════════════════

STEP 1: Parse user's length requirement
────────────────────────────────────────
Extract from custom instructions:
- target_min_words (e.g., 5000)
- target_max_words (e.g., 6000)
- target_mid_words = (target_min + target_max) / 2 (e.g., 5500)

If no explicit target given:
- Default behavior: output ≈ input length (ratio 1.0)
- If user says "expand" or "enrich": ratio 1.3-1.5
- If user says "compress" or "summarize": calculate from explicit target

STEP 2: Calculate input metrics
───────────────────────────────
- total_input_words = word count of full document
- num_chunks = number of chunks created
- avg_chunk_input_words = total_input_words / num_chunks

STEP 3: Calculate length ratio
──────────────────────────────
length_ratio = target_mid_words / total_input_words

Examples:
- Input: 15,000 words, Target: 5,500 words → ratio = 0.367 (compression)
- Input: 3,000 words, Target: 5,500 words → ratio = 1.833 (expansion)
- Input: 5,000 words, Target: 5,500 words → ratio = 1.1 (slight expansion)

STEP 4: Calculate per-chunk target
──────────────────────────────────
chunk_target_words = avg_chunk_input_words × length_ratio

Example:
- Input: 15,000 words in 19 chunks (avg 789 words/chunk)
- Target: 5,500 words
- Ratio: 0.367
- Per-chunk target: 789 × 0.367 ≈ 290 words

STEP 5: Calculate acceptable range per chunk
────────────────────────────────────────────
Allow ±15% variance to avoid rigid output that sounds forced:
- chunk_min_words = chunk_target_words × 0.85
- chunk_max_words = chunk_target_words × 1.15

Example:
- Target: 290 words
- Range: 247-334 words per chunk

═══════════════════════════════════════════════════════════════════════════════
MODIFIED CHUNK PROCESSING PROMPT
═══════════════════════════════════════════════════════════════════════════════

CHUNK PROCESSING PROMPT (WITH LENGTH ENFORCEMENT):
──────────────────────────────────────────────────

You are processing one chunk of a larger document. You must maintain coherence
with the document's established structure and commitments.

CHAPTER SKELETON (you must honor this):
{chapter_skeleton}

PROCESSING INSTRUCTIONS:
{custom_instructions}

*** OUTPUT LENGTH REQUIREMENT ***
This chunk is part of a {total_chunks}-chunk document.
- Original chunk length: {chunk_input_words} words
- YOUR OUTPUT MUST BE: {chunk_min_words}-{chunk_max_words} words
- Target: approximately {chunk_target_words} words

This is a hard requirement. If your output is outside this range, you have
failed the task. Count your words before finalizing.

{length_guidance}

*** END LENGTH REQUIREMENT ***

CONSTRAINTS:
- Do NOT contradict any commitment in the skeleton
- Use key terms EXACTLY as defined in the skeleton
- If you detect a conflict between the chunk content and the skeleton,
  FLAG IT EXPLICITLY rather than silently violating coherence
- Preserve the chunk's contribution to the chapter's argument

CHUNK TEXT:
{chunk_text}

Provide:
1. PROCESSED_TEXT: The reconstructed/processed chunk ({chunk_min_words}-{chunk_max_words} words)
2. WORD_COUNT: Actual word count of your processed text
3. DELTA_REPORT (JSON, max 100 tokens):
   {
     "new_claims": ["claim1", "claim2"],
     "terms_used": ["term1", "term2"],
     "conflicts": ["conflict description if any"],
     "cross_refs": ["reference to other section if any"],
     "word_count": <number>
   }

═══════════════════════════════════════════════════════════════════════════════
LENGTH GUIDANCE TEMPLATES
═══════════════════════════════════════════════════════════════════════════════

Use appropriate guidance based on length_ratio:

IF length_ratio < 0.5 (heavy compression):
────────────────────────────────────────
{length_guidance} = """
LENGTH MODE: HEAVY COMPRESSION
You must significantly compress this chunk while preserving core arguments.
- Remove examples, keep only the most critical one
- Remove repetition and redundancy
- Convert detailed explanations to concise statements
- Preserve thesis statements and key claims verbatim
- Remove transitional phrases and rhetorical flourishes
"""

IF length_ratio >= 0.5 AND < 0.8 (moderate compression):
─────────────────────────────────────────────────────────
{length_guidance} = """
LENGTH MODE: MODERATE COMPRESSION
You must compress this chunk while preserving argument structure.
- Keep the strongest 1-2 examples, remove weaker ones
- Tighten prose without losing meaning
- Preserve all key claims and their primary support
- Remove redundancy but keep necessary repetition for emphasis
"""

IF length_ratio >= 0.8 AND < 1.2 (maintain length):
───────────────────────────────────────────────────
{length_guidance} = """
LENGTH MODE: MAINTAIN LENGTH
Your output should be approximately the same length as input.
- Improve clarity and coherence without changing length significantly
- Replace weak examples with stronger ones of similar length
- Restructure sentences for better flow
- Do not add or remove substantial content
"""

IF length_ratio >= 1.2 AND < 1.8 (moderate expansion):
──────────────────────────────────────────────────────
{length_guidance} = """
LENGTH MODE: MODERATE EXPANSION
You must expand this chunk while maintaining focus.
- Add 1-2 supporting examples or evidence for key claims
- Elaborate on implications of major points
- Add transitional sentences to improve flow
- Expand terse statements into fuller explanations
- Do NOT add tangential content or padding
"""

IF length_ratio >= 1.8 (heavy expansion):
────────────────────────────────────────
{length_guidance} = """
LENGTH MODE: HEAVY EXPANSION
You must significantly expand this chunk with substantive additions.
- Add 2-3 concrete examples (historical, empirical, or hypothetical)
- Elaborate on each major claim with supporting analysis
- Add relevant context and background
- Develop implications and consequences of arguments
- Add appropriate qualifications and nuances
- Do NOT add filler or padding—all additions must be substantive
"""

═══════════════════════════════════════════════════════════════════════════════
IMPLEMENTATION: REPLIT AGENT INSTRUCTIONS
═══════════════════════════════════════════════════════════════════════════════

1. CREATE FUNCTION: parseTargetLength(customInstructions)
───────────────────────────────────────────────────────────
Input: custom instructions string
Output: { targetMin: number, targetMax: number } or null if not specified

Logic:
- Search for patterns like:
  - "at least X words" → targetMin = X
  - "no more than X words" → targetMax = X
  - "approximately X words" → targetMin = X * 0.9, targetMax = X * 1.1
  - "X-Y words" → targetMin = X, targetMax = Y
  - "around X words" → targetMin = X * 0.9, targetMax = X * 1.1
  - "X words" (standalone) → targetMin = X * 0.9, targetMax = X * 1.1

- If contains "expand", "enrich", "elaborate" but no number:
  - Set expansion_mode = true (will use ratio 1.3-1.5 of input)

- If contains "compress", "summarize", "shorten" but no number:
  - Set compression_mode = true (will use ratio 0.3-0.5 of input)

Example:
parseTargetLength("no less than 5000 and no more than 6000")
→ { targetMin: 5000, targetMax: 6000 }

2. CREATE FUNCTION: calculateChunkTargets(documentId, targetMin, targetMax)
───────────────────────────────────────────────────────────────────────────
Input: document ID, target word range
Output: updates all chunks in database with target word counts

Logic:
```javascript
async function calculateChunkTargets(documentId, targetMin, targetMax) {
  // Get document metrics
  const doc = await db.documents.findOne({ id: documentId });
  const totalInputWords = countWords(doc.original_text);
  
  // Get all chunks
  const chunks = await db.document_chunks.find({ document_id: documentId });
  const numChunks = chunks.length;
  
  // Calculate ratio
  const targetMid = (targetMin + targetMax) / 2;
  const lengthRatio = targetMid / totalInputWords;
  
  // Determine length mode
  let lengthMode;
  if (lengthRatio < 0.5) lengthMode = 'heavy_compression';
  else if (lengthRatio < 0.8) lengthMode = 'moderate_compression';
  else if (lengthRatio < 1.2) lengthMode = 'maintain';
  else if (lengthRatio < 1.8) lengthMode = 'moderate_expansion';
  else lengthMode = 'heavy_expansion';
  
  // Update each chunk with its target
  for (const chunk of chunks) {
    const chunkInputWords = countWords(chunk.chunk_input_text);
    const chunkTargetWords = Math.round(chunkInputWords * lengthRatio);
    const chunkMinWords = Math.round(chunkTargetWords * 0.85);
    const chunkMaxWords = Math.round(chunkTargetWords * 1.15);
    
    await db.document_chunks.update({
      id: chunk.id,
      target_words: chunkTargetWords,
      min_words: chunkMinWords,
      max_words: chunkMaxWords,
      length_mode: lengthMode
    });
  }
  
  // Store document-level metrics
  await db.documents.update({
    id: documentId,
    length_ratio: lengthRatio,
    length_mode: lengthMode,
    target_min_words: targetMin,
    target_max_words: targetMax
  });
}
```

3. MODIFY FUNCTION: processChunk(chunkId, functionType, customInstructions)
───────────────────────────────────────────────────────────────────────────
Add length parameters to chunk processing:

```javascript
async function processChunk(chunkId, functionType, customInstructions) {
  const chunk = await db.document_chunks.findOne({ id: chunkId });
  const chapter = await db.document_chapters.findOne({ id: chunk.chapter_id });
  
  // Get length guidance template based on mode
  const lengthGuidance = getLengthGuidanceTemplate(chunk.length_mode);
  
  // Build prompt with length enforcement
  const prompt = buildChunkPrompt({
    chapterSkeleton: chapter.chapter_skeleton,
    customInstructions: customInstructions,
    chunkText: chunk.chunk_input_text,
    chunkInputWords: countWords(chunk.chunk_input_text),
    chunkTargetWords: chunk.target_words,
    chunkMinWords: chunk.min_words,
    chunkMaxWords: chunk.max_words,
    totalChunks: await db.document_chunks.count({ document_id: chunk.document_id }),
    lengthGuidance: lengthGuidance
  });
  
  // Call LLM
  const response = await callLLM(prompt);
  
  // Validate output length
  const outputWords = countWords(response.processed_text);
  if (outputWords < chunk.min_words || outputWords > chunk.max_words) {
    // Log warning but don't fail - track for potential re-processing
    console.warn(`Chunk ${chunkId} output ${outputWords} words, target was ${chunk.min_words}-${chunk.max_words}`);
    
    // If significantly off, retry once with stronger instruction
    if (outputWords < chunk.min_words * 0.7 || outputWords > chunk.max_words * 1.3) {
      return await retryChunkWithLengthCorrection(chunkId, response, chunk);
    }
  }
  
  // Store result
  await db.document_chunks.update({
    id: chunkId,
    chunk_output_text: response.processed_text,
    actual_output_words: outputWords,
    chunk_delta: response.delta_report
  });
  
  return response;
}
```

4. CREATE FUNCTION: retryChunkWithLengthCorrection(chunkId, previousResponse, chunk)
────────────────────────────────────────────────────────────────────────────────────
If output is significantly off-target, retry with correction prompt:

```javascript
async function retryChunkWithLengthCorrection(chunkId, previousResponse, chunk) {
  const outputWords = countWords(previousResponse.processed_text);
  const targetWords = chunk.target_words;
  
  let correctionInstruction;
  if (outputWords < chunk.min_words) {
    const wordsNeeded = targetWords - outputWords;
    correctionInstruction = `
Your previous output was ${outputWords} words, but the target is ${chunk.min_words}-${chunk.max_words} words.
You need to ADD approximately ${wordsNeeded} more words.
Expand with additional examples, elaboration, or analysis. Do not add padding or filler.
`;
  } else {
    const wordsToRemove = outputWords - targetWords;
    correctionInstruction = `
Your previous output was ${outputWords} words, but the target is ${chunk.min_words}-${chunk.max_words} words.
You need to REMOVE approximately ${wordsToRemove} words.
Cut redundancy, weaker examples, and unnecessary elaboration while preserving core arguments.
`;
  }
  
  const correctionPrompt = `
PREVIOUS OUTPUT (needs length correction):
${previousResponse.processed_text}

CORRECTION REQUIRED:
${correctionInstruction}

Provide the corrected version that meets the word count target.
`;
  
  const correctedResponse = await callLLM(correctionPrompt);
  return correctedResponse;
}
```

5. MODIFY: Final assembly to verify total length
────────────────────────────────────────────────
After all chunks are processed and stitched, verify total output meets target:

```javascript
async function verifyFinalLength(documentId) {
  const doc = await db.documents.findOne({ id: documentId });
  const finalOutput = doc.final_output;
  const actualWords = countWords(finalOutput);
  
  if (actualWords < doc.target_min_words) {
    console.error(`Final output ${actualWords} words, below minimum ${doc.target_min_words}`);
    // Flag for review or trigger expansion pass
    return { success: false, reason: 'below_minimum', actual: actualWords };
  }
  
  if (actualWords > doc.target_max_words) {
    console.error(`Final output ${actualWords} words, above maximum ${doc.target_max_words}`);
    // Flag for review or trigger compression pass
    return { success: false, reason: 'above_maximum', actual: actualWords };
  }
  
  return { success: true, actual: actualWords };
}
```

═══════════════════════════════════════════════════════════════════════════════
DATABASE SCHEMA ADDITIONS
═══════════════════════════════════════════════════════════════════════════════

Add to documents table:
- target_min_words INTEGER
- target_max_words INTEGER
- length_ratio DECIMAL
- length_mode TEXT

Add to document_chunks table:
- target_words INTEGER
- min_words INTEGER
- max_words INTEGER
- length_mode TEXT
- actual_output_words INTEGER

═══════════════════════════════════════════════════════════════════════════════
TESTING CHECKLIST
═══════════════════════════════════════════════════════════════════════════════

Before deploying, test these scenarios:

[ ] 1. Heavy compression: 15,000 word input → 5,000 word output
[ ] 2. Moderate compression: 8,000 word input → 5,000 word output
[ ] 3. Maintain length: 5,000 word input → 5,000 word output
[ ] 4. Moderate expansion: 3,000 word input → 5,000 word output
[ ] 5. Heavy expansion: 2,000 word input → 5,000 word output

For each test, verify:
[ ] Per-chunk targets calculated correctly
[ ] Each chunk output within ±15% of target
[ ] Final output within specified range
[ ] Retry mechanism triggers when needed
[ ] Content quality maintained despite length constraints

═══════════════════════════════════════════════════════════════════════════════
END OF OUTPUT LENGTH ENFORCEMENT DOCUMENT
═══════════════════════════════════════════════════════════════════════════════