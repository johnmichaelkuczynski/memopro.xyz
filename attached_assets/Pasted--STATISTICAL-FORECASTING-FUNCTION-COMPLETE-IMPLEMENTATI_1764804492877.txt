# STATISTICAL FORECASTING FUNCTION - COMPLETE IMPLEMENTATION INSTRUCTIONS

## FUNCTION PURPOSE

The Statistical Forecasting function takes a natural language description of a time series forecasting problem and generates a complete, production-ready Python implementation. The output is a downloadable `.py` file (or `.ipynb` Jupyter notebook) containing all code necessary to load time series data, perform diagnostics, fit forecasting models, generate predictions with confidence intervals, and visualize results.

---

## SUPPORTED FORECASTING METHODS

### Category A: Classical Statistical Methods
1. **ARIMA (AutoRegressive Integrated Moving Average)** - Non-seasonal time series with trend
2. **SARIMA (Seasonal ARIMA)** - Time series with both trend and seasonality
3. **SARIMAX** - SARIMA with exogenous variables (external regressors)
4. **VAR (Vector AutoRegression)** - Multivariate time series forecasting

### Category B: Exponential Smoothing Methods
1. **Simple Exponential Smoothing (SES)** - Level only, no trend or seasonality
2. **Holt's Linear Method** - Level + trend, no seasonality
3. **Holt-Winters (Triple Exponential Smoothing)** - Level + trend + seasonality
4. **ETS (Error, Trend, Seasonal)** - Automatic exponential smoothing model selection

### Category C: Modern/ML-Based Methods
1. **Prophet** - Facebook's forecasting tool, handles holidays and multiple seasonalities
2. **TBATS** - Trigonometric seasonality, Box-Cox transformation, ARMA errors
3. **Theta Method** - Simple but effective decomposition-based method

### Category D: Decomposition Methods
1. **STL Decomposition** - Seasonal-Trend decomposition using LOESS
2. **Classical Decomposition** - Additive or multiplicative decomposition

---

## INPUT PROCESSING REQUIREMENTS

The function receives natural language input. Extract the following variables:

**Required Variables:**
- `model_type`: Specific model or "auto" for automatic selection
- `data_source`: Inline data, file path, or synthetic data generation instructions
- `date_column`: Name of the datetime column
- `value_column`: Name of the column to forecast (target variable)
- `forecast_horizon`: Number of periods to forecast ahead

**Optional Variables (use defaults if not specified):**
- `frequency`: Data frequency - "D" (daily), "W" (weekly), "M" (monthly), "Q" (quarterly), "Y" (yearly), "H" (hourly) (default: auto-detect)
- `seasonal_period`: Length of seasonal cycle (default: auto-detect based on frequency)
- `confidence_level`: Confidence interval level (default: 0.95 for 95%)
- `train_test_split`: Proportion for backtesting (default: 0.8)
- `exogenous_variables`: List of external regressor columns (default: None)
- `holiday_country`: Country code for holiday effects in Prophet (default: None)
- `decomposition_type`: "additive" or "multiplicative" (default: auto-detect)
- `differencing_order`: For ARIMA, order of differencing (default: auto via ADF test)
- `output_format`: "py" or "ipynb" (default: "py")

**Parsing Rules:**
- "forecast", "predict future", "next N periods" → Forecasting task
- "daily", "weekly", "monthly", "quarterly", "yearly", "hourly" → Set frequency
- "seasonal", "seasonality", "repeating pattern" → Include seasonal component
- "trend", "growing", "declining" → Include trend component
- "holidays", "special events" → Use Prophet with holiday effects
- "multiple variables", "multivariate" → Use VAR
- "external factors", "regressors", "predictors" → Use SARIMAX or Prophet with regressors
- Numbers like "next 12 months", "forecast 30 days" → Extract forecast_horizon
- "auto", "best model", "compare" → Auto mode with model comparison

**Frequency Detection from Keywords:**
- "hourly", "per hour", "H" → frequency="H", seasonal_period=24
- "daily", "per day", "D" → frequency="D", seasonal_period=7 (weekly seasonality)
- "weekly", "per week", "W" → frequency="W", seasonal_period=52
- "monthly", "per month", "M" → frequency="M", seasonal_period=12
- "quarterly", "per quarter", "Q" → frequency="Q", seasonal_period=4
- "yearly", "annual", "Y" → frequency="Y", seasonal_period=1

---

## OUTPUT CODE STRUCTURE

The generated Python file must contain the following sections in order:

### Section 1: Header and Imports

```python
"""
Statistical Forecasting Model: [MODEL_TYPE]
Generated by ModelWiz.xyz
Target Variable: [VALUE_COLUMN]
Forecast Horizon: [FORECAST_HORIZON] periods
Frequency: [FREQUENCY]
Generated on: [TIMESTAMP]

Required packages:
pip install numpy pandas matplotlib seaborn statsmodels scipy
Optional: pip install prophet holidays
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Statistical modeling
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
from statsmodels.tsa.seasonal import seasonal_decompose, STL
from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.api import VAR
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.diagnostic import acorr_ljungbox

# Model selection and metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from scipy import stats

# Optional: Prophet
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("Note: Prophet not installed. Install with: pip install prophet")

# Optional: Holidays
try:
    import holidays
    HOLIDAYS_AVAILABLE = True
except ImportError:
    HOLIDAYS_AVAILABLE = False
    print("Note: holidays package not installed. Install with: pip install holidays")

# Auto ARIMA (optional)
try:
    import pmdarima as pm
    from pmdarima import auto_arima
    PMDARIMA_AVAILABLE = True
except ImportError:
    PMDARIMA_AVAILABLE = False
    print("Note: pmdarima not installed. Using manual ARIMA order selection.")
    print("Install with: pip install pmdarima")
```

### Section 2: Data Loading

```python
# --- DATA LOADING ---
print("=" * 70)
print("TIME SERIES FORECASTING ANALYSIS")
print("=" * 70)

# [GENERATE APPROPRIATE DATA LOADING CODE]
# Options:
# 1. pd.read_csv(filepath) for file input
# 2. Direct DataFrame creation for inline data
# 3. Synthetic time series generation

# Ensure datetime index
df['[DATE_COLUMN]'] = pd.to_datetime(df['[DATE_COLUMN]'])
df = df.set_index('[DATE_COLUMN]')
df = df.sort_index()

# Extract the target series
series = df['[VALUE_COLUMN]']

# Infer frequency if not set
if series.index.freq is None:
    inferred_freq = pd.infer_freq(series.index)
    if inferred_freq:
        series = series.asfreq(inferred_freq)
        print(f"Inferred frequency: {inferred_freq}")
    else:
        # Try to set based on user specification or default
        series = series.asfreq('[FREQUENCY]')
        print(f"Set frequency to: [FREQUENCY]")

print(f"\nDataset loaded: {len(series)} observations")
print(f"Date range: {series.index.min()} to {series.index.max()}")
print(f"Frequency: {series.index.freq}")
```

### Section 3: Exploratory Data Analysis

```python
# --- EXPLORATORY DATA ANALYSIS ---
print("\n" + "-" * 70)
print("EXPLORATORY DATA ANALYSIS")
print("-" * 70)

print("\n=== TIME SERIES SUMMARY ===")
print(f"Number of observations: {len(series)}")
print(f"Start date: {series.index.min()}")
print(f"End date: {series.index.max()}")
print(f"Frequency: {series.index.freq}")

print("\n=== DESCRIPTIVE STATISTICS ===")
print(series.describe().round(2))

# Check for missing values
missing = series.isnull().sum()
if missing > 0:
    print(f"\n⚠️  Missing values detected: {missing} ({missing/len(series)*100:.1f}%)")
    print("Missing values will be interpolated.")
    series = series.interpolate(method='time')
else:
    print("\n✓ No missing values detected")

# Check for duplicates
duplicates = series.index.duplicated().sum()
if duplicates > 0:
    print(f"⚠️  Duplicate timestamps detected: {duplicates}")
    series = series[~series.index.duplicated(keep='first')]

# Basic time series characteristics
print("\n=== TIME SERIES CHARACTERISTICS ===")

# Mean and variance stability (rough check)
n_splits = min(4, len(series) // 30)
if n_splits >= 2:
    splits = np.array_split(series.values, n_splits)
    means = [s.mean() for s in splits]
    variances = [s.var() for s in splits]
    print(f"Mean across {n_splits} periods: {[f'{m:.2f}' for m in means]}")
    print(f"Variance across {n_splits} periods: {[f'{v:.2f}' for v in variances]}")
    
    mean_change = (max(means) - min(means)) / np.mean(means) * 100
    var_change = (max(variances) - min(variances)) / np.mean(variances) * 100
    
    if mean_change > 50:
        print("  → Significant mean change detected (possible trend)")
    if var_change > 100:
        print("  → Significant variance change detected (consider multiplicative model)")

# Seasonality detection
print("\n=== SEASONALITY ANALYSIS ===")
seasonal_period = [SEASONAL_PERIOD]
if len(series) >= 2 * seasonal_period:
    # Compute autocorrelation at seasonal lag
    autocorr = series.autocorr(lag=seasonal_period)
    print(f"Autocorrelation at seasonal lag ({seasonal_period}): {autocorr:.4f}")
    if abs(autocorr) > 0.5:
        print("  → Strong seasonal pattern detected")
    elif abs(autocorr) > 0.3:
        print("  → Moderate seasonal pattern detected")
    else:
        print("  → Weak or no seasonal pattern detected")
else:
    print(f"Insufficient data for seasonality analysis (need at least {2*seasonal_period} observations)")
```

### Section 4: Stationarity Tests

```python
# --- STATIONARITY TESTS ---
print("\n" + "-" * 70)
print("STATIONARITY ANALYSIS")
print("-" * 70)

def adf_test(series, name='Series'):
    """Augmented Dickey-Fuller test for stationarity"""
    result = adfuller(series.dropna(), autolag='AIC')
    print(f"\n=== ADF Test: {name} ===")
    print(f"Test Statistic: {result[0]:.4f}")
    print(f"P-value: {result[1]:.4f}")
    print(f"Lags Used: {result[2]}")
    print(f"Observations: {result[3]}")
    print("Critical Values:")
    for key, value in result[4].items():
        print(f"  {key}: {value:.4f}")
    
    if result[1] < 0.05:
        print("✓ Series is STATIONARY (reject null hypothesis)")
        return True
    else:
        print("✗ Series is NON-STATIONARY (fail to reject null hypothesis)")
        return False

def kpss_test(series, name='Series'):
    """KPSS test for stationarity"""
    result = kpss(series.dropna(), regression='c', nlags='auto')
    print(f"\n=== KPSS Test: {name} ===")
    print(f"Test Statistic: {result[0]:.4f}")
    print(f"P-value: {result[1]:.4f}")
    print(f"Lags Used: {result[2]}")
    print("Critical Values:")
    for key, value in result[3].items():
        print(f"  {key}: {value:.4f}")
    
    if result[1] > 0.05:
        print("✓ Series is STATIONARY (fail to reject null hypothesis)")
        return True
    else:
        print("✗ Series is NON-STATIONARY (reject null hypothesis)")
        return False

# Run tests on original series
is_stationary_adf = adf_test(series, 'Original Series')
is_stationary_kpss = kpss_test(series, 'Original Series')

# Determine differencing needed
differencing_order = 0
test_series = series.copy()

if not is_stationary_adf or not is_stationary_kpss:
    print("\n--- Testing First Difference ---")
    diff_1 = series.diff().dropna()
    is_diff1_stationary = adf_test(diff_1, 'First Difference')
    
    if is_diff1_stationary:
        differencing_order = 1
        test_series = diff_1
    else:
        print("\n--- Testing Second Difference ---")
        diff_2 = series.diff().diff().dropna()
        is_diff2_stationary = adf_test(diff_2, 'Second Difference')
        if is_diff2_stationary:
            differencing_order = 2
            test_series = diff_2
        else:
            print("⚠️  Series may require transformation (log, Box-Cox)")
            differencing_order = 1  # Default to 1

print(f"\n=== STATIONARITY CONCLUSION ===")
print(f"Recommended differencing order (d): {differencing_order}")
```

### Section 5: Time Series Decomposition

```python
# --- TIME SERIES DECOMPOSITION ---
print("\n" + "-" * 70)
print("TIME SERIES DECOMPOSITION")
print("-" * 70)

# Determine decomposition type
# Check if variance increases with level (suggests multiplicative)
rolling_mean = series.rolling(window=seasonal_period).mean()
rolling_std = series.rolling(window=seasonal_period).std()

# Correlation between mean and std suggests multiplicative
if len(rolling_mean.dropna()) > 10:
    corr_mean_std = rolling_mean.corr(rolling_std)
    print(f"\nCorrelation between rolling mean and std: {corr_mean_std:.4f}")
    
    if corr_mean_std > 0.5:
        decomp_type = 'multiplicative'
        print("  → Using MULTIPLICATIVE decomposition (variance increases with level)")
    else:
        decomp_type = 'additive'
        print("  → Using ADDITIVE decomposition (constant variance)")
else:
    decomp_type = '[DECOMPOSITION_TYPE]'  # Use user-specified or default

# Perform decomposition
if len(series) >= 2 * seasonal_period:
    try:
        # STL Decomposition (more robust)
        stl = STL(series, period=seasonal_period, robust=True)
        stl_result = stl.fit()
        
        print("\n=== STL DECOMPOSITION RESULTS ===")
        print(f"Trend strength: {1 - (stl_result.resid.var() / (stl_result.trend + stl_result.resid).var()):.4f}")
        print(f"Seasonal strength: {1 - (stl_result.resid.var() / (stl_result.seasonal + stl_result.resid).var()):.4f}")
        
        # Also do classical decomposition for comparison
        classical_result = seasonal_decompose(series, model=decomp_type, period=seasonal_period)
        
        decomposition_available = True
    except Exception as e:
        print(f"Decomposition failed: {e}")
        decomposition_available = False
else:
    print(f"Insufficient data for seasonal decomposition (need >= {2*seasonal_period} observations)")
    decomposition_available = False
```

### Section 6: ACF/PACF Analysis

```python
# --- ACF/PACF ANALYSIS ---
print("\n" + "-" * 70)
print("ACF/PACF ANALYSIS")
print("-" * 70)

# Compute ACF and PACF
n_lags = min(40, len(test_series) // 4)

acf_values = acf(test_series.dropna(), nlags=n_lags, fft=True)
pacf_values = pacf(test_series.dropna(), nlags=n_lags, method='ywm')

# Confidence interval (95%)
ci = 1.96 / np.sqrt(len(test_series))

# Identify significant lags for AR and MA order selection
significant_acf = np.where(np.abs(acf_values[1:]) > ci)[0] + 1
significant_pacf = np.where(np.abs(pacf_values[1:]) > ci)[0] + 1

print(f"\n95% Confidence Interval: ±{ci:.4f}")
print(f"\nSignificant ACF lags: {significant_acf[:10].tolist()}{'...' if len(significant_acf) > 10 else ''}")
print(f"Significant PACF lags: {significant_pacf[:10].tolist()}{'...' if len(significant_pacf) > 10 else ''}")

# Suggest ARIMA orders based on ACF/PACF
print("\n=== ARIMA ORDER SUGGESTIONS ===")

# AR order (p): Based on PACF cutoff
if len(significant_pacf) == 0:
    suggested_p = 0
elif significant_pacf[0] == 1 and (len(significant_pacf) == 1 or significant_pacf[1] > 2):
    suggested_p = 1
else:
    suggested_p = min(3, max(significant_pacf[:3]) if len(significant_pacf) > 0 else 1)

# MA order (q): Based on ACF cutoff
if len(significant_acf) == 0:
    suggested_q = 0
elif significant_acf[0] == 1 and (len(significant_acf) == 1 or significant_acf[1] > 2):
    suggested_q = 1
else:
    suggested_q = min(3, max(significant_acf[:3]) if len(significant_acf) > 0 else 1)

print(f"Suggested AR order (p): {suggested_p}")
print(f"Suggested differencing (d): {differencing_order}")
print(f"Suggested MA order (q): {suggested_q}")

# Seasonal orders
if seasonal_period > 1 and len(series) >= 2 * seasonal_period:
    # Check for significant seasonal lags
    seasonal_acf = acf_values[seasonal_period] if len(acf_values) > seasonal_period else 0
    seasonal_pacf = pacf_values[seasonal_period] if len(pacf_values) > seasonal_period else 0
    
    suggested_P = 1 if abs(seasonal_pacf) > ci else 0
    suggested_D = 1 if abs(seasonal_acf) > 0.5 else 0
    suggested_Q = 1 if abs(seasonal_acf) > ci else 0
    
    print(f"\nSuggested Seasonal AR (P): {suggested_P}")
    print(f"Suggested Seasonal Differencing (D): {suggested_D}")
    print(f"Suggested Seasonal MA (Q): {suggested_Q}")
    print(f"Seasonal Period (s): {seasonal_period}")
```

### Section 7: Train-Test Split

```python
# --- TRAIN-TEST SPLIT ---
print("\n" + "-" * 70)
print("TRAIN-TEST SPLIT FOR BACKTESTING")
print("-" * 70)

# Split data for model validation
train_size = int(len(series) * [TRAIN_TEST_SPLIT])
train = series[:train_size]
test = series[train_size:]

print(f"\nTraining set: {len(train)} observations ({len(train)/len(series)*100:.1f}%)")
print(f"  Date range: {train.index.min()} to {train.index.max()}")
print(f"\nTest set: {len(test)} observations ({len(test)/len(series)*100:.1f}%)")
print(f"  Date range: {test.index.min()} to {test.index.max()}")

# Forecast horizon for out-of-sample prediction
forecast_horizon = [FORECAST_HORIZON]
print(f"\nForecast horizon: {forecast_horizon} periods")
```

---

## MODEL-SPECIFIC SECTIONS

### Section 8A: ARIMA/SARIMA Model

```python
# --- ARIMA/SARIMA MODEL ---
print("\n" + "-" * 70)
print("ARIMA/SARIMA MODEL FITTING")
print("-" * 70)

# [IF AUTO ARIMA AVAILABLE AND REQUESTED]
if PMDARIMA_AVAILABLE:
    print("\nRunning auto_arima for optimal order selection...")
    
    auto_model = auto_arima(
        train,
        start_p=0, start_q=0,
        max_p=5, max_q=5,
        d=None,  # Let it determine
        start_P=0, start_Q=0,
        max_P=2, max_Q=2,
        D=None,  # Let it determine
        m=seasonal_period,  # Seasonal period
        seasonal=True if seasonal_period > 1 else False,
        trace=True,
        error_action='ignore',
        suppress_warnings=True,
        stepwise=True,
        random_state=42
    )
    
    print(f"\nBest ARIMA order: {auto_model.order}")
    if seasonal_period > 1:
        print(f"Best Seasonal order: {auto_model.seasonal_order}")
    
    order = auto_model.order
    seasonal_order = auto_model.seasonal_order if seasonal_period > 1 else (0, 0, 0, 0)
    
else:
    # Use suggested orders from ACF/PACF analysis
    order = (suggested_p, differencing_order, suggested_q)
    seasonal_order = (suggested_P, suggested_D, suggested_Q, seasonal_period) if seasonal_period > 1 else (0, 0, 0, 0)
    print(f"\nUsing suggested ARIMA order: {order}")
    if seasonal_period > 1:
        print(f"Using suggested Seasonal order: {seasonal_order}")
# [END IF]

# Fit SARIMAX model
print("\nFitting SARIMAX model...")

# [IF EXOGENOUS VARIABLES]
exog_train = train_exog  # Exogenous variables for training
exog_test = test_exog    # Exogenous variables for testing
model = SARIMAX(
    train,
    exog=exog_train,
    order=order,
    seasonal_order=seasonal_order,
    enforce_stationarity=False,
    enforce_invertibility=False
)
# [ELSE]
model = SARIMAX(
    train,
    order=order,
    seasonal_order=seasonal_order,
    enforce_stationarity=False,
    enforce_invertibility=False
)
# [END IF]

model_fit = model.fit(disp=False)

print("\n=== MODEL SUMMARY ===")
print(model_fit.summary().tables[0])
print(model_fit.summary().tables[1])

# Model diagnostics
print("\n=== MODEL DIAGNOSTICS ===")
print(f"AIC: {model_fit.aic:.2f}")
print(f"BIC: {model_fit.bic:.2f}")
print(f"Log-Likelihood: {model_fit.llf:.2f}")

# Ljung-Box test for residual autocorrelation
residuals = model_fit.resid
lb_test = acorr_ljungbox(residuals, lags=[10, 20, 30], return_df=True)
print("\nLjung-Box Test (residual autocorrelation):")
print(lb_test.round(4))

if all(lb_test['lb_pvalue'] > 0.05):
    print("✓ Residuals show no significant autocorrelation (good model fit)")
else:
    print("⚠️  Residuals show autocorrelation (consider different orders)")

# Residual normality test
_, normality_pvalue = stats.normaltest(residuals.dropna())
print(f"\nResidual Normality Test p-value: {normality_pvalue:.4f}")
if normality_pvalue > 0.05:
    print("✓ Residuals are approximately normally distributed")
else:
    print("⚠️  Residuals deviate from normality (prediction intervals may be less reliable)")
```

### Section 8B: Exponential Smoothing Model

```python
# --- EXPONENTIAL SMOOTHING MODEL ---
print("\n" + "-" * 70)
print("EXPONENTIAL SMOOTHING MODEL")
print("-" * 70)

# Determine model type based on data characteristics
has_trend = abs(series.diff().mean()) > series.std() * 0.1
has_seasonality = seasonal_period > 1 and len(series) >= 2 * seasonal_period

print(f"\nData characteristics:")
print(f"  Trend detected: {'Yes' if has_trend else 'No'}")
print(f"  Seasonality detected: {'Yes' if has_seasonality else 'No'}")

# [IF SIMPLE EXPONENTIAL SMOOTHING - no trend, no seasonality]
if not has_trend and not has_seasonality:
    print("\nFitting Simple Exponential Smoothing...")
    model = SimpleExpSmoothing(
        train,
        initialization_method='estimated'
    )
    model_fit = model.fit(optimized=True)
    model_name = "Simple Exponential Smoothing"

# [ELSE IF HOLT'S METHOD - trend, no seasonality]
elif has_trend and not has_seasonality:
    print("\nFitting Holt's Linear Method...")
    model = Holt(
        train,
        exponential=False,  # Try True for multiplicative trend
        damped_trend=True,
        initialization_method='estimated'
    )
    model_fit = model.fit(optimized=True)
    model_name = "Holt's Linear Method (Damped)"

# [ELSE - HOLT-WINTERS - trend and/or seasonality]
else:
    print("\nFitting Holt-Winters Exponential Smoothing...")
    
    # Determine trend and seasonal type
    trend_type = 'add' if has_trend else None
    seasonal_type = decomp_type if has_seasonality else None
    
    model = ExponentialSmoothing(
        train,
        trend=trend_type,
        seasonal=seasonal_type,
        seasonal_periods=seasonal_period if has_seasonality else None,
        damped_trend=True if has_trend else False,
        initialization_method='estimated'
    )
    model_fit = model.fit(optimized=True)
    model_name = f"Holt-Winters ({trend_type} trend, {seasonal_type} seasonal)"
# [END IF]

print(f"\n=== {model_name.upper()} RESULTS ===")
print(f"\nSmoothing Parameters:")
if hasattr(model_fit, 'params'):
    for param, value in model_fit.params.items():
        if value is not None and not np.isnan(value):
            print(f"  {param}: {value:.4f}")

print(f"\nModel Metrics:")
print(f"  AIC: {model_fit.aic:.2f}")
print(f"  BIC: {model_fit.bic:.2f}")
print(f"  SSE: {model_fit.sse:.2f}")
```

### Section 8C: Prophet Model

```python
# --- PROPHET MODEL ---
print("\n" + "-" * 70)
print("PROPHET MODEL")
print("-" * 70)

if not PROPHET_AVAILABLE:
    print("⚠️  Prophet is not installed. Skipping Prophet model.")
    print("Install with: pip install prophet")
else:
    # Prepare data for Prophet (requires 'ds' and 'y' columns)
    prophet_train = train.reset_index()
    prophet_train.columns = ['ds', 'y']
    
    prophet_test = test.reset_index()
    prophet_test.columns = ['ds', 'y']
    
    # Initialize Prophet model
    print("\nFitting Prophet model...")
    
    prophet_model = Prophet(
        yearly_seasonality=True if seasonal_period >= 365 or frequency in ['D', 'W'] else False,
        weekly_seasonality=True if frequency in ['D', 'H'] else False,
        daily_seasonality=True if frequency == 'H' else False,
        seasonality_mode=decomp_type,  # 'additive' or 'multiplicative'
        changepoint_prior_scale=0.05,  # Flexibility of trend
        seasonality_prior_scale=10,
        interval_width=[CONFIDENCE_LEVEL]
    )
    
    # [IF HOLIDAY_COUNTRY]
    # Add country holidays
    if HOLIDAYS_AVAILABLE:
        prophet_model.add_country_holidays(country_name='[HOLIDAY_COUNTRY]')
        print(f"Added holidays for: [HOLIDAY_COUNTRY]")
    # [END IF]
    
    # [IF EXOGENOUS VARIABLES]
    # Add regressors
    for regressor in [EXOGENOUS_VARIABLES]:
        prophet_model.add_regressor(regressor)
        prophet_train[regressor] = train_exog[regressor].values
    # [END IF]
    
    prophet_model.fit(prophet_train)
    
    print("\n=== PROPHET MODEL COMPONENTS ===")
    
    # Print trend changepoints
    print(f"\nNumber of trend changepoints: {len(prophet_model.changepoints)}")
    if len(prophet_model.changepoints) > 0:
        print(f"Changepoint dates: {prophet_model.changepoints[:5].tolist()}...")
    
    # Seasonality components
    print("\nSeasonality components:")
    for name, params in prophet_model.seasonalities.items():
        print(f"  {name}: period={params['period']}, fourier_order={params['fourier_order']}")
```

### Section 8D: Model Comparison (Auto Mode)

```python
# --- MODEL COMPARISON ---
print("\n" + "-" * 70)
print("MODEL COMPARISON")
print("-" * 70)

def evaluate_forecast(actual, predicted, model_name):
    """Calculate forecast accuracy metrics"""
    # Align series
    actual = actual.values if hasattr(actual, 'values') else actual
    predicted = predicted.values if hasattr(predicted, 'values') else predicted
    
    # Handle any NaN
    mask = ~(np.isnan(actual) | np.isnan(predicted))
    actual = actual[mask]
    predicted = predicted[mask]
    
    if len(actual) == 0:
        return None
    
    mae = mean_absolute_error(actual, predicted)
    rmse = np.sqrt(mean_squared_error(actual, predicted))
    mape = mean_absolute_percentage_error(actual, predicted) * 100
    
    # Mean Absolute Scaled Error (MASE) - scale-independent
    naive_mae = np.mean(np.abs(np.diff(actual)))
    mase = mae / naive_mae if naive_mae > 0 else np.inf
    
    return {
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'MAPE': mape,
        'MASE': mase
    }

# Store results for all models
comparison_results = []
fitted_models = {}
forecasts = {}

# 1. SARIMA
print("\n1. Fitting SARIMA...")
try:
    sarima = SARIMAX(train, order=order, seasonal_order=seasonal_order,
                     enforce_stationarity=False, enforce_invertibility=False)
    sarima_fit = sarima.fit(disp=False)
    sarima_pred = sarima_fit.get_forecast(steps=len(test))
    sarima_forecast = sarima_pred.predicted_mean
    
    result = evaluate_forecast(test, sarima_forecast, 'SARIMA')
    if result:
        comparison_results.append(result)
        fitted_models['SARIMA'] = sarima_fit
        forecasts['SARIMA'] = sarima_forecast
        print(f"   SARIMA RMSE: {result['RMSE']:.4f}")
except Exception as e:
    print(f"   SARIMA failed: {e}")

# 2. Holt-Winters
print("\n2. Fitting Holt-Winters...")
try:
    hw = ExponentialSmoothing(train, trend='add', seasonal=decomp_type,
                               seasonal_periods=seasonal_period, damped_trend=True)
    hw_fit = hw.fit(optimized=True)
    hw_forecast = hw_fit.forecast(len(test))
    
    result = evaluate_forecast(test, hw_forecast, 'Holt-Winters')
    if result:
        comparison_results.append(result)
        fitted_models['Holt-Winters'] = hw_fit
        forecasts['Holt-Winters'] = hw_forecast
        print(f"   Holt-Winters RMSE: {result['RMSE']:.4f}")
except Exception as e:
    print(f"   Holt-Winters failed: {e}")

# 3. Simple Exponential Smoothing (baseline)
print("\n3. Fitting Simple Exponential Smoothing...")
try:
    ses = SimpleExpSmoothing(train)
    ses_fit = ses.fit(optimized=True)
    ses_forecast = ses_fit.forecast(len(test))
    
    result = evaluate_forecast(test, ses_forecast, 'Simple Exp Smoothing')
    if result:
        comparison_results.append(result)
        fitted_models['Simple Exp Smoothing'] = ses_fit
        forecasts['Simple Exp Smoothing'] = ses_forecast
        print(f"   SES RMSE: {result['RMSE']:.4f}")
except Exception as e:
    print(f"   SES failed: {e}")

# 4. Prophet (if available)
if PROPHET_AVAILABLE:
    print("\n4. Fitting Prophet...")
    try:
        prophet = Prophet(yearly_seasonality='auto', weekly_seasonality='auto',
                          seasonality_mode=decomp_type, interval_width=[CONFIDENCE_LEVEL])
        prophet_df = train.reset_index()
        prophet_df.columns = ['ds', 'y']
        prophet.fit(prophet_df)
        
        future = prophet.make_future_dataframe(periods=len(test), freq=series.index.freq)
        prophet_pred = prophet.predict(future)
        prophet_forecast = prophet_pred.iloc[-len(test):]['yhat'].values
        
        result = evaluate_forecast(test.values, prophet_forecast, 'Prophet')
        if result:
            comparison_results.append(result)
            fitted_models['Prophet'] = prophet
            forecasts['Prophet'] = pd.Series(prophet_forecast, index=test.index)
            print(f"   Prophet RMSE: {result['RMSE']:.4f}")
    except Exception as e:
        print(f"   Prophet failed: {e}")

# 5. Naive baseline (last value)
print("\n5. Computing Naive Forecast (baseline)...")
naive_forecast = pd.Series([train.iloc[-1]] * len(test), index=test.index)
result = evaluate_forecast(test, naive_forecast, 'Naive (Last Value)')
if result:
    comparison_results.append(result)
    forecasts['Naive'] = naive_forecast
    print(f"   Naive RMSE: {result['RMSE']:.4f}")

# Comparison summary
comparison_df = pd.DataFrame(comparison_results)
comparison_df = comparison_df.sort_values('RMSE')

print("\n" + "=" * 70)
print("MODEL COMPARISON SUMMARY")
print("=" * 70)
print(comparison_df.round(4).to_string(index=False))

# Select best model
best_model_name = comparison_df.iloc[0]['Model']
print(f"\n✓ BEST MODEL: {best_model_name}")
print(f"  RMSE: {comparison_df.iloc[0]['RMSE']:.4f}")
print(f"  MAPE: {comparison_df.iloc[0]['MAPE']:.2f}%")
print(f"  MASE: {comparison_df.iloc[0]['MASE']:.4f}")

# Use best model for final forecast
model_fit = fitted_models.get(best_model_name)
model_name = best_model_name
```

### Section 9: Generate Forecast

```python
# --- GENERATE FORECAST ---
print("\n" + "-" * 70)
print("GENERATING FORECAST")
print("-" * 70)

# Refit model on full data for production forecast
print(f"\nRefitting {model_name} on full dataset for production forecast...")

# [IF SARIMA]
if model_name in ['SARIMA', 'ARIMA']:
    full_model = SARIMAX(series, order=order, seasonal_order=seasonal_order,
                          enforce_stationarity=False, enforce_invertibility=False)
    full_fit = full_model.fit(disp=False)
    
    # Generate forecast with confidence intervals
    forecast_result = full_fit.get_forecast(steps=forecast_horizon)
    forecast_values = forecast_result.predicted_mean
    conf_int = forecast_result.conf_int(alpha=1-[CONFIDENCE_LEVEL])
    forecast_lower = conf_int.iloc[:, 0]
    forecast_upper = conf_int.iloc[:, 1]

# [ELSE IF EXPONENTIAL SMOOTHING]
elif model_name in ['Holt-Winters', 'Simple Exp Smoothing', "Holt's Linear"]:
    if model_name == 'Holt-Winters':
        full_model = ExponentialSmoothing(series, trend='add', seasonal=decomp_type,
                                           seasonal_periods=seasonal_period, damped_trend=True)
    elif model_name == 'Simple Exp Smoothing':
        full_model = SimpleExpSmoothing(series)
    else:
        full_model = Holt(series, damped_trend=True)
    
    full_fit = full_model.fit(optimized=True)
    forecast_values = full_fit.forecast(forecast_horizon)
    
    # Simulate confidence intervals for exponential smoothing
    residual_std = full_fit.resid.std()
    z_score = stats.norm.ppf((1 + [CONFIDENCE_LEVEL]) / 2)
    
    # Widen intervals for longer horizons
    horizon_factor = np.sqrt(np.arange(1, forecast_horizon + 1))
    forecast_lower = forecast_values - z_score * residual_std * horizon_factor
    forecast_upper = forecast_values + z_score * residual_std * horizon_factor

# [ELSE IF PROPHET]
elif model_name == 'Prophet':
    full_prophet = Prophet(yearly_seasonality='auto', weekly_seasonality='auto',
                           seasonality_mode=decomp_type, interval_width=[CONFIDENCE_LEVEL])
    full_df = series.reset_index()
    full_df.columns = ['ds', 'y']
    full_prophet.fit(full_df)
    
    future = full_prophet.make_future_dataframe(periods=forecast_horizon, freq=series.index.freq)
    prediction = full_prophet.predict(future)
    
    forecast_values = prediction.iloc[-forecast_horizon:]['yhat']
    forecast_lower = prediction.iloc[-forecast_horizon:]['yhat_lower']
    forecast_upper = prediction.iloc[-forecast_horizon:]['yhat_upper']
    forecast_values.index = future.iloc[-forecast_horizon:]['ds']
    forecast_lower.index = forecast_values.index
    forecast_upper.index = forecast_values.index
# [END IF]

# Create forecast DataFrame
forecast_df = pd.DataFrame({
    'Date': forecast_values.index,
    'Forecast': forecast_values.values,
    'Lower_CI': forecast_lower.values,
    'Upper_CI': forecast_upper.values
})
forecast_df['Date'] = pd.to_datetime(forecast_df['Date'])
forecast_df = forecast_df.set_index('Date')

print(f"\n=== FORECAST RESULTS ({forecast_horizon} periods) ===")
print(f"Confidence Level: {[CONFIDENCE_LEVEL]*100:.0f}%")
print(f"\nForecast Summary:")
print(forecast_df.round(2))

print(f"\n=== FORECAST STATISTICS ===")
print(f"Mean forecast value: {forecast_df['Forecast'].mean():.2f}")
print(f"Min forecast value: {forecast_df['Forecast'].min():.2f}")
print(f"Max forecast value: {forecast_df['Forecast'].max():.2f}")
print(f"Average CI width: {(forecast_df['Upper_CI'] - forecast_df['Lower_CI']).mean():.2f}")
```

### Section 10: Visualizations

```python
# --- VISUALIZATIONS ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

fig = plt.figure(figsize=(16, 14))

# Plot 1: Full Time Series with Forecast
ax1 = fig.add_subplot(3, 2, 1)
ax1.plot(series.index, series.values, 'b-', linewidth=1.5, label='Historical Data')
ax1.plot(forecast_df.index, forecast_df['Forecast'], 'r-', linewidth=2, label='Forecast')
ax1.fill_between(forecast_df.index, forecast_df['Lower_CI'], forecast_df['Upper_CI'],
                  color='red', alpha=0.2, label=f'{[CONFIDENCE_LEVEL]*100:.0f}% CI')
ax1.axvline(x=series.index[-1], color='gray', linestyle='--', alpha=0.7, label='Forecast Start')
ax1.set_xlabel('Date', fontsize=11)
ax1.set_ylabel('[VALUE_COLUMN]', fontsize=11)
ax1.set_title(f'Time Series Forecast - {model_name}', fontsize=12)
ax1.legend(loc='upper left')
ax1.grid(True, alpha=0.3)

# Plot 2: Zoomed Forecast (last N historical + forecast)
ax2 = fig.add_subplot(3, 2, 2)
zoom_periods = min(len(series), forecast_horizon * 3)
recent_data = series.iloc[-zoom_periods:]
ax2.plot(recent_data.index, recent_data.values, 'b-o', linewidth=1.5, markersize=4, label='Historical')
ax2.plot(forecast_df.index, forecast_df['Forecast'], 'r-o', linewidth=2, markersize=6, label='Forecast')
ax2.fill_between(forecast_df.index, forecast_df['Lower_CI'], forecast_df['Upper_CI'],
                  color='red', alpha=0.2, label=f'{[CONFIDENCE_LEVEL]*100:.0f}% CI')
ax2.axvline(x=series.index[-1], color='gray', linestyle='--', alpha=0.7)
ax2.set_xlabel('Date', fontsize=11)
ax2.set_ylabel('[VALUE_COLUMN]', fontsize=11)
ax2.set_title('Zoomed View: Recent Data + Forecast', fontsize=12)
ax2.legend(loc='upper left')
ax2.grid(True, alpha=0.3)

# Plot 3: Seasonal Decomposition (if available)
ax3 = fig.add_subplot(3, 2, 3)
if decomposition_available:
    ax3.plot(stl_result.trend.index, stl_result.trend.values, 'b-', linewidth=1.5, label='Trend')
    ax3.set_xlabel('Date', fontsize=11)
    ax3.set_ylabel('Trend Component', fontsize=11)
    ax3.set_title('Extracted Trend Component (STL)', fontsize=12)
    ax3.grid(True, alpha=0.3)
else:
    ax3.text(0.5, 0.5, 'Decomposition not available\n(insufficient data)', 
             ha='center', va='center', fontsize=12)
    ax3.set_title('Trend Component', fontsize=12)

# Plot 4: Seasonal Component (if available)
ax4 = fig.add_subplot(3, 2, 4)
if decomposition_available:
    # Show one full seasonal cycle
    seasonal_data = stl_result.seasonal[:seasonal_period*2] if len(stl_result.seasonal) > seasonal_period*2 else stl_result.seasonal
    ax4.plot(range(len(seasonal_data)), seasonal_data.values, 'g-', linewidth=1.5)
    ax4.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
    ax4.set_xlabel('Period', fontsize=11)
    ax4.set_ylabel('Seasonal Component', fontsize=11)
    ax4.set_title(f'Seasonal Pattern (Period = {seasonal_period})', fontsize=12)
    ax4.grid(True, alpha=0.3)
else:
    ax4.text(0.5, 0.5, 'Seasonal analysis not available', ha='center', va='center', fontsize=12)
    ax4.set_title('Seasonal Component', fontsize=12)

# Plot 5: ACF Plot
ax5 = fig.add_subplot(3, 2, 5)
plot_acf(test_series.dropna(), lags=n_lags, ax=ax5, alpha=0.05)
ax5.set_xlabel('Lag', fontsize=11)
ax5.set_ylabel('Autocorrelation', fontsize=11)
ax5.set_title('Autocorrelation Function (ACF)', fontsize=12)

# Plot 6: Residuals Analysis
ax6 = fig.add_subplot(3, 2, 6)
if hasattr(full_fit, 'resid'):
    residuals = full_fit.resid.dropna()
elif model_name == 'Prophet':
    # Prophet residuals
    fitted = full_prophet.predict(full_df)['yhat'].values[:len(series)]
    residuals = series.values - fitted
else:
    residuals = series.values[1:] - series.values[:-1]  # Naive residuals

ax6.hist(residuals, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='k')
# Overlay normal distribution
x_range = np.linspace(residuals.min(), residuals.max(), 100)
ax6.plot(x_range, stats.norm.pdf(x_range, residuals.mean(), residuals.std()),
         'r-', linewidth=2, label='Normal Distribution')
ax6.axvline(x=0, color='black', linestyle='--', alpha=0.7)
ax6.set_xlabel('Residual Value', fontsize=11)
ax6.set_ylabel('Density', fontsize=11)
ax6.set_title('Residual Distribution', fontsize=12)
ax6.legend()

plt.tight_layout()
plt.savefig('forecast_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'forecast_analysis.png'")

# Additional: Model Comparison Plot (if multiple models)
if len(comparison_results) > 1:
    fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))
    
    # Metric comparison bar chart
    ax_bar = axes2[0]
    metrics = ['RMSE', 'MAPE', 'MASE']
    x = np.arange(len(comparison_df))
    width = 0.25
    
    for i, metric in enumerate(metrics):
        offset = (i - 1) * width
        bars = ax_bar.bar(x + offset, comparison_df[metric], width, label=metric)
    
    ax_bar.set_xticks(x)
    ax_bar.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
    ax_bar.set_ylabel('Error Metric Value', fontsize=11)
    ax_bar.set_title('Model Comparison: Error Metrics', fontsize=12)
    ax_bar.legend()
    ax_bar.grid(True, alpha=0.3, axis='y')
    
    # Forecast comparison on test set
    ax_fc = axes2[1]
    ax_fc.plot(test.index, test.values, 'k-', linewidth=2, label='Actual')
    colors = plt.cm.tab10(np.linspace(0, 1, len(forecasts)))
    for (name, fc), color in zip(forecasts.items(), colors):
        if name != 'Naive':
            ax_fc.plot(test.index[:len(fc)], fc.values[:len(test)], 
                      linestyle='--', linewidth=1.5, color=color, label=name)
    ax_fc.set_xlabel('Date', fontsize=11)
    ax_fc.set_ylabel('[VALUE_COLUMN]', fontsize=11)
    ax_fc.set_title('Model Comparison: Test Set Predictions', fontsize=12)
    ax_fc.legend(loc='upper left')
    ax_fc.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("\nModel comparison saved as 'model_comparison.png'")
```

### Section 11: Forecast Function

```python
# --- FORECAST FUNCTION ---

def generate_forecast(historical_data, periods, model_type='[MODEL_TYPE]'):
    """
    Generate forecast for new data.
    
    Parameters:
    -----------
    historical_data : pd.Series
        Time series data with datetime index
    periods : int
        Number of periods to forecast
    model_type : str
        'sarima', 'holtwinters', 'prophet', or 'auto'
    
    Returns:
    --------
    dict with:
        - forecast: pd.Series of point forecasts
        - lower_ci: pd.Series of lower confidence bound
        - upper_ci: pd.Series of upper confidence bound
        - model_info: dict with model details
    """
    
    # Ensure datetime index and frequency
    if not isinstance(historical_data.index, pd.DatetimeIndex):
        historical_data.index = pd.to_datetime(historical_data.index)
    
    if historical_data.index.freq is None:
        freq = pd.infer_freq(historical_data.index)
        if freq:
            historical_data = historical_data.asfreq(freq)
    
    # Fit model
    if model_type.lower() == 'sarima':
        model = SARIMAX(historical_data, order=order, seasonal_order=seasonal_order,
                        enforce_stationarity=False, enforce_invertibility=False)
        fit = model.fit(disp=False)
        result = fit.get_forecast(steps=periods)
        forecast = result.predicted_mean
        ci = result.conf_int(alpha=1-[CONFIDENCE_LEVEL])
        lower = ci.iloc[:, 0]
        upper = ci.iloc[:, 1]
        
    elif model_type.lower() in ['holtwinters', 'hw', 'ets']:
        model = ExponentialSmoothing(historical_data, trend='add', seasonal=decomp_type,
                                     seasonal_periods=seasonal_period, damped_trend=True)
        fit = model.fit(optimized=True)
        forecast = fit.forecast(periods)
        
        # Confidence intervals
        std = fit.resid.std()
        z = stats.norm.ppf((1 + [CONFIDENCE_LEVEL]) / 2)
        horizon_factor = np.sqrt(np.arange(1, periods + 1))
        lower = forecast - z * std * horizon_factor
        upper = forecast + z * std * horizon_factor
        
    elif model_type.lower() == 'prophet' and PROPHET_AVAILABLE:
        prophet = Prophet(seasonality_mode=decomp_type, interval_width=[CONFIDENCE_LEVEL])
        df = historical_data.reset_index()
        df.columns = ['ds', 'y']
        prophet.fit(df)
        future = prophet.make_future_dataframe(periods=periods, freq=historical_data.index.freq)
        pred = prophet.predict(future)
        forecast = pred.iloc[-periods:]['yhat']
        lower = pred.iloc[-periods:]['yhat_lower']
        upper = pred.iloc[-periods:]['yhat_upper']
        forecast.index = future.iloc[-periods:]['ds']
        lower.index = forecast.index
        upper.index = forecast.index
    
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    return {
        'forecast': forecast,
        'lower_ci': lower,
        'upper_ci': upper,
        'model_info': {
            'model_type': model_type,
            'periods': periods,
            'confidence_level': [CONFIDENCE_LEVEL]
        }
    }

# Example usage
print("\n--- FORECAST FUNCTION EXAMPLE ---")
print("""
# Generate a new forecast:
result = generate_forecast(
    historical_data=series,
    periods=12,
    model_type='sarima'
)

print("Forecast:", result['forecast'])
print("Lower CI:", result['lower_ci'])
print("Upper CI:", result['upper_ci'])
""")
```

### Section 12: Model Persistence

```python
# --- SAVE MODEL AND ARTIFACTS ---
print("\n" + "-" * 70)
print("SAVING MODEL ARTIFACTS")
print("-" * 70)

import joblib
import json
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save model (method depends on model type)
if model_name in ['SARIMA', 'ARIMA']:
    model_filename = f'forecast_sarima_{timestamp}.pkl'
    joblib.dump(full_fit, model_filename)
elif model_name in ['Holt-Winters', 'Simple Exp Smoothing']:
    model_filename = f'forecast_expsmooth_{timestamp}.pkl'
    joblib.dump(full_fit, model_filename)
elif model_name == 'Prophet':
    model_filename = f'forecast_prophet_{timestamp}.pkl'
    # Prophet models need special serialization
    with open(model_filename, 'wb') as f:
        joblib.dump(full_prophet, f)
else:
    model_filename = f'forecast_model_{timestamp}.pkl'
    joblib.dump(full_fit, model_filename)

print(f"Model saved: {model_filename}")

# Save forecast results
forecast_filename = f'forecast_results_{timestamp}.csv'
forecast_df.to_csv(forecast_filename)
print(f"Forecast saved: {forecast_filename}")

# Save configuration
config = {
    'model_type': model_name,
    'order': list(order) if model_name in ['SARIMA', 'ARIMA'] else None,
    'seasonal_order': list(seasonal_order) if model_name in ['SARIMA', 'ARIMA'] else None,
    'seasonal_period': seasonal_period,
    'frequency': str(series.index.freq),
    'decomposition_type': decomp_type,
    'confidence_level': [CONFIDENCE_LEVEL],
    'forecast_horizon': forecast_horizon,
    'value_column': '[VALUE_COLUMN]',
    'timestamp': timestamp,
    'metrics': {
        'test_rmse': float(comparison_df[comparison_df['Model'] == model_name]['RMSE'].values[0]) if len(comparison_df) > 0 else None,
        'test_mape': float(comparison_df[comparison_df['Model'] == model_name]['MAPE'].values[0]) if len(comparison_df) > 0 else None
    }
}

config_filename = f'forecast_config_{timestamp}.json'
with open(config_filename, 'w') as f:
    json.dump(config, f, indent=2, default=str)
print(f"Configuration saved: {config_filename}")

# Save historical data for reproducibility
data_filename = f'historical_data_{timestamp}.csv'
series.to_csv(data_filename)
print(f"Historical data saved: {data_filename}")

print("\n" + "=" * 70)
print("FORECASTING ANALYSIS COMPLETE")
print("=" * 70)
print(f"\nBest Model: {model_name}")
if len(comparison_df) > 0:
    print(f"Test RMSE: {comparison_df[comparison_df['Model'] == model_name]['RMSE'].values[0]:.4f}")
    print(f"Test MAPE: {comparison_df[comparison_df['Model'] == model_name]['MAPE'].values[0]:.2f}%")
print(f"\nForecast generated for next {forecast_horizon} periods")
print(f"Confidence level: {[CONFIDENCE_LEVEL]*100:.0f}%")

print("\nFiles generated:")
print(f"  - {model_filename}")
print(f"  - {forecast_filename}")
print(f"  - {config_filename}")
print(f"  - {data_filename}")
print(f"  - forecast_analysis.png")
if len(comparison_results) > 1:
    print(f"  - model_comparison.png")

# --- LOADING INSTRUCTIONS ---
"""
To load and use this model later:

import joblib
import json
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Load artifacts
model = joblib.load('[model_filename]')
with open('[config_filename]', 'r') as f:
    config = json.load(f)

# Load new data
new_data = pd.read_csv('new_data.csv', index_col=0, parse_dates=True)
new_data = new_data.asfreq(config['frequency'])

# For SARIMA: Extend the model with new data
# model = model.extend(new_data)
# forecast = model.get_forecast(steps=12)

# For simpler forecasts, use the generate_forecast function from this script
"""
```

---

## ERROR HANDLING

```python
# At data loading
try:
    df = pd.read_csv('[FILEPATH]', parse_dates=['[DATE_COLUMN]'])
except FileNotFoundError:
    print("ERROR: Data file not found. Please check the file path.")
    raise
except Exception as e:
    print(f"ERROR loading data: {e}")
    raise

# Validate time series requirements
if len(series) < 30:
    print("⚠️  WARNING: Very short time series (<30 observations). Results may be unreliable.")
    print("   Consider using simpler models (Simple Exponential Smoothing).")

if len(series) < 2 * seasonal_period:
    print(f"⚠️  WARNING: Insufficient data for seasonal analysis (need >= {2*seasonal_period} observations).")
    print("   Seasonal components will be disabled.")
    seasonal_period = 1

# Check for constant values
if series.std() == 0:
    raise ValueError("ERROR: Time series has zero variance (constant values). Cannot forecast.")

# Check for too many missing values
missing_pct = series.isnull().sum() / len(series)
if missing_pct > 0.3:
    print(f"⚠️  WARNING: {missing_pct*100:.1f}% missing values. Consider data quality review.")

# Model fitting errors
try:
    model_fit = model.fit(disp=False)
except Exception as e:
    print(f"ERROR fitting model: {e}")
    print("Attempting simpler model specification...")
    # Fall back to simpler model
    order = (1, 1, 1)
    seasonal_order = (0, 0, 0, 0)
    model = SARIMAX(train, order=order, seasonal_order=seasonal_order)
    model_fit = model.fit(disp=False)
```

---

## FILE DELIVERY

- Generate file named: `forecast_[MODEL_TYPE]_[TIMESTAMP].py`
- Timestamp format: YYYYMMDD_HHMMSS
- Provide as downloadable file
- All code must be syntactically valid Python 3.8+
- Required libraries listed at top with installation instructions

---

## EXAMPLE INPUT

```
Build me a time series forecasting model for monthly retail sales data. I have sales 
figures from January 2018 through October 2025 (about 7 years of monthly data). The 
data shows strong yearly seasonality with peaks in November-December (holiday shopping) 
and troughs in January-February. There's also an upward trend over time.

Generate synthetic data that mimics this pattern: base sales around $2 million per month, 
growing at about 5% per year, with December sales typically 40% higher than average and 
January sales 20% lower than average. Add some random noise.

I want to forecast the next 12 months (November 2025 through October 2026). Compare 
SARIMA, Holt-Winters, and Prophet models to find the best one. Use an 80/20 train-test 
split for validation. Give me 95% confidence intervals.

Output as a Python script.
```

---

## EXAMPLE OUTPUT

File: `forecast_comparison_20251203_163512.py`

```python
"""
Statistical Forecasting Model: Model Comparison (SARIMA, Holt-Winters, Prophet)
Generated by ModelWiz.xyz
Target Variable: Sales
Forecast Horizon: 12 months
Frequency: Monthly (MS)
Generated on: 2025-12-03 16:35:12

Required packages:
pip install numpy pandas matplotlib seaborn statsmodels scipy pmdarima prophet
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Statistical modeling
from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
from statsmodels.tsa.seasonal import seasonal_decompose, STL
from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.stats.diagnostic import acorr_ljungbox

# Metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from scipy import stats

# Auto ARIMA
try:
    from pmdarima import auto_arima
    PMDARIMA_AVAILABLE = True
except ImportError:
    PMDARIMA_AVAILABLE = False
    print("Note: pmdarima not installed. Install with: pip install pmdarima")

# Prophet
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    PROPHET_AVAILABLE = False
    print("Note: Prophet not installed. Install with: pip install prophet")

print("=" * 70)
print("RETAIL SALES FORECASTING MODEL")
print("=" * 70)

# --- DATA GENERATION ---
print("\n" + "-" * 70)
print("GENERATING SYNTHETIC RETAIL SALES DATA")
print("-" * 70)

np.random.seed(42)

# Generate date range: January 2018 to October 2025
dates = pd.date_range(start='2018-01-01', end='2025-10-01', freq='MS')
n_months = len(dates)

print(f"\nGenerating {n_months} months of data: {dates[0].strftime('%B %Y')} to {dates[-1].strftime('%B %Y')}")

# Base sales: $2 million
base_sales = 2_000_000

# Trend: 5% annual growth (compounded monthly)
monthly_growth = (1.05) ** (1/12) - 1
trend = base_sales * (1 + monthly_growth) ** np.arange(n_months)

# Seasonality: Monthly seasonal factors (multiplicative)
# December: +40%, January: -20%, February: -15%, etc.
seasonal_factors = {
    1: 0.80,   # January (post-holiday slump)
    2: 0.85,   # February
    3: 0.95,   # March
    4: 1.00,   # April
    5: 1.00,   # May
    6: 0.98,   # June
    7: 0.95,   # July
    8: 1.02,   # August (back to school)
    9: 1.05,   # September
    10: 1.08,  # October (pre-holiday)
    11: 1.15,  # November (Black Friday)
    12: 1.40,  # December (holiday peak)
}

seasonality = np.array([seasonal_factors[d.month] for d in dates])

# Combine trend and seasonality
sales = trend * seasonality

# Add noise (random variation ~5% of sales)
noise = np.random.normal(0, 0.05, n_months)
sales = sales * (1 + noise)

# Ensure non-negative
sales = np.maximum(sales, 0)

# Create DataFrame
df = pd.DataFrame({
    'Date': dates,
    'Sales': sales.round(2)
})
df = df.set_index('Date')
df.index.freq = 'MS'

series = df['Sales']

print(f"\nData generated successfully!")
print(f"Date range: {series.index.min().strftime('%Y-%m-%d')} to {series.index.max().strftime('%Y-%m-%d')}")
print(f"Number of observations: {len(series)}")

# --- EXPLORATORY DATA ANALYSIS ---
print("\n" + "-" * 70)
print("EXPLORATORY DATA ANALYSIS")
print("-" * 70)

print("\n=== DESCRIPTIVE STATISTICS ===")
print(f"Mean sales: ${series.mean():,.2f}")
print(f"Std deviation: ${series.std():,.2f}")
print(f"Min sales: ${series.min():,.2f} ({series.idxmin().strftime('%B %Y')})")
print(f"Max sales: ${series.max():,.2f} ({series.idxmax().strftime('%B %Y')})")
print(f"Coefficient of variation: {series.std()/series.mean()*100:.1f}%")

# Year-over-year growth
print("\n=== YEAR-OVER-YEAR ANALYSIS ===")
yearly_sales = series.groupby(series.index.year).sum()
for i in range(1, len(yearly_sales)):
    year = yearly_sales.index[i]
    prev_year = yearly_sales.index[i-1]
    growth = (yearly_sales[year] - yearly_sales[prev_year]) / yearly_sales[prev_year] * 100
    print(f"{prev_year} → {year}: {growth:+.1f}% growth")

# Monthly patterns
print("\n=== MONTHLY PATTERNS (Average by Month) ===")
monthly_avg = series.groupby(series.index.month).mean()
month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
for month, avg in monthly_avg.items():
    pct_of_mean = (avg / series.mean() - 1) * 100
    bar = '█' * int(abs(pct_of_mean) / 2)
    sign = '+' if pct_of_mean > 0 else ''
    print(f"  {month_names[month-1]}: ${avg:,.0f} ({sign}{pct_of_mean:.1f}%) {bar}")

# Configuration
seasonal_period = 12  # Monthly data with yearly seasonality
frequency = 'MS'
confidence_level = 0.95
decomp_type = 'multiplicative'

# --- STATIONARITY TESTS ---
print("\n" + "-" * 70)
print("STATIONARITY ANALYSIS")
print("-" * 70)

def adf_test(series, name='Series'):
    """Augmented Dickey-Fuller test"""
    result = adfuller(series.dropna(), autolag='AIC')
    print(f"\n=== ADF Test: {name} ===")
    print(f"Test Statistic: {result[0]:.4f}")
    print(f"P-value: {result[1]:.4f}")
    
    if result[1] < 0.05:
        print("✓ Series is STATIONARY")
        return True
    else:
        print("✗ Series is NON-STATIONARY")
        return False

is_stationary = adf_test(series, 'Original Sales Series')

# Test differenced series
if not is_stationary:
    print("\n--- Testing First Difference ---")
    diff_1 = series.diff().dropna()
    is_diff1_stationary = adf_test(diff_1, 'First Difference')
    differencing_order = 1 if is_diff1_stationary else 2
else:
    differencing_order = 0

# --- TIME SERIES DECOMPOSITION ---
print("\n" + "-" * 70)
print("TIME SERIES DECOMPOSITION")
print("-" * 70)

# STL Decomposition
stl = STL(series, period=seasonal_period, robust=True)
stl_result = stl.fit()

trend_strength = 1 - (stl_result.resid.var() / (stl_result.trend + stl_result.resid).var())
seasonal_strength = 1 - (stl_result.resid.var() / (stl_result.seasonal + stl_result.resid).var())

print(f"\n=== STL DECOMPOSITION RESULTS ===")
print(f"Trend strength: {trend_strength:.4f} (0=weak, 1=strong)")
print(f"Seasonal strength: {seasonal_strength:.4f} (0=weak, 1=strong)")

if trend_strength > 0.6:
    print("  → Strong trend component detected")
if seasonal_strength > 0.6:
    print("  → Strong seasonal component detected")

# --- TRAIN-TEST SPLIT ---
print("\n" + "-" * 70)
print("TRAIN-TEST SPLIT")
print("-" * 70)

train_size = int(len(series) * 0.8)
train = series[:train_size]
test = series[train_size:]

print(f"\nTraining set: {len(train)} months ({train.index.min().strftime('%Y-%m')} to {train.index.max().strftime('%Y-%m')})")
print(f"Test set: {len(test)} months ({test.index.min().strftime('%Y-%m')} to {test.index.max().strftime('%Y-%m')})")

forecast_horizon = 12
print(f"\nForecast horizon: {forecast_horizon} months")

# --- MODEL COMPARISON ---
print("\n" + "-" * 70)
print("MODEL COMPARISON")
print("-" * 70)

def evaluate_forecast(actual, predicted, model_name):
    """Calculate forecast accuracy metrics"""
    actual_vals = actual.values if hasattr(actual, 'values') else np.array(actual)
    pred_vals = predicted.values if hasattr(predicted, 'values') else np.array(predicted)
    
    mae = mean_absolute_error(actual_vals, pred_vals)
    rmse = np.sqrt(mean_squared_error(actual_vals, pred_vals))
    mape = mean_absolute_percentage_error(actual_vals, pred_vals) * 100
    
    naive_mae = np.mean(np.abs(np.diff(actual_vals)))
    mase = mae / naive_mae if naive_mae > 0 else np.inf
    
    return {
        'Model': model_name,
        'MAE': mae,
        'RMSE': rmse,
        'MAPE': mape,
        'MASE': mase
    }

comparison_results = []
fitted_models = {}
forecasts = {}

# 1. SARIMA with auto_arima
print("\n1. Fitting SARIMA (with auto_arima)...")
if PMDARIMA_AVAILABLE:
    auto_model = auto_arima(
        train,
        start_p=0, start_q=0,
        max_p=3, max_q=3,
        d=None,
        start_P=0, start_Q=0,
        max_P=2, max_Q=2,
        D=None,
        m=seasonal_period,
        seasonal=True,
        trace=False,
        error_action='ignore',
        suppress_warnings=True,
        stepwise=True,
        random_state=42
    )
    order = auto_model.order
    seasonal_order = auto_model.seasonal_order
    print(f"   Best ARIMA order: {order}")
    print(f"   Best Seasonal order: {seasonal_order}")
else:
    order = (1, 1, 1)
    seasonal_order = (1, 1, 1, 12)
    print(f"   Using default orders: {order} x {seasonal_order}")

try:
    sarima = SARIMAX(train, order=order, seasonal_order=seasonal_order,
                      enforce_stationarity=False, enforce_invertibility=False)
    sarima_fit = sarima.fit(disp=False)
    sarima_pred = sarima_fit.get_forecast(steps=len(test))
    sarima_forecast = sarima_pred.predicted_mean
    sarima_forecast.index = test.index
    
    result = evaluate_forecast(test, sarima_forecast, 'SARIMA')
    comparison_results.append(result)
    fitted_models['SARIMA'] = sarima_fit
    forecasts['SARIMA'] = sarima_forecast
    print(f"   SARIMA RMSE: ${result['RMSE']:,.2f}")
    print(f"   SARIMA MAPE: {result['MAPE']:.2f}%")
except Exception as e:
    print(f"   SARIMA failed: {e}")

# 2. Holt-Winters
print("\n2. Fitting Holt-Winters Exponential Smoothing...")
try:
    hw = ExponentialSmoothing(
        train,
        trend='mul',  # Multiplicative trend
        seasonal='mul',  # Multiplicative seasonality
        seasonal_periods=seasonal_period,
        damped_trend=True
    )
    hw_fit = hw.fit(optimized=True)
    hw_forecast = hw_fit.forecast(len(test))
    hw_forecast.index = test.index
    
    result = evaluate_forecast(test, hw_forecast, 'Holt-Winters')
    comparison_results.append(result)
    fitted_models['Holt-Winters'] = hw_fit
    forecasts['Holt-Winters'] = hw_forecast
    print(f"   Holt-Winters RMSE: ${result['RMSE']:,.2f}")
    print(f"   Holt-Winters MAPE: {result['MAPE']:.2f}%")
except Exception as e:
    print(f"   Holt-Winters failed: {e}")

# 3. Prophet
if PROPHET_AVAILABLE:
    print("\n3. Fitting Prophet...")
    try:
        prophet_train = train.reset_index()
        prophet_train.columns = ['ds', 'y']
        
        prophet = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=False,
            daily_seasonality=False,
            seasonality_mode='multiplicative',
            interval_width=confidence_level,
            changepoint_prior_scale=0.05
        )
        prophet.fit(prophet_train)
        
        future = prophet.make_future_dataframe(periods=len(test), freq='MS')
        prophet_pred = prophet.predict(future)
        prophet_forecast = prophet_pred.iloc[-len(test):]['yhat']
        prophet_forecast.index = test.index
        
        result = evaluate_forecast(test, prophet_forecast, 'Prophet')
        comparison_results.append(result)
        fitted_models['Prophet'] = prophet
        forecasts['Prophet'] = prophet_forecast
        print(f"   Prophet RMSE: ${result['RMSE']:,.2f}")
        print(f"   Prophet MAPE: {result['MAPE']:.2f}%")
    except Exception as e:
        print(f"   Prophet failed: {e}")
else:
    print("\n3. Prophet not available (skipped)")

# 4. Naive baseline
print("\n4. Computing Naive Seasonal Forecast (baseline)...")
# Use same month from previous year
naive_forecast = series.shift(12).loc[test.index]
result = evaluate_forecast(test, naive_forecast, 'Naive Seasonal')
comparison_results.append(result)
forecasts['Naive'] = naive_forecast
print(f"   Naive RMSE: ${result['RMSE']:,.2f}")
print(f"   Naive MAPE: {result['MAPE']:.2f}%")

# Summary
comparison_df = pd.DataFrame(comparison_results).sort_values('RMSE')

print("\n" + "=" * 70)
print("MODEL COMPARISON SUMMARY")
print("=" * 70)
print(f"\n{'Model':<20} {'RMSE':>15} {'MAPE':>10} {'MASE':>10}")
print("-" * 55)
for _, row in comparison_df.iterrows():
    print(f"{row['Model']:<20} ${row['RMSE']:>13,.2f} {row['MAPE']:>9.2f}% {row['MASE']:>9.3f}")

best_model_name = comparison_df.iloc[0]['Model']
print(f"\n✓ BEST MODEL: {best_model_name}")
print(f"  Test RMSE: ${comparison_df.iloc[0]['RMSE']:,.2f}")
print(f"  Test MAPE: {comparison_df.iloc[0]['MAPE']:.2f}%")

# --- GENERATE PRODUCTION FORECAST ---
print("\n" + "-" * 70)
print("GENERATING PRODUCTION FORECAST")
print("-" * 70)

print(f"\nRefitting {best_model_name} on full dataset...")

if best_model_name == 'SARIMA':
    full_model = SARIMAX(series, order=order, seasonal_order=seasonal_order,
                          enforce_stationarity=False, enforce_invertibility=False)
    full_fit = full_model.fit(disp=False)
    
    forecast_result = full_fit.get_forecast(steps=forecast_horizon)
    forecast_values = forecast_result.predicted_mean
    conf_int = forecast_result.conf_int(alpha=1-confidence_level)
    forecast_lower = conf_int.iloc[:, 0]
    forecast_upper = conf_int.iloc[:, 1]
    
elif best_model_name == 'Holt-Winters':
    full_model = ExponentialSmoothing(series, trend='mul', seasonal='mul',
                                       seasonal_periods=seasonal_period, damped_trend=True)
    full_fit = full_model.fit(optimized=True)
    forecast_values = full_fit.forecast(forecast_horizon)
    
    residual_std = full_fit.resid.std()
    z_score = stats.norm.ppf((1 + confidence_level) / 2)
    horizon_factor = np.sqrt(np.arange(1, forecast_horizon + 1))
    forecast_lower = forecast_values - z_score * residual_std * horizon_factor
    forecast_upper = forecast_values + z_score * residual_std * horizon_factor
    
elif best_model_name == 'Prophet':
    full_prophet = Prophet(yearly_seasonality=True, seasonality_mode='multiplicative',
                           interval_width=confidence_level)
    full_df = series.reset_index()
    full_df.columns = ['ds', 'y']
    full_prophet.fit(full_df)
    
    future = full_prophet.make_future_dataframe(periods=forecast_horizon, freq='MS')
    prediction = full_prophet.predict(future)
    
    forecast_values = prediction.iloc[-forecast_horizon:]['yhat']
    forecast_lower = prediction.iloc[-forecast_horizon:]['yhat_lower']
    forecast_upper = prediction.iloc[-forecast_horizon:]['yhat_upper']
    forecast_values.index = future.iloc[-forecast_horizon:]['ds']
    forecast_lower.index = forecast_values.index
    forecast_upper.index = forecast_values.index

# Create forecast DataFrame
forecast_df = pd.DataFrame({
    'Date': forecast_values.index,
    'Forecast': forecast_values.values,
    'Lower_95CI': forecast_lower.values,
    'Upper_95CI': forecast_upper.values
})
forecast_df['Date'] = pd.to_datetime(forecast_df['Date'])
forecast_df = forecast_df.set_index('Date')

print(f"\n=== FORECAST: {forecast_horizon} MONTHS ===")
print(f"Period: {forecast_df.index[0].strftime('%B %Y')} to {forecast_df.index[-1].strftime('%B %Y')}")
print(f"Confidence Level: {confidence_level*100:.0f}%\n")

print(f"{'Month':<15} {'Forecast':>15} {'Lower CI':>15} {'Upper CI':>15}")
print("-" * 60)
for date, row in forecast_df.iterrows():
    print(f"{date.strftime('%B %Y'):<15} ${row['Forecast']:>13,.2f} ${row['Lower_95CI']:>13,.2f} ${row['Upper_95CI']:>13,.2f}")

print(f"\n=== FORECAST SUMMARY ===")
print(f"Total forecasted sales: ${forecast_df['Forecast'].sum():,.2f}")
print(f"Average monthly sales: ${forecast_df['Forecast'].mean():,.2f}")
print(f"Peak month: {forecast_df['Forecast'].idxmax().strftime('%B %Y')} (${forecast_df['Forecast'].max():,.2f})")
print(f"Low month: {forecast_df['Forecast'].idxmin().strftime('%B %Y')} (${forecast_df['Forecast'].min():,.2f})")

# Year-over-year comparison for forecast period
print(f"\n=== YEAR-OVER-YEAR COMPARISON ===")
for date in forecast_df.index:
    prior_year_date = date - pd.DateOffset(years=1)
    if prior_year_date in series.index:
        prior_value = series.loc[prior_year_date]
        forecast_value = forecast_df.loc[date, 'Forecast']
        yoy_change = (forecast_value - prior_value) / prior_value * 100
        print(f"{date.strftime('%B %Y')}: {yoy_change:+.1f}% vs prior year")

# --- VISUALIZATIONS ---
print("\n" + "-" * 70)
print("GENERATING VISUALIZATIONS")
print("-" * 70)

fig = plt.figure(figsize=(16, 14))

# Plot 1: Full Time Series with Forecast
ax1 = fig.add_subplot(3, 2, 1)
ax1.plot(series.index, series.values/1e6, 'b-', linewidth=1.5, label='Historical Sales')
ax1.plot(forecast_df.index, forecast_df['Forecast']/1e6, 'r-', linewidth=2.5, label='Forecast')
ax1.fill_between(forecast_df.index, forecast_df['Lower_95CI']/1e6, forecast_df['Upper_95CI']/1e6,
                  color='red', alpha=0.2, label='95% CI')
ax1.axvline(x=series.index[-1], color='gray', linestyle='--', alpha=0.7)
ax1.set_xlabel('Date', fontsize=11)
ax1.set_ylabel('Sales ($ Millions)', fontsize=11)
ax1.set_title(f'Retail Sales Forecast - {best_model_name}', fontsize=13, fontweight='bold')
ax1.legend(loc='upper left')
ax1.grid(True, alpha=0.3)

# Plot 2: Zoomed View
ax2 = fig.add_subplot(3, 2, 2)
recent = series.iloc[-24:]  # Last 2 years
ax2.plot(recent.index, recent.values/1e6, 'b-o', linewidth=1.5, markersize=5, label='Historical')
ax2.plot(forecast_df.index, forecast_df['Forecast']/1e6, 'r-o', linewidth=2, markersize=7, label='Forecast')
ax2.fill_between(forecast_df.index, forecast_df['Lower_95CI']/1e6, forecast_df['Upper_95CI']/1e6,
                  color='red', alpha=0.2, label='95% CI')
ax2.axvline(x=series.index[-1], color='gray', linestyle='--', alpha=0.7)
ax2.set_xlabel('Date', fontsize=11)
ax2.set_ylabel('Sales ($ Millions)', fontsize=11)
ax2.set_title('Zoomed: Last 2 Years + Forecast', fontsize=12)
ax2.legend(loc='upper left')
ax2.grid(True, alpha=0.3)

# Plot 3: Seasonal Decomposition - Trend
ax3 = fig.add_subplot(3, 2, 3)
ax3.plot(stl_result.trend.index, stl_result.trend.values/1e6, 'b-', linewidth=2)
ax3.set_xlabel('Date', fontsize=11)
ax3.set_ylabel('Trend ($ Millions)', fontsize=11)
ax3.set_title('Extracted Trend Component', fontsize=12)
ax3.grid(True, alpha=0.3)

# Plot 4: Seasonal Pattern
ax4 = fig.add_subplot(3, 2, 4)
monthly_pattern = series.groupby(series.index.month).mean()
colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, 12))
bars = ax4.bar(range(1, 13), monthly_pattern.values/1e6, color=colors, edgecolor='k')
ax4.set_xticks(range(1, 13))
ax4.set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])
ax4.axhline(y=series.mean()/1e6, color='red', linestyle='--', linewidth=2, label='Overall Mean')
ax4.set_xlabel('Month', fontsize=11)
ax4.set_ylabel('Average Sales ($ Millions)', fontsize=11)
ax4.set_title('Seasonal Pattern (Monthly Averages)', fontsize=12)
ax4.legend()
ax4.grid(True, alpha=0.3, axis='y')

# Plot 5: Model Comparison on Test Set
ax5 = fig.add_subplot(3, 2, 5)
ax5.plot(test.index, test.values/1e6, 'k-', linewidth=2.5, label='Actual')
colors_models = {'SARIMA': 'blue', 'Holt-Winters': 'green', 'Prophet': 'orange', 'Naive': 'gray'}
for name, fc in forecasts.items():
    if name != 'Naive':
        ax5.plot(test.index, fc.values/1e6, linestyle='--', linewidth=1.5, 
                color=colors_models.get(name, 'purple'), label=name)
ax5.set_xlabel('Date', fontsize=11)
ax5.set_ylabel('Sales ($ Millions)', fontsize=11)
ax5.set_title('Model Comparison: Test Set Performance', fontsize=12)
ax5.legend(loc='upper left')
ax5.grid(True, alpha=0.3)

# Plot 6: Error Metrics Comparison
ax6 = fig.add_subplot(3, 2, 6)
metrics = ['RMSE', 'MAPE']
x = np.arange(len(comparison_df))
width = 0.35

# Normalize RMSE for comparison (divide by 100000)
rmse_normalized = comparison_df['RMSE'] / 100000

ax6_twin = ax6.twinx()
bars1 = ax6.bar(x - width/2, rmse_normalized, width, label='RMSE (÷100K)', color='steelblue')
bars2 = ax6_twin.bar(x + width/2, comparison_df['MAPE'], width, label='MAPE (%)', color='coral')

ax6.set_xticks(x)
ax6.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')
ax6.set_ylabel('RMSE (÷$100K)', fontsize=11, color='steelblue')
ax6_twin.set_ylabel('MAPE (%)', fontsize=11, color='coral')
ax6.set_title('Model Comparison: Error Metrics', fontsize=12)

# Add legend
lines1, labels1 = ax6.get_legend_handles_labels()
lines2, labels2 = ax6_twin.get_legend_handles_labels()
ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper right')

plt.tight_layout()
plt.savefig('sales_forecast_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'sales_forecast_analysis.png'")

# --- FORECAST FUNCTION ---
def generate_sales_forecast(historical_data, periods=12, model='sarima'):
    """
    Generate sales forecast for new data.
    
    Parameters:
    -----------
    historical_data : pd.Series
        Monthly sales data with datetime index
    periods : int
        Number of months to forecast
    model : str
        'sarima', 'holtwinters', or 'prophet'
    
    Returns:
    --------
    dict with forecast, lower_ci, upper_ci, and model_info
    """
    if not isinstance(historical_data.index, pd.DatetimeIndex):
        historical_data.index = pd.to_datetime(historical_data.index)
    
    if historical_data.index.freq is None:
        historical_data = historical_data.asfreq('MS')
    
    if model.lower() == 'sarima':
        fit_model = SARIMAX(historical_data, order=order, seasonal_order=seasonal_order,
                            enforce_stationarity=False, enforce_invertibility=False)
        fit = fit_model.fit(disp=False)
        result = fit.get_forecast(steps=periods)
        fc = result.predicted_mean
        ci = result.conf_int(alpha=0.05)
        lower, upper = ci.iloc[:, 0], ci.iloc[:, 1]
        
    elif model.lower() in ['holtwinters', 'hw']:
        fit_model = ExponentialSmoothing(historical_data, trend='mul', seasonal='mul',
                                         seasonal_periods=12, damped_trend=True)
        fit = fit_model.fit(optimized=True)
        fc = fit.forecast(periods)
        std = fit.resid.std()
        z = 1.96
        factor = np.sqrt(np.arange(1, periods + 1))
        lower = fc - z * std * factor
        upper = fc + z * std * factor
        
    elif model.lower() == 'prophet' and PROPHET_AVAILABLE:
        p = Prophet(yearly_seasonality=True, seasonality_mode='multiplicative', interval_width=0.95)
        df = historical_data.reset_index()
        df.columns = ['ds', 'y']
        p.fit(df)
        future = p.make_future_dataframe(periods=periods, freq='MS')
        pred = p.predict(future)
        fc = pred.iloc[-periods:]['yhat']
        lower = pred.iloc[-periods:]['yhat_lower']
        upper = pred.iloc[-periods:]['yhat_upper']
        fc.index = future.iloc[-periods:]['ds']
        lower.index = fc.index
        upper.index = fc.index
    else:
        raise ValueError(f"Unknown model: {model}")
    
    return {
        'forecast': fc,
        'lower_ci': lower,
        'upper_ci': upper,
        'model_info': {'model': model, 'periods': periods}
    }

# --- SAVE ARTIFACTS ---
print("\n" + "-" * 70)
print("SAVING MODEL ARTIFACTS")
print("-" * 70)

import joblib
import json

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save best model
if best_model_name == 'SARIMA':
    model_filename = f'sales_forecast_sarima_{timestamp}.pkl'
    joblib.dump(full_fit, model_filename)
elif best_model_name == 'Holt-Winters':
    model_filename = f'sales_forecast_hw_{timestamp}.pkl'
    joblib.dump(full_fit, model_filename)
elif best_model_name == 'Prophet':
    model_filename = f'sales_forecast_prophet_{timestamp}.pkl'
    joblib.dump(full_prophet, model_filename)

print(f"Model saved: {model_filename}")

# Save forecast
forecast_filename = f'sales_forecast_results_{timestamp}.csv'
forecast_df.to_csv(forecast_filename)
print(f"Forecast saved: {forecast_filename}")

# Save configuration
config = {
    'best_model': best_model_name,
    'sarima_order': list(order) if 'order' in dir() else None,
    'sarima_seasonal_order': list(seasonal_order) if 'seasonal_order' in dir() else None,
    'seasonal_period': seasonal_period,
    'frequency': 'MS',
    'confidence_level': confidence_level,
    'forecast_horizon': forecast_horizon,
    'test_rmse': float(comparison_df.iloc[0]['RMSE']),
    'test_mape': float(comparison_df.iloc[0]['MAPE']),
    'timestamp': timestamp
}

config_filename = f'sales_forecast_config_{timestamp}.json'
with open(config_filename, 'w') as f:
    json.dump(config, f, indent=2)
print(f"Config saved: {config_filename}")

# Save historical data
data_filename = f'sales_historical_{timestamp}.csv'
series.to_csv(data_filename)
print(f"Historical data saved: {data_filename}")

print("\n" + "=" * 70)
print("FORECASTING COMPLETE")
print("=" * 70)
print(f"\nBest Model: {best_model_name}")
print(f"Forecast Period: {forecast_df.index[0].strftime('%B %Y')} - {forecast_df.index[-1].strftime('%B %Y')}")
print(f"Total Forecasted Sales: ${forecast_df['Forecast'].sum():,.2f}")

print("\nFiles generated:")
print(f"  - {model_filename}")
print(f"  - {forecast_filename}")
print(f"  - {config_filename}")
print(f"  - {data_filename}")
print(f"  - sales_forecast_analysis.png")

"""
To load and use this model later:

import joblib
import json
import pandas as pd

# Load model and config
model = joblib.load('sales_forecast_sarima_XXXXXXXX_XXXXXX.pkl')
with open('sales_forecast_config_XXXXXXXX_XXXXXX.json', 'r') as f:
    config = json.load(f)

# For SARIMA: extend with new data and forecast
# new_data = pd.read_csv('new_sales.csv', index_col=0, parse_dates=True)
# extended = model.extend(new_data)
# new_forecast = extended.get_forecast(steps=12)

# Or use the generate_sales_forecast function directly
"""
```

---

## IMPLEMENTATION NOTES FOR REPLIT AGENT

1. **Frequency Detection**: Auto-detect data frequency from the datetime index or user description. Map common terms like "monthly", "daily", "weekly" to pandas frequency codes.

2. **Seasonal Period**: Set based on frequency (12 for monthly, 7 for daily, 52 for weekly, 4 for quarterly). Allow user override.

3. **Decomposition Type**: Auto-detect by checking correlation between rolling mean and rolling standard deviation. If positive correlation > 0.5, use multiplicative; otherwise use additive.

4. **Auto ARIMA**: When pmdarima is available and user wants automatic order selection, use `auto_arima()`. Otherwise fall back to ACF/PACF analysis for manual order suggestions.

5. **Prophet Configuration**: Map data frequency to Prophet's seasonality settings. For monthly data: yearly_seasonality=True, weekly=False, daily=False.

6. **Confidence Intervals**: SARIMA provides native CI. For exponential smoothing, simulate CI using residual standard deviation with expanding uncertainty over horizon.

7. **Model Comparison**: Always include a naive baseline (seasonal naive or last value) for comparison context.

8. **Validation**: Before returning code:
   - Ensure datetime handling is correct
   - Verify all model parameters are valid
   - Check that forecast horizon doesn't exceed reasonable limits (typically < 50% of historical data)

9. **File Delivery**: Provide as downloadable `.py` file. Name format: `forecast_[MODEL_TYPE]_[TIMESTAMP].py`

---

**END OF STATISTICAL FORECASTING FUNCTION SPECIFICATION**