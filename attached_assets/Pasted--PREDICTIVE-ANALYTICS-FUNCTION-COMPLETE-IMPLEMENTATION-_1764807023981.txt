# PREDICTIVE ANALYTICS FUNCTION - COMPLETE IMPLEMENTATION INSTRUCTIONS

## FUNCTION PURPOSE

The Predictive Analytics function takes a natural language description of a business prediction problem and generates a complete, end-to-end, production-ready analytics pipeline. Unlike the individual ML Models function which focuses on specific algorithms, this function delivers a comprehensive solution including automated EDA, feature engineering, model comparison, ensemble methods, model interpretation, and deployment-ready code with monitoring recommendations.

This is the most comprehensive function in the Data Science panel—designed for business analysts, data scientists, and ML engineers who need a complete predictive solution from problem definition to production deployment.

---

## KEY DIFFERENTIATORS FROM ML MODELS FUNCTION

| Aspect | ML Models Function | Predictive Analytics Function |
|--------|-------------------|------------------------------|
| Scope | Single model or comparison | Full end-to-end pipeline |
| Feature Engineering | Basic preprocessing | Automated feature creation, selection, transformation |
| Model Selection | User-specified or basic comparison | Exhaustive comparison with ensemble stacking |
| Interpretability | Feature importance only | SHAP values, partial dependence, model explanations |
| Output | Model + predictions | Complete pipeline class + deployment artifacts |
| Production Focus | Basic save/load | Monitoring, drift detection, retraining triggers |
| Business Context | Technical metrics | Business KPIs, ROI estimation, decision thresholds |

---

## SUPPORTED PROBLEM TYPES

### 1. Binary Classification
- Customer churn prediction
- Fraud detection
- Conversion prediction
- Default/credit risk
- Medical diagnosis (positive/negative)

### 2. Multi-class Classification
- Customer segmentation
- Product categorization
- Sentiment classification (positive/neutral/negative)
- Disease classification

### 3. Regression
- Revenue/sales forecasting
- Price prediction
- Demand forecasting
- Customer lifetime value
- Risk scoring (continuous)

### 4. Ranking/Scoring
- Lead scoring
- Recommendation ranking
- Risk prioritization

---

## INPUT PROCESSING REQUIREMENTS

The function receives natural language input describing a business problem. Extract the following:

**Required Variables:**
- `problem_type`: "binary_classification", "multiclass_classification", "regression", or "auto"
- `target_variable`: The outcome to predict
- `data_source`: Inline data, file path, or synthetic data generation instructions
- `business_context`: Description of the business problem and goals

**Optional Variables (use intelligent defaults):**
- `feature_columns`: List of features (default: all non-target columns)
- `id_column`: Unique identifier column to exclude from modeling (default: auto-detect)
- `date_column`: Date column for time-based splits (default: auto-detect)
- `test_size`: Holdout test proportion (default: 0.2)
- `validation_strategy`: "holdout", "kfold", "stratified_kfold", "time_series" (default: auto based on data)
- `optimization_metric`: Metric to optimize (default: based on problem type)
- `class_weight`: Handling for imbalanced data (default: auto-detect and handle)
- `feature_engineering_level`: "minimal", "standard", "aggressive" (default: "standard")
- `interpretability_level`: "basic", "full" (default: "full")
- `ensemble_methods`: Whether to include stacking/blending (default: True)
- `business_threshold`: Custom decision threshold for classification (default: 0.5)
- `output_format`: "py" or "ipynb" (default: "py")

**Parsing Rules:**
- "predict whether", "will they", "yes or no" → binary_classification
- "which category", "classify into", "segment into groups" → multiclass_classification
- "predict how much", "forecast value", "estimate amount" → regression
- "score", "rank", "prioritize" → regression or ranking
- "imbalanced", "rare event", "few positives" → Enable class balancing
- "explainable", "interpretable", "understand why" → Full interpretability
- "production", "deploy", "real-time" → Include deployment artifacts
- "monitor", "drift", "retrain" → Include monitoring code

**Auto-Detection Logic:**
- Target with 2 unique values → binary_classification
- Target with 3-20 unique values (categorical) → multiclass_classification
- Target with >20 unique values or continuous → regression
- Date column present + "time" in description → time_series validation
- Class ratio < 0.1 → Enable SMOTE/class weights

---

## OUTPUT CODE STRUCTURE

The generated Python file contains a complete, modular pipeline organized as follows:

### Section 1: Header and Imports

```python
"""
Predictive Analytics Pipeline: [BUSINESS_PROBLEM]
Problem Type: [PROBLEM_TYPE]
Generated by ModelWiz.xyz

Target Variable: [TARGET]
Business Objective: [BUSINESS_CONTEXT]
Generated on: [TIMESTAMP]

This pipeline includes:
- Automated Exploratory Data Analysis
- Feature Engineering & Selection
- Multiple Model Training & Comparison
- Ensemble Methods (Stacking)
- Model Interpretation (SHAP)
- Production Deployment Artifacts
- Monitoring & Drift Detection

Required packages:
pip install numpy pandas matplotlib seaborn scikit-learn xgboost lightgbm catboost shap optuna imbalanced-learn

Optional packages:
pip install plotly kaleido feature-engine category_encoders
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import pickle
import json
import os
warnings.filterwarnings('ignore')

# Core ML
from sklearn.model_selection import (train_test_split, cross_val_score, StratifiedKFold, 
                                      KFold, GridSearchCV, RandomizedSearchCV)
from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler,
                                    LabelEncoder, OneHotEncoder, OrdinalEncoder)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import (SelectKBest, f_classif, f_regression, 
                                        mutual_info_classif, mutual_info_regression,
                                        RFE, RFECV, SelectFromModel)

# Models
from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,
                               GradientBoostingClassifier, GradientBoostingRegressor,
                               AdaBoostClassifier, AdaBoostRegressor,
                               ExtraTreesClassifier, ExtraTreesRegressor,
                               StackingClassifier, StackingRegressor,
                               VotingClassifier, VotingRegressor)
from sklearn.svm import SVC, SVR
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.naive_bayes import GaussianNB

# Metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                              roc_auc_score, average_precision_score, log_loss,
                              confusion_matrix, classification_report, roc_curve,
                              precision_recall_curve, mean_squared_error, 
                              mean_absolute_error, r2_score, mean_absolute_percentage_error)

# XGBoost
try:
    from xgboost import XGBClassifier, XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("Note: XGBoost not installed. pip install xgboost")

# LightGBM
try:
    from lightgbm import LGBMClassifier, LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("Note: LightGBM not installed. pip install lightgbm")

# CatBoost
try:
    from catboost import CatBoostClassifier, CatBoostRegressor
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("Note: CatBoost not installed. pip install catboost")

# SHAP for interpretability
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Note: SHAP not installed. pip install shap")

# Imbalanced-learn
try:
    from imblearn.over_sampling import SMOTE, ADASYN
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.combine import SMOTETomek
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("Note: imbalanced-learn not installed. pip install imbalanced-learn")

# Optuna for hyperparameter optimization
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
    optuna.logging.set_verbosity(optuna.logging.WARNING)
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Note: Optuna not installed. pip install optuna")

print("=" * 80)
print("PREDICTIVE ANALYTICS PIPELINE")
print("[BUSINESS_PROBLEM]")
print("=" * 80)
```

### Section 2: Configuration Class

```python
# --- CONFIGURATION ---
class PipelineConfig:
    """Central configuration for the predictive analytics pipeline"""
    
    # Problem definition
    PROBLEM_TYPE = '[PROBLEM_TYPE]'  # 'binary_classification', 'multiclass_classification', 'regression'
    TARGET_COLUMN = '[TARGET_COLUMN]'
    ID_COLUMN = [ID_COLUMN]  # None or column name
    DATE_COLUMN = [DATE_COLUMN]  # None or column name
    
    # Data splitting
    TEST_SIZE = [TEST_SIZE]
    VALIDATION_FOLDS = 5
    RANDOM_STATE = 42
    
    # Feature engineering
    FEATURE_ENGINEERING_LEVEL = '[FEATURE_ENGINEERING_LEVEL]'  # 'minimal', 'standard', 'aggressive'
    
    # Model training
    OPTIMIZATION_METRIC = '[OPTIMIZATION_METRIC]'  # 'roc_auc', 'f1', 'accuracy', 'rmse', 'mae', etc.
    USE_ENSEMBLE = True
    HYPERPARAMETER_TUNING = 'optuna'  # 'none', 'random', 'grid', 'optuna'
    TUNING_TRIALS = 50
    
    # Class imbalance
    HANDLE_IMBALANCE = [HANDLE_IMBALANCE]  # True/False
    IMBALANCE_STRATEGY = 'smote'  # 'smote', 'class_weight', 'undersample'
    
    # Interpretability
    INTERPRETABILITY_LEVEL = '[INTERPRETABILITY_LEVEL]'  # 'basic', 'full'
    
    # Business settings
    BUSINESS_THRESHOLD = [BUSINESS_THRESHOLD]  # Decision threshold for classification
    POSITIVE_CLASS_LABEL = '[POSITIVE_CLASS_LABEL]'
    
    # Output paths
    OUTPUT_DIR = './model_artifacts'
    
    @classmethod
    def print_config(cls):
        print("\n=== PIPELINE CONFIGURATION ===")
        for attr in dir(cls):
            if not attr.startswith('_') and not callable(getattr(cls, attr)):
                print(f"  {attr}: {getattr(cls, attr)}")

PipelineConfig.print_config()
```

### Section 3: Data Loading and Initial Assessment

```python
# --- DATA LOADING ---
print("\n" + "-" * 80)
print("PHASE 1: DATA LOADING & INITIAL ASSESSMENT")
print("-" * 80)

# [GENERATE APPROPRIATE DATA LOADING CODE]
# Options: pd.read_csv(), synthetic generation, inline data

print(f"\n✓ Data loaded: {df.shape[0]:,} rows × {df.shape[1]} columns")

# Initial data assessment
class DataAssessment:
    """Automated data quality assessment"""
    
    def __init__(self, df, target_col, id_col=None, date_col=None):
        self.df = df.copy()
        self.target_col = target_col
        self.id_col = id_col
        self.date_col = date_col
        self.assessment = {}
        
    def run_assessment(self):
        print("\n=== DATA QUALITY ASSESSMENT ===\n")
        
        # Basic info
        self.assessment['n_rows'] = len(self.df)
        self.assessment['n_cols'] = len(self.df.columns)
        print(f"Dataset dimensions: {self.assessment['n_rows']:,} rows × {self.assessment['n_cols']} columns")
        
        # Target variable analysis
        self._assess_target()
        
        # Feature types
        self._assess_feature_types()
        
        # Missing values
        self._assess_missing()
        
        # Duplicates
        self._assess_duplicates()
        
        # Cardinality
        self._assess_cardinality()
        
        # Outliers
        self._assess_outliers()
        
        # Class imbalance (for classification)
        if PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
            self._assess_class_balance()
        
        return self.assessment
    
    def _assess_target(self):
        print(f"\n--- Target Variable: {self.target_col} ---")
        target = self.df[self.target_col]
        
        self.assessment['target_dtype'] = str(target.dtype)
        self.assessment['target_missing'] = target.isnull().sum()
        self.assessment['target_unique'] = target.nunique()
        
        print(f"  Data type: {self.assessment['target_dtype']}")
        print(f"  Unique values: {self.assessment['target_unique']}")
        print(f"  Missing: {self.assessment['target_missing']}")
        
        if PipelineConfig.PROBLEM_TYPE == 'regression':
            print(f"  Mean: {target.mean():.4f}")
            print(f"  Std: {target.std():.4f}")
            print(f"  Range: [{target.min():.4f}, {target.max():.4f}]")
        else:
            print(f"  Distribution:")
            for val, count in target.value_counts().items():
                pct = count / len(target) * 100
                print(f"    {val}: {count:,} ({pct:.1f}%)")
    
    def _assess_feature_types(self):
        print(f"\n--- Feature Types ---")
        
        feature_cols = [c for c in self.df.columns 
                       if c not in [self.target_col, self.id_col, self.date_col]]
        
        numeric_cols = self.df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = self.df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()
        boolean_cols = self.df[feature_cols].select_dtypes(include=['bool']).columns.tolist()
        datetime_cols = self.df[feature_cols].select_dtypes(include=['datetime64']).columns.tolist()
        
        self.assessment['numeric_features'] = numeric_cols
        self.assessment['categorical_features'] = categorical_cols
        self.assessment['boolean_features'] = boolean_cols
        self.assessment['datetime_features'] = datetime_cols
        
        print(f"  Numeric: {len(numeric_cols)} features")
        print(f"  Categorical: {len(categorical_cols)} features")
        print(f"  Boolean: {len(boolean_cols)} features")
        print(f"  Datetime: {len(datetime_cols)} features")
        
    def _assess_missing(self):
        print(f"\n--- Missing Values ---")
        
        missing = self.df.isnull().sum()
        missing_pct = (missing / len(self.df) * 100).round(2)
        
        missing_df = pd.DataFrame({
            'Missing Count': missing,
            'Missing %': missing_pct
        }).sort_values('Missing %', ascending=False)
        
        cols_with_missing = missing_df[missing_df['Missing Count'] > 0]
        
        self.assessment['cols_with_missing'] = len(cols_with_missing)
        self.assessment['total_missing_cells'] = missing.sum()
        self.assessment['missing_details'] = cols_with_missing.to_dict()
        
        if len(cols_with_missing) > 0:
            print(f"  Columns with missing values: {len(cols_with_missing)}")
            print(f"  Total missing cells: {missing.sum():,} ({missing.sum()/(len(self.df)*len(self.df.columns))*100:.2f}%)")
            print("\n  Top columns with missing data:")
            for col, row in cols_with_missing.head(10).iterrows():
                print(f"    {col}: {row['Missing Count']:,} ({row['Missing %']:.1f}%)")
        else:
            print("  ✓ No missing values detected")
    
    def _assess_duplicates(self):
        print(f"\n--- Duplicate Rows ---")
        
        n_duplicates = self.df.duplicated().sum()
        self.assessment['duplicate_rows'] = n_duplicates
        
        if n_duplicates > 0:
            print(f"  ⚠️  Duplicate rows: {n_duplicates:,} ({n_duplicates/len(self.df)*100:.2f}%)")
        else:
            print("  ✓ No duplicate rows detected")
    
    def _assess_cardinality(self):
        print(f"\n--- Cardinality Analysis ---")
        
        high_cardinality = []
        low_cardinality_numeric = []
        
        for col in self.assessment.get('categorical_features', []):
            n_unique = self.df[col].nunique()
            if n_unique > 50:
                high_cardinality.append((col, n_unique))
        
        for col in self.assessment.get('numeric_features', []):
            n_unique = self.df[col].nunique()
            if n_unique < 10:
                low_cardinality_numeric.append((col, n_unique))
        
        self.assessment['high_cardinality_features'] = high_cardinality
        self.assessment['low_cardinality_numeric'] = low_cardinality_numeric
        
        if high_cardinality:
            print(f"  High cardinality categorical (>50 unique):")
            for col, n in high_cardinality[:5]:
                print(f"    {col}: {n} unique values")
        
        if low_cardinality_numeric:
            print(f"  Low cardinality numeric (<10 unique, consider as categorical):")
            for col, n in low_cardinality_numeric[:5]:
                print(f"    {col}: {n} unique values")
    
    def _assess_outliers(self):
        print(f"\n--- Outlier Detection (IQR Method) ---")
        
        outlier_cols = []
        for col in self.assessment.get('numeric_features', []):
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            n_outliers = ((self.df[col] < lower) | (self.df[col] > upper)).sum()
            if n_outliers > 0:
                outlier_cols.append((col, n_outliers, n_outliers/len(self.df)*100))
        
        self.assessment['outlier_features'] = outlier_cols
        
        if outlier_cols:
            print(f"  Features with outliers:")
            for col, n, pct in sorted(outlier_cols, key=lambda x: x[1], reverse=True)[:10]:
                print(f"    {col}: {n:,} outliers ({pct:.1f}%)")
        else:
            print("  ✓ No significant outliers detected")
    
    def _assess_class_balance(self):
        print(f"\n--- Class Balance ---")
        
        target = self.df[self.target_col]
        value_counts = target.value_counts()
        
        majority_class = value_counts.index[0]
        minority_class = value_counts.index[-1]
        imbalance_ratio = value_counts.iloc[-1] / value_counts.iloc[0]
        
        self.assessment['imbalance_ratio'] = imbalance_ratio
        self.assessment['majority_class'] = majority_class
        self.assessment['minority_class'] = minority_class
        
        if imbalance_ratio < 0.2:
            print(f"  ⚠️  SEVERE class imbalance detected!")
            print(f"  Imbalance ratio: {imbalance_ratio:.3f}")
            print(f"  Majority class ({majority_class}): {value_counts.iloc[0]:,}")
            print(f"  Minority class ({minority_class}): {value_counts.iloc[-1]:,}")
            print(f"  → Recommend: SMOTE, class weights, or undersampling")
        elif imbalance_ratio < 0.5:
            print(f"  ⚠️  Moderate class imbalance detected")
            print(f"  Imbalance ratio: {imbalance_ratio:.3f}")
        else:
            print(f"  ✓ Classes are relatively balanced (ratio: {imbalance_ratio:.3f})")

# Run assessment
assessor = DataAssessment(
    df, 
    target_col=PipelineConfig.TARGET_COLUMN,
    id_col=PipelineConfig.ID_COLUMN,
    date_col=PipelineConfig.DATE_COLUMN
)
data_assessment = assessor.run_assessment()
```

### Section 4: Automated Exploratory Data Analysis

```python
# --- AUTOMATED EDA ---
print("\n" + "-" * 80)
print("PHASE 2: AUTOMATED EXPLORATORY DATA ANALYSIS")
print("-" * 80)

class AutomatedEDA:
    """Automated exploratory data analysis with visualizations"""
    
    def __init__(self, df, target_col, numeric_features, categorical_features, problem_type):
        self.df = df.copy()
        self.target_col = target_col
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        self.problem_type = problem_type
        self.insights = []
        
    def run_eda(self, save_plots=True):
        print("\n=== AUTOMATED EDA ===\n")
        
        # Numeric feature analysis
        self._analyze_numeric_features()
        
        # Categorical feature analysis
        self._analyze_categorical_features()
        
        # Correlation analysis
        self._analyze_correlations()
        
        # Target relationship analysis
        self._analyze_target_relationships()
        
        # Generate visualizations
        if save_plots:
            self._generate_eda_visualizations()
        
        # Print insights
        self._print_insights()
        
        return self.insights
    
    def _analyze_numeric_features(self):
        print("--- Numeric Feature Analysis ---")
        
        if not self.numeric_features:
            print("  No numeric features to analyze")
            return
        
        stats = self.df[self.numeric_features].describe().T
        stats['skewness'] = self.df[self.numeric_features].skew()
        stats['kurtosis'] = self.df[self.numeric_features].kurtosis()
        
        # Identify highly skewed features
        skewed = stats[abs(stats['skewness']) > 1].index.tolist()
        if skewed:
            self.insights.append(f"Highly skewed features (consider log transform): {skewed[:5]}")
            print(f"  Highly skewed features: {len(skewed)}")
        
        # Identify features with high kurtosis
        high_kurtosis = stats[stats['kurtosis'] > 3].index.tolist()
        if high_kurtosis:
            self.insights.append(f"Features with heavy tails: {high_kurtosis[:5]}")
        
        # Features with zero variance
        zero_var = stats[stats['std'] == 0].index.tolist()
        if zero_var:
            self.insights.append(f"⚠️ Zero variance features (remove): {zero_var}")
            print(f"  ⚠️ Zero variance features: {zero_var}")
        
        print(f"  Analyzed {len(self.numeric_features)} numeric features")
        
    def _analyze_categorical_features(self):
        print("\n--- Categorical Feature Analysis ---")
        
        if not self.categorical_features:
            print("  No categorical features to analyze")
            return
        
        for col in self.categorical_features[:10]:  # Limit to first 10
            n_unique = self.df[col].nunique()
            mode = self.df[col].mode().iloc[0] if len(self.df[col].mode()) > 0 else None
            mode_pct = (self.df[col] == mode).mean() * 100 if mode else 0
            
            if mode_pct > 90:
                self.insights.append(f"⚠️ {col}: Single value dominates ({mode_pct:.1f}%), consider removing")
            
            if n_unique > 100:
                self.insights.append(f"⚠️ {col}: High cardinality ({n_unique}), consider encoding strategy")
        
        print(f"  Analyzed {len(self.categorical_features)} categorical features")
    
    def _analyze_correlations(self):
        print("\n--- Correlation Analysis ---")
        
        if len(self.numeric_features) < 2:
            print("  Not enough numeric features for correlation analysis")
            return
        
        corr_matrix = self.df[self.numeric_features].corr()
        
        # Find highly correlated pairs
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.8:
                    high_corr_pairs.append((
                        corr_matrix.columns[i],
                        corr_matrix.columns[j],
                        corr_val
                    ))
        
        if high_corr_pairs:
            print(f"  Highly correlated feature pairs (|r| > 0.8): {len(high_corr_pairs)}")
            self.insights.append(f"Highly correlated pairs found: {len(high_corr_pairs)} (consider removing redundant features)")
            for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]:
                print(f"    {f1} <-> {f2}: {corr:.3f}")
        else:
            print("  No highly correlated feature pairs detected")
        
        # Correlation with target
        if self.problem_type == 'regression':
            target_corr = self.df[self.numeric_features + [self.target_col]].corr()[self.target_col].drop(self.target_col)
            top_corr = target_corr.abs().sort_values(ascending=False).head(10)
            print(f"\n  Top features correlated with target:")
            for feat, corr in top_corr.items():
                actual_corr = target_corr[feat]
                print(f"    {feat}: {actual_corr:.3f}")
    
    def _analyze_target_relationships(self):
        print("\n--- Target Relationship Analysis ---")
        
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            # Analyze feature distributions by class
            significant_features = []
            
            for col in self.numeric_features[:20]:
                groups = [group[col].dropna().values for name, group in self.df.groupby(self.target_col)]
                if len(groups) >= 2 and all(len(g) > 1 for g in groups):
                    from scipy import stats as scipy_stats
                    if len(groups) == 2:
                        stat, pval = scipy_stats.ttest_ind(groups[0], groups[1])
                    else:
                        stat, pval = scipy_stats.f_oneway(*groups)
                    
                    if pval < 0.05:
                        significant_features.append((col, pval))
            
            if significant_features:
                print(f"  Features with significant target relationship (p < 0.05): {len(significant_features)}")
                self.insights.append(f"Top predictive features identified: {[f[0] for f in significant_features[:5]]}")
    
    def _generate_eda_visualizations(self):
        print("\n--- Generating EDA Visualizations ---")
        
        fig = plt.figure(figsize=(20, 16))
        
        # 1. Target distribution
        ax1 = fig.add_subplot(3, 3, 1)
        if self.problem_type == 'regression':
            self.df[self.target_col].hist(bins=50, ax=ax1, color='steelblue', edgecolor='k')
            ax1.set_xlabel(self.target_col)
            ax1.set_ylabel('Frequency')
            ax1.set_title('Target Distribution')
        else:
            self.df[self.target_col].value_counts().plot(kind='bar', ax=ax1, color='steelblue', edgecolor='k')
            ax1.set_xlabel('Class')
            ax1.set_ylabel('Count')
            ax1.set_title('Target Class Distribution')
            ax1.tick_params(axis='x', rotation=45)
        
        # 2. Missing values heatmap
        ax2 = fig.add_subplot(3, 3, 2)
        missing = self.df.isnull().sum()
        missing = missing[missing > 0].sort_values(ascending=False).head(15)
        if len(missing) > 0:
            missing.plot(kind='barh', ax=ax2, color='coral')
            ax2.set_xlabel('Missing Count')
            ax2.set_title('Top 15 Features with Missing Values')
        else:
            ax2.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)
            ax2.set_title('Missing Values')
        
        # 3. Correlation heatmap (top features)
        ax3 = fig.add_subplot(3, 3, 3)
        if len(self.numeric_features) > 1:
            # Select top features by variance
            top_numeric = self.df[self.numeric_features].var().sort_values(ascending=False).head(10).index.tolist()
            corr_subset = self.df[top_numeric].corr()
            sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='RdBu_r', center=0, 
                       ax=ax3, annot_kws={'size': 8}, square=True)
            ax3.set_title('Feature Correlation Heatmap (Top 10)')
        else:
            ax3.text(0.5, 0.5, 'Not enough features', ha='center', va='center')
        
        # 4-6. Top numeric feature distributions
        top_numeric = self.numeric_features[:3] if self.numeric_features else []
        for idx, col in enumerate(top_numeric):
            ax = fig.add_subplot(3, 3, 4 + idx)
            if self.problem_type in ['binary_classification', 'multiclass_classification']:
                for label in self.df[self.target_col].unique():
                    subset = self.df[self.df[self.target_col] == label][col]
                    subset.hist(bins=30, alpha=0.5, label=str(label), ax=ax)
                ax.legend()
            else:
                self.df[col].hist(bins=50, ax=ax, color='steelblue', edgecolor='k', alpha=0.7)
            ax.set_xlabel(col)
            ax.set_title(f'Distribution: {col}')
        
        # 7-9. Top categorical feature distributions
        top_categorical = self.categorical_features[:3] if self.categorical_features else []
        for idx, col in enumerate(top_categorical):
            ax = fig.add_subplot(3, 3, 7 + idx)
            value_counts = self.df[col].value_counts().head(10)
            value_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='k')
            ax.set_xlabel(col)
            ax.set_title(f'Distribution: {col}')
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('eda_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()
        print("  ✓ EDA visualizations saved as 'eda_analysis.png'")
    
    def _print_insights(self):
        print("\n=== KEY EDA INSIGHTS ===")
        for i, insight in enumerate(self.insights, 1):
            print(f"  {i}. {insight}")

# Run automated EDA
eda = AutomatedEDA(
    df=df,
    target_col=PipelineConfig.TARGET_COLUMN,
    numeric_features=data_assessment['numeric_features'],
    categorical_features=data_assessment['categorical_features'],
    problem_type=PipelineConfig.PROBLEM_TYPE
)
eda_insights = eda.run_eda()
```

### Section 5: Feature Engineering

```python
# --- FEATURE ENGINEERING ---
print("\n" + "-" * 80)
print("PHASE 3: FEATURE ENGINEERING")
print("-" * 80)

class FeatureEngineer:
    """Automated feature engineering pipeline"""
    
    def __init__(self, df, target_col, numeric_features, categorical_features, 
                 id_col=None, date_col=None, level='standard'):
        self.df = df.copy()
        self.target_col = target_col
        self.numeric_features = numeric_features
        self.categorical_features = categorical_features
        self.id_col = id_col
        self.date_col = date_col
        self.level = level
        self.feature_log = []
        self.new_features = []
        
    def engineer_features(self):
        print(f"\n=== FEATURE ENGINEERING (Level: {self.level}) ===\n")
        
        # Remove ID column from features
        if self.id_col and self.id_col in self.df.columns:
            self.df = self.df.drop(columns=[self.id_col])
            print(f"  Removed ID column: {self.id_col}")
        
        # Handle missing values
        self._handle_missing()
        
        # Encode categorical features
        self._encode_categoricals()
        
        # Scale numeric features (for certain models)
        # Note: We'll do this in the pipeline to avoid data leakage
        
        if self.level in ['standard', 'aggressive']:
            # Create interaction features
            self._create_interactions()
            
            # Polynomial features for top numeric
            self._create_polynomial_features()
            
            # Binning for numeric features
            self._create_binned_features()
        
        if self.level == 'aggressive':
            # Statistical aggregations
            self._create_statistical_features()
            
            # Ratio features
            self._create_ratio_features()
        
        # Date features
        if self.date_col:
            self._create_date_features()
        
        # Print summary
        self._print_summary()
        
        return self.df, self.feature_log
    
    def _handle_missing(self):
        print("--- Handling Missing Values ---")
        
        # Numeric: median imputation
        for col in self.numeric_features:
            if col in self.df.columns and self.df[col].isnull().sum() > 0:
                median_val = self.df[col].median()
                self.df[col] = self.df[col].fillna(median_val)
                self.feature_log.append(f"Imputed {col} with median: {median_val:.4f}")
        
        # Categorical: mode imputation
        for col in self.categorical_features:
            if col in self.df.columns and self.df[col].isnull().sum() > 0:
                mode_val = self.df[col].mode().iloc[0] if len(self.df[col].mode()) > 0 else 'UNKNOWN'
                self.df[col] = self.df[col].fillna(mode_val)
                self.feature_log.append(f"Imputed {col} with mode: {mode_val}")
        
        remaining_missing = self.df.isnull().sum().sum()
        print(f"  ✓ Missing values handled. Remaining: {remaining_missing}")
    
    def _encode_categoricals(self):
        print("\n--- Encoding Categorical Features ---")
        
        encoded_cols = []
        for col in self.categorical_features:
            if col not in self.df.columns or col == self.target_col:
                continue
            
            n_unique = self.df[col].nunique()
            
            if n_unique == 2:
                # Binary: Label encode
                le = LabelEncoder()
                self.df[col] = le.fit_transform(self.df[col].astype(str))
                self.feature_log.append(f"Label encoded {col} (binary)")
                encoded_cols.append(col)
                
            elif n_unique <= 10:
                # Low cardinality: One-hot encode
                dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)
                self.df = pd.concat([self.df, dummies], axis=1)
                self.df = self.df.drop(columns=[col])
                self.new_features.extend(dummies.columns.tolist())
                self.feature_log.append(f"One-hot encoded {col} ({n_unique} categories)")
                
            else:
                # High cardinality: Target encoding or frequency encoding
                freq_map = self.df[col].value_counts(normalize=True).to_dict()
                self.df[f'{col}_freq'] = self.df[col].map(freq_map)
                self.df = self.df.drop(columns=[col])
                self.new_features.append(f'{col}_freq')
                self.feature_log.append(f"Frequency encoded {col} ({n_unique} categories)")
        
        print(f"  ✓ Encoded {len(self.categorical_features)} categorical features")
    
    def _create_interactions(self):
        print("\n--- Creating Interaction Features ---")
        
        # Get top numeric features by correlation with target (if classification, use dummy)
        numeric_cols = [c for c in self.numeric_features if c in self.df.columns]
        
        if len(numeric_cols) < 2:
            print("  Not enough numeric features for interactions")
            return
        
        # Create interactions between top 5 numeric features
        top_cols = numeric_cols[:5]
        interaction_count = 0
        
        for i in range(len(top_cols)):
            for j in range(i+1, len(top_cols)):
                col1, col2 = top_cols[i], top_cols[j]
                
                # Multiplication
                new_col = f'{col1}_x_{col2}'
                self.df[new_col] = self.df[col1] * self.df[col2]
                self.new_features.append(new_col)
                interaction_count += 1
        
        self.feature_log.append(f"Created {interaction_count} interaction features")
        print(f"  ✓ Created {interaction_count} interaction features")
    
    def _create_polynomial_features(self):
        print("\n--- Creating Polynomial Features ---")
        
        numeric_cols = [c for c in self.numeric_features if c in self.df.columns][:5]
        poly_count = 0
        
        for col in numeric_cols:
            # Squared
            self.df[f'{col}_squared'] = self.df[col] ** 2
            self.new_features.append(f'{col}_squared')
            poly_count += 1
            
            # Square root (for positive values)
            if (self.df[col] >= 0).all():
                self.df[f'{col}_sqrt'] = np.sqrt(self.df[col])
                self.new_features.append(f'{col}_sqrt')
                poly_count += 1
            
            # Log (for positive values > 0)
            if (self.df[col] > 0).all():
                self.df[f'{col}_log'] = np.log1p(self.df[col])
                self.new_features.append(f'{col}_log')
                poly_count += 1
        
        self.feature_log.append(f"Created {poly_count} polynomial features")
        print(f"  ✓ Created {poly_count} polynomial features")
    
    def _create_binned_features(self):
        print("\n--- Creating Binned Features ---")
        
        numeric_cols = [c for c in self.numeric_features if c in self.df.columns][:5]
        bin_count = 0
        
        for col in numeric_cols:
            # Quantile binning
            try:
                self.df[f'{col}_binned'] = pd.qcut(self.df[col], q=5, labels=False, duplicates='drop')
                self.new_features.append(f'{col}_binned')
                bin_count += 1
            except:
                pass  # Skip if binning fails
        
        self.feature_log.append(f"Created {bin_count} binned features")
        print(f"  ✓ Created {bin_count} binned features")
    
    def _create_statistical_features(self):
        print("\n--- Creating Statistical Features ---")
        
        numeric_cols = [c for c in self.numeric_features if c in self.df.columns]
        
        if len(numeric_cols) < 3:
            return
        
        # Row-wise statistics across numeric features
        numeric_df = self.df[numeric_cols]
        
        self.df['numeric_mean'] = numeric_df.mean(axis=1)
        self.df['numeric_std'] = numeric_df.std(axis=1)
        self.df['numeric_min'] = numeric_df.min(axis=1)
        self.df['numeric_max'] = numeric_df.max(axis=1)
        self.df['numeric_range'] = self.df['numeric_max'] - self.df['numeric_min']
        
        stat_features = ['numeric_mean', 'numeric_std', 'numeric_min', 'numeric_max', 'numeric_range']
        self.new_features.extend(stat_features)
        self.feature_log.append(f"Created {len(stat_features)} statistical aggregation features")
        print(f"  ✓ Created {len(stat_features)} statistical features")
    
    def _create_ratio_features(self):
        print("\n--- Creating Ratio Features ---")
        
        numeric_cols = [c for c in self.numeric_features if c in self.df.columns][:5]
        ratio_count = 0
        
        for i in range(len(numeric_cols)):
            for j in range(i+1, len(numeric_cols)):
                col1, col2 = numeric_cols[i], numeric_cols[j]
                
                # Avoid division by zero
                if (self.df[col2] != 0).all():
                    new_col = f'{col1}_div_{col2}'
                    self.df[new_col] = self.df[col1] / (self.df[col2] + 1e-8)
                    self.new_features.append(new_col)
                    ratio_count += 1
        
        self.feature_log.append(f"Created {ratio_count} ratio features")
        print(f"  ✓ Created {ratio_count} ratio features")
    
    def _create_date_features(self):
        print("\n--- Creating Date Features ---")
        
        if self.date_col not in self.df.columns:
            return
        
        date_series = pd.to_datetime(self.df[self.date_col], errors='coerce')
        
        self.df['year'] = date_series.dt.year
        self.df['month'] = date_series.dt.month
        self.df['day'] = date_series.dt.day
        self.df['dayofweek'] = date_series.dt.dayofweek
        self.df['quarter'] = date_series.dt.quarter
        self.df['is_weekend'] = (date_series.dt.dayofweek >= 5).astype(int)
        self.df['day_of_year'] = date_series.dt.dayofyear
        
        date_features = ['year', 'month', 'day', 'dayofweek', 'quarter', 'is_weekend', 'day_of_year']
        self.new_features.extend(date_features)
        
        # Drop original date column
        self.df = self.df.drop(columns=[self.date_col])
        
        self.feature_log.append(f"Created {len(date_features)} date features from {self.date_col}")
        print(f"  ✓ Created {len(date_features)} date features")
    
    def _print_summary(self):
        print("\n=== FEATURE ENGINEERING SUMMARY ===")
        print(f"  Original features: {len(self.numeric_features) + len(self.categorical_features)}")
        print(f"  New features created: {len(self.new_features)}")
        print(f"  Total features: {len([c for c in self.df.columns if c != self.target_col])}")
        print(f"\n  Feature Engineering Log:")
        for log in self.feature_log:
            print(f"    • {log}")

# Run feature engineering
feature_engineer = FeatureEngineer(
    df=df,
    target_col=PipelineConfig.TARGET_COLUMN,
    numeric_features=data_assessment['numeric_features'],
    categorical_features=data_assessment['categorical_features'],
    id_col=PipelineConfig.ID_COLUMN,
    date_col=PipelineConfig.DATE_COLUMN,
    level=PipelineConfig.FEATURE_ENGINEERING_LEVEL
)

df_engineered, feature_log = feature_engineer.engineer_features()
```

### Section 6: Feature Selection

```python
# --- FEATURE SELECTION ---
print("\n" + "-" * 80)
print("PHASE 4: FEATURE SELECTION")
print("-" * 80)

class FeatureSelector:
    """Automated feature selection using multiple methods"""
    
    def __init__(self, df, target_col, problem_type):
        self.df = df.copy()
        self.target_col = target_col
        self.problem_type = problem_type
        self.feature_cols = [c for c in df.columns if c != target_col]
        self.selection_results = {}
        
    def select_features(self, method='combined', n_features=None):
        print(f"\n=== FEATURE SELECTION ===\n")
        
        X = self.df[self.feature_cols]
        y = self.df[self.target_col]
        
        # Handle any remaining missing values
        X = X.fillna(0)
        
        # Default to selecting top 50% of features
        if n_features is None:
            n_features = max(10, int(len(self.feature_cols) * 0.5))
        
        print(f"  Original features: {len(self.feature_cols)}")
        print(f"  Target selection: {n_features} features\n")
        
        # Method 1: Correlation-based
        corr_features = self._correlation_selection(X, y, n_features)
        
        # Method 2: Mutual Information
        mi_features = self._mutual_info_selection(X, y, n_features)
        
        # Method 3: Model-based importance
        importance_features = self._model_importance_selection(X, y, n_features)
        
        # Combine selections (features that appear in multiple methods)
        all_selections = [set(corr_features), set(mi_features), set(importance_features)]
        
        # Features selected by at least 2 methods
        feature_counts = {}
        for selection in all_selections:
            for feat in selection:
                feature_counts[feat] = feature_counts.get(feat, 0) + 1
        
        # Sort by count and take top n
        sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)
        final_features = [f[0] for f in sorted_features[:n_features]]
        
        # Ensure we have enough features
        if len(final_features) < n_features:
            remaining = [f for f in importance_features if f not in final_features]
            final_features.extend(remaining[:n_features - len(final_features)])
        
        print(f"\n=== FINAL SELECTED FEATURES ({len(final_features)}) ===")
        print(f"  Features selected by multiple methods: {sum(1 for f in final_features if feature_counts.get(f, 0) >= 2)}")
        
        self.selected_features = final_features
        return final_features
    
    def _correlation_selection(self, X, y, n_features):
        print("--- Method 1: Correlation Analysis ---")
        
        if self.problem_type == 'regression':
            correlations = X.apply(lambda col: col.corr(y)).abs()
        else:
            # For classification, use correlation with target
            y_numeric = LabelEncoder().fit_transform(y)
            correlations = X.apply(lambda col: np.corrcoef(col.fillna(0), y_numeric)[0, 1]).abs()
        
        correlations = correlations.fillna(0)
        top_features = correlations.sort_values(ascending=False).head(n_features).index.tolist()
        
        self.selection_results['correlation'] = top_features
        print(f"  Top correlated features: {top_features[:5]}...")
        
        return top_features
    
    def _mutual_info_selection(self, X, y, n_features):
        print("\n--- Method 2: Mutual Information ---")
        
        # Handle any remaining issues
        X_clean = X.fillna(0)
        
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            mi_func = mutual_info_classif
        else:
            mi_func = mutual_info_regression
        
        try:
            mi_scores = mi_func(X_clean, y, random_state=42)
            mi_series = pd.Series(mi_scores, index=X.columns)
            top_features = mi_series.sort_values(ascending=False).head(n_features).index.tolist()
            
            self.selection_results['mutual_info'] = top_features
            print(f"  Top MI features: {top_features[:5]}...")
        except Exception as e:
            print(f"  Mutual info failed: {e}")
            top_features = X.columns[:n_features].tolist()
        
        return top_features
    
    def _model_importance_selection(self, X, y, n_features):
        print("\n--- Method 3: Model-Based Importance ---")
        
        X_clean = X.fillna(0)
        
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        else:
            model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        
        model.fit(X_clean, y)
        
        importance = pd.Series(model.feature_importances_, index=X.columns)
        top_features = importance.sort_values(ascending=False).head(n_features).index.tolist()
        
        self.selection_results['model_importance'] = top_features
        print(f"  Top important features: {top_features[:5]}...")
        
        return top_features

# Run feature selection
selector = FeatureSelector(
    df=df_engineered,
    target_col=PipelineConfig.TARGET_COLUMN,
    problem_type=PipelineConfig.PROBLEM_TYPE
)

selected_features = selector.select_features()
print(f"\n✓ Selected {len(selected_features)} features for modeling")
```

### Section 7: Data Splitting

```python
# --- DATA SPLITTING ---
print("\n" + "-" * 80)
print("PHASE 5: DATA SPLITTING")
print("-" * 80)

# Prepare final datasets
X = df_engineered[selected_features]
y = df_engineered[PipelineConfig.TARGET_COLUMN]

# Encode target if needed
if PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
    if y.dtype == 'object':
        label_encoder = LabelEncoder()
        y = pd.Series(label_encoder.fit_transform(y), index=y.index)
        class_names = label_encoder.classes_.tolist()
        print(f"\nTarget encoded: {dict(zip(class_names, range(len(class_names))))}")
    else:
        class_names = sorted(y.unique().tolist())
else:
    class_names = None
    label_encoder = None

# Train-test split
if PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=PipelineConfig.TEST_SIZE, 
        random_state=PipelineConfig.RANDOM_STATE,
        stratify=y
    )
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=PipelineConfig.TEST_SIZE, 
        random_state=PipelineConfig.RANDOM_STATE
    )

print(f"\n=== DATA SPLIT ===")
print(f"  Training set: {len(X_train):,} samples ({(1-PipelineConfig.TEST_SIZE)*100:.0f}%)")
print(f"  Test set: {len(X_test):,} samples ({PipelineConfig.TEST_SIZE*100:.0f}%)")
print(f"  Features: {X_train.shape[1]}")

# Handle class imbalance
if PipelineConfig.HANDLE_IMBALANCE and PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
    if IMBLEARN_AVAILABLE and PipelineConfig.IMBALANCE_STRATEGY == 'smote':
        print(f"\n--- Applying SMOTE for class imbalance ---")
        print(f"  Before SMOTE: {dict(pd.Series(y_train).value_counts())}")
        
        smote = SMOTE(random_state=PipelineConfig.RANDOM_STATE)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        print(f"  After SMOTE: {dict(pd.Series(y_train_balanced).value_counts())}")
        
        X_train, y_train = X_train_balanced, y_train_balanced
    else:
        print(f"\n  Using class_weight='balanced' for imbalance handling")

# Feature scaling (fit on training data only)
scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

print(f"\n✓ Data prepared for modeling")
```

### Section 8: Model Training and Comparison

```python
# --- MODEL TRAINING & COMPARISON ---
print("\n" + "-" * 80)
print("PHASE 6: MODEL TRAINING & COMPARISON")
print("-" * 80)

class ModelTrainer:
    """Train and compare multiple models"""
    
    def __init__(self, problem_type, optimization_metric, random_state=42):
        self.problem_type = problem_type
        self.optimization_metric = optimization_metric
        self.random_state = random_state
        self.models = {}
        self.results = []
        self.best_model = None
        self.best_model_name = None
        
    def get_models(self):
        """Get candidate models based on problem type"""
        
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            models = {
                'Logistic Regression': LogisticRegression(
                    random_state=self.random_state, max_iter=1000, class_weight='balanced'
                ),
                'Random Forest': RandomForestClassifier(
                    n_estimators=100, random_state=self.random_state, n_jobs=-1, class_weight='balanced'
                ),
                'Gradient Boosting': GradientBoostingClassifier(
                    n_estimators=100, random_state=self.random_state
                ),
                'Extra Trees': ExtraTreesClassifier(
                    n_estimators=100, random_state=self.random_state, n_jobs=-1, class_weight='balanced'
                ),
                'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),
                'SVM': SVC(probability=True, random_state=self.random_state, class_weight='balanced'),
                'Naive Bayes': GaussianNB(),
            }
            
            if XGBOOST_AVAILABLE:
                models['XGBoost'] = XGBClassifier(
                    n_estimators=100, random_state=self.random_state, 
                    use_label_encoder=False, eval_metric='logloss'
                )
            
            if LIGHTGBM_AVAILABLE:
                models['LightGBM'] = LGBMClassifier(
                    n_estimators=100, random_state=self.random_state, verbose=-1,
                    class_weight='balanced'
                )
            
            if CATBOOST_AVAILABLE:
                models['CatBoost'] = CatBoostClassifier(
                    iterations=100, random_state=self.random_state, verbose=0,
                    auto_class_weights='Balanced'
                )
        
        else:  # Regression
            models = {
                'Ridge': Ridge(random_state=self.random_state),
                'Lasso': Lasso(random_state=self.random_state),
                'ElasticNet': ElasticNet(random_state=self.random_state),
                'Random Forest': RandomForestRegressor(
                    n_estimators=100, random_state=self.random_state, n_jobs=-1
                ),
                'Gradient Boosting': GradientBoostingRegressor(
                    n_estimators=100, random_state=self.random_state
                ),
                'Extra Trees': ExtraTreesRegressor(
                    n_estimators=100, random_state=self.random_state, n_jobs=-1
                ),
                'KNN': KNeighborsRegressor(n_neighbors=5, n_jobs=-1),
                'SVR': SVR(),
            }
            
            if XGBOOST_AVAILABLE:
                models['XGBoost'] = XGBRegressor(
                    n_estimators=100, random_state=self.random_state
                )
            
            if LIGHTGBM_AVAILABLE:
                models['LightGBM'] = LGBMRegressor(
                    n_estimators=100, random_state=self.random_state, verbose=-1
                )
            
            if CATBOOST_AVAILABLE:
                models['CatBoost'] = CatBoostRegressor(
                    iterations=100, random_state=self.random_state, verbose=0
                )
        
        return models
    
    def train_and_evaluate(self, X_train, X_test, y_train, y_test, cv_folds=5):
        """Train all models and evaluate performance"""
        
        print(f"\n=== TRAINING {len(self.get_models())} MODELS ===\n")
        
        models = self.get_models()
        
        for name, model in models.items():
            print(f"Training {name}...", end=" ")
            
            try:
                # Fit model
                model.fit(X_train, y_train)
                
                # Cross-validation
                if self.problem_type in ['binary_classification', 'multiclass_classification']:
                    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
                    scoring = 'roc_auc' if self.problem_type == 'binary_classification' else 'f1_weighted'
                else:
                    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
                    scoring = 'neg_root_mean_squared_error'
                
                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)
                
                # Test predictions
                y_pred = model.predict(X_test)
                
                # Calculate metrics
                if self.problem_type == 'binary_classification':
                    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred
                    
                    metrics = {
                        'Model': name,
                        'CV Score': cv_scores.mean(),
                        'CV Std': cv_scores.std(),
                        'Accuracy': accuracy_score(y_test, y_pred),
                        'Precision': precision_score(y_test, y_pred, zero_division=0),
                        'Recall': recall_score(y_test, y_pred, zero_division=0),
                        'F1': f1_score(y_test, y_pred, zero_division=0),
                        'ROC-AUC': roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0,
                        'PR-AUC': average_precision_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else 0
                    }
                    
                elif self.problem_type == 'multiclass_classification':
                    y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
                    
                    metrics = {
                        'Model': name,
                        'CV Score': cv_scores.mean(),
                        'CV Std': cv_scores.std(),
                        'Accuracy': accuracy_score(y_test, y_pred),
                        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                        'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                        'F1': f1_score(y_test, y_pred, average='weighted', zero_division=0),
                    }
                    
                else:  # Regression
                    metrics = {
                        'Model': name,
                        'CV Score': -cv_scores.mean(),  # Negate because sklearn uses negative RMSE
                        'CV Std': cv_scores.std(),
                        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
                        'MAE': mean_absolute_error(y_test, y_pred),
                        'MAPE': mean_absolute_percentage_error(y_test, y_pred) * 100,
                        'R2': r2_score(y_test, y_pred)
                    }
                
                self.models[name] = model
                self.results.append(metrics)
                
                # Print brief result
                if self.problem_type in ['binary_classification', 'multiclass_classification']:
                    print(f"F1: {metrics['F1']:.4f}, CV: {metrics['CV Score']:.4f}")
                else:
                    print(f"RMSE: {metrics['RMSE']:.4f}, R2: {metrics['R2']:.4f}")
                    
            except Exception as e:
                print(f"Failed: {e}")
        
        # Create results DataFrame
        self.results_df = pd.DataFrame(self.results)
        
        # Sort by primary metric
        if self.problem_type == 'binary_classification':
            sort_col = 'ROC-AUC'
        elif self.problem_type == 'multiclass_classification':
            sort_col = 'F1'
        else:
            sort_col = 'RMSE'
        
        ascending = (sort_col == 'RMSE')
        self.results_df = self.results_df.sort_values(sort_col, ascending=ascending)
        
        # Set best model
        self.best_model_name = self.results_df.iloc[0]['Model']
        self.best_model = self.models[self.best_model_name]
        
        return self.results_df
    
    def print_results(self):
        """Print formatted results"""
        
        print("\n" + "=" * 80)
        print("MODEL COMPARISON RESULTS")
        print("=" * 80)
        
        # Format for display
        display_df = self.results_df.copy()
        for col in display_df.columns:
            if display_df[col].dtype == 'float64':
                display_df[col] = display_df[col].round(4)
        
        print(display_df.to_string(index=False))
        
        print(f"\n✓ BEST MODEL: {self.best_model_name}")

# Train models
trainer = ModelTrainer(
    problem_type=PipelineConfig.PROBLEM_TYPE,
    optimization_metric=PipelineConfig.OPTIMIZATION_METRIC,
    random_state=PipelineConfig.RANDOM_STATE
)

results_df = trainer.train_and_evaluate(
    X_train_scaled, X_test_scaled, y_train, y_test,
    cv_folds=PipelineConfig.VALIDATION_FOLDS
)

trainer.print_results()
```

### Section 9: Ensemble Methods

```python
# --- ENSEMBLE METHODS ---
print("\n" + "-" * 80)
print("PHASE 7: ENSEMBLE METHODS")
print("-" * 80)

if PipelineConfig.USE_ENSEMBLE:
    print("\n=== BUILDING ENSEMBLE MODELS ===\n")
    
    # Get top 3 models for stacking
    top_models = results_df.head(3)['Model'].tolist()
    print(f"  Base models for stacking: {top_models}")
    
    # Build stacking ensemble
    if PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
        estimators = [(name, trainer.models[name]) for name in top_models]
        
        # Stacking Classifier
        stacking_model = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(random_state=PipelineConfig.RANDOM_STATE),
            cv=5,
            n_jobs=-1
        )
        
        # Voting Classifier
        voting_model = VotingClassifier(
            estimators=estimators,
            voting='soft',
            n_jobs=-1
        )
        
    else:  # Regression
        estimators = [(name, trainer.models[name]) for name in top_models]
        
        stacking_model = StackingRegressor(
            estimators=estimators,
            final_estimator=Ridge(random_state=PipelineConfig.RANDOM_STATE),
            cv=5,
            n_jobs=-1
        )
        
        voting_model = VotingRegressor(
            estimators=estimators,
            n_jobs=-1
        )
    
    # Train and evaluate ensemble models
    ensemble_results = []
    
    for name, model in [('Stacking', stacking_model), ('Voting', voting_model)]:
        print(f"\nTraining {name} Ensemble...")
        
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        
        if PipelineConfig.PROBLEM_TYPE == 'binary_classification':
            y_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            metrics = {
                'Model': f'{name} Ensemble',
                'Accuracy': accuracy_score(y_test, y_pred),
                'Precision': precision_score(y_test, y_pred, zero_division=0),
                'Recall': recall_score(y_test, y_pred, zero_division=0),
                'F1': f1_score(y_test, y_pred, zero_division=0),
                'ROC-AUC': roc_auc_score(y_test, y_proba)
            }
            print(f"  {name}: F1={metrics['F1']:.4f}, ROC-AUC={metrics['ROC-AUC']:.4f}")
            
        elif PipelineConfig.PROBLEM_TYPE == 'multiclass_classification':
            metrics = {
                'Model': f'{name} Ensemble',
                'Accuracy': accuracy_score(y_test, y_pred),
                'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
                'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
                'F1': f1_score(y_test, y_pred, average='weighted', zero_division=0)
            }
            print(f"  {name}: F1={metrics['F1']:.4f}")
            
        else:  # Regression
            metrics = {
                'Model': f'{name} Ensemble',
                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
                'MAE': mean_absolute_error(y_test, y_pred),
                'R2': r2_score(y_test, y_pred)
            }
            print(f"  {name}: RMSE={metrics['RMSE']:.4f}, R2={metrics['R2']:.4f}")
        
        ensemble_results.append(metrics)
        trainer.models[f'{name} Ensemble'] = model
    
    # Compare with best single model
    ensemble_df = pd.DataFrame(ensemble_results)
    print("\n=== ENSEMBLE RESULTS ===")
    print(ensemble_df.round(4).to_string(index=False))
    
    # Update best model if ensemble is better
    if PipelineConfig.PROBLEM_TYPE == 'binary_classification':
        best_ensemble = ensemble_df.loc[ensemble_df['ROC-AUC'].idxmax()]
        if best_ensemble['ROC-AUC'] > results_df.iloc[0]['ROC-AUC']:
            trainer.best_model_name = best_ensemble['Model']
            trainer.best_model = trainer.models[best_ensemble['Model']]
            print(f"\n✓ Ensemble outperforms single models!")
    elif PipelineConfig.PROBLEM_TYPE == 'multiclass_classification':
        best_ensemble = ensemble_df.loc[ensemble_df['F1'].idxmax()]
        if best_ensemble['F1'] > results_df.iloc[0]['F1']:
            trainer.best_model_name = best_ensemble['Model']
            trainer.best_model = trainer.models[best_ensemble['Model']]
    else:
        best_ensemble = ensemble_df.loc[ensemble_df['RMSE'].idxmin()]
        if best_ensemble['RMSE'] < results_df.iloc[0]['RMSE']:
            trainer.best_model_name = best_ensemble['Model']
            trainer.best_model = trainer.models[best_ensemble['Model']]

print(f"\n✓ FINAL BEST MODEL: {trainer.best_model_name}")
best_model = trainer.best_model
```

### Section 10: Model Interpretation

```python
# --- MODEL INTERPRETATION ---
print("\n" + "-" * 80)
print("PHASE 8: MODEL INTERPRETATION")
print("-" * 80)

if PipelineConfig.INTERPRETABILITY_LEVEL == 'full' and SHAP_AVAILABLE:
    print("\n=== SHAP ANALYSIS ===\n")
    
    # Use a tree-based model for SHAP if available
    interpretable_model = trainer.best_model
    model_type = type(interpretable_model).__name__
    
    print(f"  Generating SHAP explanations for {trainer.best_model_name}...")
    
    try:
        # Sample data for SHAP (limit to 500 for speed)
        shap_sample_size = min(500, len(X_test_scaled))
        X_shap = X_test_scaled.iloc[:shap_sample_size]
        
        # Create appropriate explainer
        if 'Forest' in model_type or 'XGB' in model_type or 'LGBM' in model_type or 'CatBoost' in model_type:
            explainer = shap.TreeExplainer(interpretable_model)
            shap_values = explainer.shap_values(X_shap)
        else:
            explainer = shap.KernelExplainer(
                interpretable_model.predict_proba if hasattr(interpretable_model, 'predict_proba') else interpretable_model.predict,
                shap.sample(X_train_scaled, 100)
            )
            shap_values = explainer.shap_values(X_shap)
        
        # For binary classification, get values for positive class
        if PipelineConfig.PROBLEM_TYPE == 'binary_classification' and isinstance(shap_values, list):
            shap_values = shap_values[1]
        
        # Calculate feature importance from SHAP
        if isinstance(shap_values, list):
            shap_importance = np.abs(shap_values[0]).mean(axis=0)
        else:
            shap_importance = np.abs(shap_values).mean(axis=0)
        
        shap_importance_df = pd.DataFrame({
            'Feature': X_shap.columns,
            'SHAP Importance': shap_importance
        }).sort_values('SHAP Importance', ascending=False)
        
        print("\n=== TOP FEATURES BY SHAP IMPORTANCE ===")
        print(shap_importance_df.head(15).to_string(index=False))
        
        # Generate SHAP visualizations
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # Summary plot
        plt.sca(axes[0])
        shap.summary_plot(shap_values, X_shap, plot_type="bar", show=False, max_display=15)
        axes[0].set_title('SHAP Feature Importance', fontsize=12)
        
        # Beeswarm plot
        plt.sca(axes[1])
        shap.summary_plot(shap_values, X_shap, show=False, max_display=15)
        axes[1].set_title('SHAP Value Distribution', fontsize=12)
        
        plt.tight_layout()
        plt.savefig('shap_analysis.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("\n  ✓ SHAP visualizations saved as 'shap_analysis.png'")
        
    except Exception as e:
        print(f"  SHAP analysis failed: {e}")
        print("  Falling back to model feature importance...")
        
        if hasattr(best_model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'Feature': selected_features,
                'Importance': best_model.feature_importances_
            }).sort_values('Importance', ascending=False)
            
            print("\n=== TOP FEATURES BY MODEL IMPORTANCE ===")
            print(importance_df.head(15).to_string(index=False))

else:
    print("\n=== FEATURE IMPORTANCE (Model-Based) ===\n")
    
    if hasattr(best_model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'Feature': selected_features,
            'Importance': best_model.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        print(importance_df.head(15).to_string(index=False))
    elif hasattr(best_model, 'coef_'):
        if len(best_model.coef_.shape) == 1:
            coef = best_model.coef_
        else:
            coef = np.abs(best_model.coef_).mean(axis=0)
        
        importance_df = pd.DataFrame({
            'Feature': selected_features,
            'Coefficient': coef
        }).sort_values('Coefficient', key=abs, ascending=False)
        
        print(importance_df.head(15).to_string(index=False))
```

### Section 11: Final Evaluation and Visualizations

```python
# --- FINAL EVALUATION ---
print("\n" + "-" * 80)
print("PHASE 9: FINAL EVALUATION & VISUALIZATIONS")
print("-" * 80)

# Final predictions
y_pred_final = best_model.predict(X_test_scaled)

if PipelineConfig.PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:
    if hasattr(best_model, 'predict_proba'):
        y_proba_final = best_model.predict_proba(X_test_scaled)
    else:
        y_proba_final = None

# Generate comprehensive visualizations
fig = plt.figure(figsize=(20, 16))

if PipelineConfig.PROBLEM_TYPE == 'binary_classification':
    # Plot 1: Confusion Matrix
    ax1 = fig.add_subplot(2, 3, 1)
    cm = confusion_matrix(y_test, y_pred_final)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=class_names, yticklabels=class_names)
    ax1.set_xlabel('Predicted')
    ax1.set_ylabel('Actual')
    ax1.set_title('Confusion Matrix')
    
    # Plot 2: ROC Curve
    ax2 = fig.add_subplot(2, 3, 2)
    if y_proba_final is not None:
        fpr, tpr, _ = roc_curve(y_test, y_proba_final[:, 1])
        auc = roc_auc_score(y_test, y_proba_final[:, 1])
        ax2.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.4f})')
        ax2.plot([0, 1], [0, 1], 'r--', linewidth=1)
        ax2.fill_between(fpr, tpr, alpha=0.2)
        ax2.set_xlabel('False Positive Rate')
        ax2.set_ylabel('True Positive Rate')
        ax2.set_title('ROC Curve')
        ax2.legend()
    
    # Plot 3: Precision-Recall Curve
    ax3 = fig.add_subplot(2, 3, 3)
    if y_proba_final is not None:
        precision, recall, thresholds = precision_recall_curve(y_test, y_proba_final[:, 1])
        ap = average_precision_score(y_test, y_proba_final[:, 1])
        ax3.plot(recall, precision, 'b-', linewidth=2, label=f'PR (AP = {ap:.4f})')
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.set_title('Precision-Recall Curve')
        ax3.legend()
    
    # Plot 4: Probability Distribution
    ax4 = fig.add_subplot(2, 3, 4)
    if y_proba_final is not None:
        ax4.hist(y_proba_final[y_test == 0, 1], bins=30, alpha=0.6, label=class_names[0], color='steelblue')
        ax4.hist(y_proba_final[y_test == 1, 1], bins=30, alpha=0.6, label=class_names[1], color='coral')
        ax4.axvline(x=PipelineConfig.BUSINESS_THRESHOLD, color='red', linestyle='--', label='Threshold')
        ax4.set_xlabel('Predicted Probability')
        ax4.set_ylabel('Count')
        ax4.set_title('Probability Distribution by Class')
        ax4.legend()
    
    # Plot 5: Feature Importance
    ax5 = fig.add_subplot(2, 3, 5)
    if hasattr(best_model, 'feature_importances_'):
        imp = pd.Series(best_model.feature_importances_, index=selected_features).sort_values(ascending=True).tail(15)
        imp.plot(kind='barh', ax=ax5, color='steelblue')
        ax5.set_xlabel('Importance')
        ax5.set_title('Top 15 Feature Importance')
    
    # Plot 6: Model Comparison
    ax6 = fig.add_subplot(2, 3, 6)
    comparison_plot = results_df.head(8)[['Model', 'ROC-AUC']].set_index('Model')
    comparison_plot.plot(kind='barh', ax=ax6, color='steelblue', legend=False)
    ax6.set_xlabel('ROC-AUC')
    ax6.set_title('Model Comparison')

elif PipelineConfig.PROBLEM_TYPE == 'regression':
    # Plot 1: Actual vs Predicted
    ax1 = fig.add_subplot(2, 3, 1)
    ax1.scatter(y_test, y_pred_final, alpha=0.5)
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
    ax1.set_xlabel('Actual')
    ax1.set_ylabel('Predicted')
    ax1.set_title('Actual vs Predicted')
    
    # Plot 2: Residuals Distribution
    ax2 = fig.add_subplot(2, 3, 2)
    residuals = y_test - y_pred_final
    ax2.hist(residuals, bins=50, color='steelblue', edgecolor='k')
    ax2.axvline(x=0, color='red', linestyle='--')
    ax2.set_xlabel('Residuals')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Residuals Distribution')
    
    # Plot 3: Residuals vs Predicted
    ax3 = fig.add_subplot(2, 3, 3)
    ax3.scatter(y_pred_final, residuals, alpha=0.5)
    ax3.axhline(y=0, color='red', linestyle='--')
    ax3.set_xlabel('Predicted')
    ax3.set_ylabel('Residuals')
    ax3.set_title('Residuals vs Predicted')
    
    # Plot 4: Feature Importance
    ax4 = fig.add_subplot(2, 3, 4)
    if hasattr(best_model, 'feature_importances_'):
        imp = pd.Series(best_model.feature_importances_, index=selected_features).sort_values(ascending=True).tail(15)
        imp.plot(kind='barh', ax=ax4, color='steelblue')
        ax4.set_xlabel('Importance')
        ax4.set_title('Top 15 Feature Importance')
    
    # Plot 5: Model Comparison
    ax5 = fig.add_subplot(2, 3, 5)
    comparison_plot = results_df.head(8)[['Model', 'RMSE']].set_index('Model')
    comparison_plot.plot(kind='barh', ax=ax5, color='steelblue', legend=False)
    ax5.set_xlabel('RMSE')
    ax5.set_title('Model Comparison (lower is better)')
    
    # Plot 6: Prediction Error Distribution
    ax6 = fig.add_subplot(2, 3, 6)
    pct_error = (y_pred_final - y_test) / y_test * 100
    ax6.hist(pct_error, bins=50, color='steelblue', edgecolor='k')
    ax6.set_xlabel('Percentage Error')
    ax6.set_ylabel('Frequency')
    ax6.set_title('Percentage Error Distribution')

plt.tight_layout()
plt.savefig('final_evaluation.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n✓ Final evaluation visualizations saved as 'final_evaluation.png'")

# Print final metrics
print("\n" + "=" * 80)
print("FINAL MODEL PERFORMANCE")
print("=" * 80)

if PipelineConfig.PROBLEM_TYPE == 'binary_classification':
    print(f"\n  Model: {trainer.best_model_name}")
    print(f"\n  Test Set Metrics:")
    print(f"    Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}")
    print(f"    Precision: {precision_score(y_test, y_pred_final):.4f}")
    print(f"    Recall:    {recall_score(y_test, y_pred_final):.4f}")
    print(f"    F1 Score:  {f1_score(y_test, y_pred_final):.4f}")
    if y_proba_final is not None:
        print(f"    ROC-AUC:   {roc_auc_score(y_test, y_proba_final[:, 1]):.4f}")
        print(f"    PR-AUC:    {average_precision_score(y_test, y_proba_final[:, 1]):.4f}")
    
    print(f"\n  Classification Report:")
    print(classification_report(y_test, y_pred_final, target_names=class_names))

elif PipelineConfig.PROBLEM_TYPE == 'regression':
    print(f"\n  Model: {trainer.best_model_name}")
    print(f"\n  Test Set Metrics:")
    print(f"    RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_final)):.4f}")
    print(f"    MAE:  {mean_absolute_error(y_test, y_pred_final):.4f}")
    print(f"    MAPE: {mean_absolute_percentage_error(y_test, y_pred_final)*100:.2f}%")
    print(f"    R²:   {r2_score(y_test, y_pred_final):.4f}")
```

### Section 12: Production Pipeline Class

```python
# --- PRODUCTION PIPELINE ---
print("\n" + "-" * 80)
print("PHASE 10: PRODUCTION PIPELINE")
print("-" * 80)

class PredictiveAnalyticsPipeline:
    """
    Production-ready predictive analytics pipeline.
    
    Usage:
        # Load pipeline
        pipeline = PredictiveAnalyticsPipeline.load('pipeline_artifacts/')
        
        # Make predictions
        predictions = pipeline.predict(new_data)
        
        # Get prediction with confidence
        result = pipeline.predict_with_details(new_data)
    """
    
    def __init__(self, model, scaler, selected_features, label_encoder=None, 
                 problem_type='binary_classification', threshold=0.5, class_names=None):
        self.model = model
        self.scaler = scaler
        self.selected_features = selected_features
        self.label_encoder = label_encoder
        self.problem_type = problem_type
        self.threshold = threshold
        self.class_names = class_names
        self.version = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def preprocess(self, X):
        """Preprocess input data"""
        # Ensure all required features are present
        missing_features = set(self.selected_features) - set(X.columns)
        if missing_features:
            raise ValueError(f"Missing features: {missing_features}")
        
        # Select and order features
        X = X[self.selected_features].copy()
        
        # Handle missing values
        X = X.fillna(0)
        
        # Scale features
        X_scaled = pd.DataFrame(
            self.scaler.transform(X),
            columns=X.columns,
            index=X.index
        )
        
        return X_scaled
    
    def predict(self, X):
        """Make predictions"""
        X_processed = self.preprocess(X)
        
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            predictions = self.model.predict(X_processed)
            
            if self.label_encoder:
                predictions = self.label_encoder.inverse_transform(predictions)
        else:
            predictions = self.model.predict(X_processed)
        
        return predictions
    
    def predict_proba(self, X):
        """Get prediction probabilities (classification only)"""
        if self.problem_type not in ['binary_classification', 'multiclass_classification']:
            raise ValueError("predict_proba only available for classification")
        
        X_processed = self.preprocess(X)
        
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X_processed)
        else:
            raise ValueError("Model does not support probability predictions")
    
    def predict_with_details(self, X):
        """Get predictions with detailed information"""
        X_processed = self.preprocess(X)
        
        results = []
        
        for idx in range(len(X_processed)):
            row = X_processed.iloc[[idx]]
            result = {'index': X.index[idx] if hasattr(X, 'index') else idx}
            
            if self.problem_type == 'binary_classification':
                proba = self.model.predict_proba(row)[0]
                pred_class = 1 if proba[1] >= self.threshold else 0
                
                result['prediction'] = self.class_names[pred_class] if self.class_names else pred_class
                result['probability'] = proba[1]
                result['confidence'] = max(proba)
                result['threshold'] = self.threshold
                
            elif self.problem_type == 'multiclass_classification':
                proba = self.model.predict_proba(row)[0]
                pred_class = np.argmax(proba)
                
                result['prediction'] = self.class_names[pred_class] if self.class_names else pred_class
                result['probability'] = proba[pred_class]
                result['all_probabilities'] = dict(zip(self.class_names or range(len(proba)), proba))
                
            else:  # Regression
                pred = self.model.predict(row)[0]
                result['prediction'] = pred
            
            results.append(result)
        
        return pd.DataFrame(results)
    
    def save(self, output_dir):
        """Save pipeline artifacts"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Save model
        with open(os.path.join(output_dir, 'model.pkl'), 'wb') as f:
            pickle.dump(self.model, f)
        
        # Save scaler
        with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save label encoder
        if self.label_encoder:
            with open(os.path.join(output_dir, 'label_encoder.pkl'), 'wb') as f:
                pickle.dump(self.label_encoder, f)
        
        # Save config
        config = {
            'selected_features': self.selected_features,
            'problem_type': self.problem_type,
            'threshold': self.threshold,
            'class_names': self.class_names,
            'version': self.version,
            'model_type': type(self.model).__name__
        }
        
        with open(os.path.join(output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"  ✓ Pipeline saved to {output_dir}/")
        
    @classmethod
    def load(cls, input_dir):
        """Load pipeline from artifacts"""
        # Load model
        with open(os.path.join(input_dir, 'model.pkl'), 'rb') as f:
            model = pickle.load(f)
        
        # Load scaler
        with open(os.path.join(input_dir, 'scaler.pkl'), 'rb') as f:
            scaler = pickle.load(f)
        
        # Load label encoder if exists
        label_encoder_path = os.path.join(input_dir, 'label_encoder.pkl')
        if os.path.exists(label_encoder_path):
            with open(label_encoder_path, 'rb') as f:
                label_encoder = pickle.load(f)
        else:
            label_encoder = None
        
        # Load config
        with open(os.path.join(input_dir, 'config.json'), 'r') as f:
            config = json.load(f)
        
        return cls(
            model=model,
            scaler=scaler,
            selected_features=config['selected_features'],
            label_encoder=label_encoder,
            problem_type=config['problem_type'],
            threshold=config.get('threshold', 0.5),
            class_names=config.get('class_names')
        )

# Create and save production pipeline
pipeline = PredictiveAnalyticsPipeline(
    model=best_model,
    scaler=scaler,
    selected_features=selected_features,
    label_encoder=label_encoder if 'label_encoder' in dir() else None,
    problem_type=PipelineConfig.PROBLEM_TYPE,
    threshold=PipelineConfig.BUSINESS_THRESHOLD,
    class_names=class_names
)

# Save pipeline
output_dir = PipelineConfig.OUTPUT_DIR
pipeline.save(output_dir)

print(f"\n✓ Production pipeline created and saved")
```

### Section 13: Monitoring and Drift Detection

```python
# --- MONITORING & DRIFT DETECTION ---
print("\n" + "-" * 80)
print("PHASE 11: MONITORING & DRIFT DETECTION SETUP")
print("-" * 80)

class ModelMonitor:
    """
    Monitor model performance and detect data drift in production.
    """
    
    def __init__(self, reference_data, feature_names, problem_type):
        self.reference_stats = self._compute_stats(reference_data)
        self.feature_names = feature_names
        self.problem_type = problem_type
        self.performance_history = []
        
    def _compute_stats(self, data):
        """Compute reference statistics for drift detection"""
        stats = {}
        for col in data.columns:
            if data[col].dtype in ['int64', 'float64']:
                stats[col] = {
                    'mean': data[col].mean(),
                    'std': data[col].std(),
                    'min': data[col].min(),
                    'max': data[col].max(),
                    'q25': data[col].quantile(0.25),
                    'q75': data[col].quantile(0.75)
                }
        return stats
    
    def detect_drift(self, new_data, threshold=0.2):
        """
        Detect data drift using statistical tests.
        
        Returns drift score and affected features.
        """
        drift_results = {}
        
        for col in self.feature_names:
            if col not in new_data.columns or col not in self.reference_stats:
                continue
            
            ref = self.reference_stats[col]
            new_mean = new_data[col].mean()
            new_std = new_data[col].std()
            
            # Z-score of mean shift
            if ref['std'] > 0:
                mean_shift = abs(new_mean - ref['mean']) / ref['std']
            else:
                mean_shift = abs(new_mean - ref['mean'])
            
            # Variance ratio
            if ref['std'] > 0:
                var_ratio = new_std / ref['std']
            else:
                var_ratio = 1.0
            
            drift_score = (mean_shift + abs(1 - var_ratio)) / 2
            
            drift_results[col] = {
                'drift_score': drift_score,
                'mean_shift': mean_shift,
                'variance_ratio': var_ratio,
                'is_drifted': drift_score > threshold
            }
        
        # Overall drift assessment
        n_drifted = sum(1 for r in drift_results.values() if r['is_drifted'])
        overall_drift = n_drifted / len(drift_results) if drift_results else 0
        
        return {
            'overall_drift_score': overall_drift,
            'n_drifted_features': n_drifted,
            'total_features': len(drift_results),
            'feature_drift': drift_results,
            'alert': overall_drift > 0.3
        }
    
    def log_performance(self, timestamp, y_true, y_pred, y_proba=None):
        """Log model performance for tracking"""
        
        if self.problem_type == 'binary_classification':
            metrics = {
                'timestamp': timestamp,
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0),
            }
            if y_proba is not None:
                metrics['roc_auc'] = roc_auc_score(y_true, y_proba)
        else:
            metrics = {
                'timestamp': timestamp,
                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
                'mae': mean_absolute_error(y_true, y_pred),
                'r2': r2_score(y_true, y_pred)
            }
        
        self.performance_history.append(metrics)
        return metrics
    
    def check_performance_degradation(self, baseline_metric, current_metric, threshold=0.1):
        """Check if performance has degraded beyond threshold"""
        if self.problem_type in ['binary_classification', 'multiclass_classification']:
            degradation = baseline_metric - current_metric
        else:  # For RMSE, higher is worse
            degradation = current_metric - baseline_metric
        
        return {
            'degradation': degradation,
            'threshold': threshold,
            'alert': degradation > threshold,
            'recommendation': 'Consider retraining' if degradation > threshold else 'Performance acceptable'
        }
    
    def generate_monitoring_report(self, new_data, y_true=None, y_pred=None):
        """Generate comprehensive monitoring report"""
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'data_samples': len(new_data)
        }
        
        # Drift detection
        drift = self.detect_drift(new_data)
        report['drift'] = drift
        
        # Performance tracking (if ground truth available)
        if y_true is not None and y_pred is not None:
            perf = self.log_performance(datetime.now(), y_true, y_pred)
            report['performance'] = perf
        
        # Recommendations
        recommendations = []
        if drift['alert']:
            recommendations.append(f"⚠️ Data drift detected in {drift['n_drifted_features']} features. Consider retraining.")
        
        report['recommendations'] = recommendations
        
        return report

# Initialize monitor with training data
monitor = ModelMonitor(
    reference_data=X_train_scaled,
    feature_names=selected_features,
    problem_type=PipelineConfig.PROBLEM_TYPE
)

# Save monitor
with open(os.path.join(output_dir, 'monitor.pkl'), 'wb') as f:
    pickle.dump(monitor, f)

print("\n✓ Model monitoring setup complete")
print(f"\n  Monitoring artifacts saved to {output_dir}/")

# Example monitoring usage
print("\n=== MONITORING USAGE EXAMPLE ===")
print("""
# Load monitor
with open('model_artifacts/monitor.pkl', 'rb') as f:
    monitor = pickle.load(f)

# Check for data drift on new data
drift_report = monitor.detect_drift(new_data)
if drift_report['alert']:
    print("⚠️ Data drift detected!")
    print(f"Drifted features: {drift_report['n_drifted_features']}")

# Log performance when ground truth is available
metrics = monitor.log_performance(
    timestamp=datetime.now(),
    y_true=actual_values,
    y_pred=predictions
)

# Generate full monitoring report
report = monitor.generate_monitoring_report(new_data, y_true, y_pred)
""")
```

### Section 14: Summary and Next Steps

```python
# --- FINAL SUMMARY ---
print("\n" + "=" * 80)
print("PREDICTIVE ANALYTICS PIPELINE - SUMMARY")
print("=" * 80)

print(f"""
┌─────────────────────────────────────────────────────────────────────────────┐
│                           PIPELINE SUMMARY                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│  Business Problem: [BUSINESS_PROBLEM]
│  Problem Type: {PipelineConfig.PROBLEM_TYPE}
│  Target Variable: {PipelineConfig.TARGET_COLUMN}
│                                                                             │
│  DATA:                                                                      │
│    • Original samples: {data_assessment['n_rows']:,}
│    • Original features: {data_assessment['n_cols']}
│    • Final features (after engineering): {len(selected_features)}
│                                                                             │
│  BEST MODEL: {trainer.best_model_name}
│""")

if PipelineConfig.PROBLEM_TYPE == 'binary_classification':
    final_metrics = {
        'Accuracy': accuracy_score(y_test, y_pred_final),
        'Precision': precision_score(y_test, y_pred_final),
        'Recall': recall_score(y_test, y_pred_final),
        'F1': f1_score(y_test, y_pred_final),
        'ROC-AUC': roc_auc_score(y_test, y_proba_final[:, 1]) if y_proba_final is not None else 'N/A'
    }
    print(f"│    • Accuracy:  {final_metrics['Accuracy']:.4f}")
    print(f"│    • Precision: {final_metrics['Precision']:.4f}")
    print(f"│    • Recall:    {final_metrics['Recall']:.4f}")
    print(f"│    • F1 Score:  {final_metrics['F1']:.4f}")
    print(f"│    • ROC-AUC:   {final_metrics['ROC-AUC']:.4f}" if isinstance(final_metrics['ROC-AUC'], float) else "")

elif PipelineConfig.PROBLEM_TYPE == 'regression':
    final_metrics = {
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_final)),
        'MAE': mean_absolute_error(y_test, y_pred_final),
        'R2': r2_score(y_test, y_pred_final)
    }
    print(f"│    • RMSE: {final_metrics['RMSE']:.4f}")
    print(f"│    • MAE:  {final_metrics['MAE']:.4f}")
    print(f"│    • R²:   {final_metrics['R2']:.4f}")

print(f"""│                                                                             │
│  ARTIFACTS GENERATED:                                                       │
│    • {output_dir}/model.pkl
│    • {output_dir}/scaler.pkl
│    • {output_dir}/config.json
│    • {output_dir}/monitor.pkl
│    • eda_analysis.png
│    • final_evaluation.png
│    • shap_analysis.png (if available)
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
""")

print("""
=== USAGE INSTRUCTIONS ===

# Load the production pipeline
from your_module import PredictiveAnalyticsPipeline

pipeline = PredictiveAnalyticsPipeline.load('model_artifacts/')

# Make predictions on new data
new_data = pd.read_csv('new_data.csv')
predictions = pipeline.predict(new_data)

# Get predictions with confidence scores
detailed_results = pipeline.predict_with_details(new_data)
print(detailed_results)

# Monitor for data drift
with open('model_artifacts/monitor.pkl', 'rb') as f:
    monitor = pickle.load(f)

drift_report = monitor.detect_drift(new_data)
if drift_report['alert']:
    print("⚠️ Data drift detected - consider retraining")


=== RECOMMENDED NEXT STEPS ===

1. VALIDATE: Test pipeline on holdout data before deployment
2. DEPLOY: Wrap pipeline in API (FastAPI/Flask) for production serving
3. MONITOR: Set up scheduled drift detection and performance tracking
4. RETRAIN: Establish retraining triggers (drift threshold, time-based, performance-based)
5. DOCUMENT: Create model card documenting assumptions, limitations, and intended use

""")

print("✓ Predictive Analytics Pipeline Complete!")
```

---

## FILE DELIVERY

- Generate file named: `predictive_analytics_[PROBLEM_TYPE]_[TIMESTAMP].py`
- Timestamp format: YYYYMMDD_HHMMSS
- Provide as downloadable file
- All code must be syntactically valid Python 3.8+
- Required libraries listed at top with installation instructions

---

## EXAMPLE INPUT

```
Build me a complete predictive analytics pipeline for customer churn prediction. This is 
a binary classification problem where I want to predict whether a telecom customer will 
churn (leave the company) within the next month.

Generate synthetic data for 10,000 customers with features including: tenure (months as 
customer), monthly_charges, total_charges, contract_type (Month-to-month, One year, Two 
year), payment_method (Electronic check, Mailed check, Bank transfer, Credit card), 
internet_service (DSL, Fiber optic, None), multiple_lines (Yes, No), online_security (Yes, 
No), tech_support (Yes, No), streaming_tv (Yes, No), streaming_movies (Yes, No), and 
paperless_billing (Yes, No).

The churn rate should be around 25%. Month-to-month contracts should have much higher 
churn than annual contracts. Customers with fiber optic internet but no tech support 
should have elevated churn.

I want:
- Standard feature engineering
- Full model comparison including XGBoost and LightGBM
- Ensemble methods (stacking)
- Full interpretability with SHAP values
- Production-ready pipeline
- Monitoring for data drift

Use 80/20 train-test split and optimize for ROC-AUC.
```

---

## EXAMPLE OUTPUT

*(Due to length constraints, this follows the exact template above with all sections filled in for the customer churn use case. The output would be approximately 1,200-1,500 lines of production-ready Python code including all phases from data assessment through monitoring setup.)*

---

## IMPLEMENTATION NOTES FOR REPLIT AGENT

1. **Problem Type Detection**: Analyze target variable to auto-detect problem type. Binary (2 classes) → binary_classification, 3-20 classes → multiclass, continuous → regression.

2. **Feature Engineering Level**: 
   - "minimal": Only handle missing values and encode categoricals
   - "standard": Add interactions, polynomials, binning
   - "aggressive": Add all above plus statistical aggregations and ratios

3. **Model Selection**: Include all available models. Skip those where packages aren't installed but continue with others.

4. **SHAP Integration**: Use TreeExplainer for tree-based models (faster), fall back to KernelExplainer for others.

5. **Class Imbalance**: Auto-detect if minority class < 20% of data. Apply SMOTE if imblearn available, otherwise use class_weight='balanced'.

6. **Production Pipeline**: Generate a self-contained class that can be pickled and loaded independently.

7. **Monitoring**: Include baseline statistics from training data for drift detection.

8. **Validation**: Before returning:
   - Ensure all imports are present
   - Verify class names are consistent
   - Check pipeline saves and loads correctly
   - Validate all visualizations render

9. **File Delivery**: Provide as downloadable `.py` file. Name format: `predictive_analytics_[PROBLEM_TYPE]_[TIMESTAMP].py`

---

**END OF PREDICTIVE ANALYTICS FUNCTION SPECIFICATION**